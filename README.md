# ICCV-2025-Papers
![image](https://github.com/user-attachments/assets/0b93ce8a-4383-46ba-9672-6c746728c9f9)

## 会议时间：2025年10月19日至23日
## 会议网址：https://iccv.thecvf.com/



## 查看2025年综述文献点这里↘️[2025-CV-Surveys](https://github.com/52CV/CV-Surveys)

## 2025 年论文分类汇总戳这里
↘️[WACV-2025-Papers](https://github.com/52CV/WACV-2025-Papers)
↘️[CVPR-2025-Papers](https://github.com/52CV/CVPR-2025-Papers)
↘️[ICCV-2025-Papers](https://github.com/52CV/ICCV-2025-Papers)

## 2024 年论文分类汇总戳这里
↘️[WACV-2024-Papers](https://github.com/52CV/WACV-2024-Papers)
↘️[CVPR-2024-Papers](https://github.com/52CV/CVPR-2024-Papers)
↘️[ECCV-2024-Papers](https://github.com/52CV/ECCV-2024-Papers)


## [2023 年论文分类汇总戳这里](#0000)
## [2022 年论文分类汇总戳这里](#000)
## [2021 年论文分类汇总戳这里](#00)
## [2020 年论文分类汇总戳这里](#0)



## 已全部分类完

## 目录

|:cat:|:dog:|:tiger:|:wolf:|
|------|------|------|------|
|[1.Other](#1)|[2.Image Progress(图像/视频处理)](#2)|[3.Super-Resolution(超分辨率)](#3)|[4.Image Captioning(图像字幕)](#4)|
|[5.Image Generation(图像生成)](#5)|[6.Image Segmentation(图像分割)](#6)|[7.Image Classification(图像分类)](#7)|[8.Image/Video Retrieval(图像/视频检索)](#8)|
|[9.Image/Video Compression(图像/视频压缩)](#9)|[10.Medical Image Progress(医学图像处理)](#10)|[11.Face](#11)|[12.Avatar](#12)|
|[13.Object Detection(目标检测) ](#13)|[14.Object Track(目标跟踪)](#14)|[15.pose](#15)|[16.Human Motion](#16)|
|[17.Action Recognition(动作识别)](17#)|[18.Re-Id(行人重识别)](18#)|[19.Video](19#)|[20.OCR](20#)|
|[21.UAV/RS/Satellite Image(无人机/遥感/卫星图像)](21#)|[22.3D](22#)|[23.Point Cloud(点云)](23#)|[24.Autonomous Driving(自动驾驶)](24#)|
|[25.HOI(人机交互)](#25)|[26.Robot](#26)|[27.Visual Question Answering(视觉问答)](#27)|[28.Optical Flow Estimation(光流估计)](#28)|
|[29.Deepfake Detection/AI生成图像检测](#29)|[30.Image Fusion(图像融合)](#30)|[31.Image Matching(图像匹配)](#31)|[32.Image Registration(图像配准)](#32)|
|[33.Keypoint Detection(关键点检测)](#33)|[34.Object Pose Estimation(物体姿态估计)](#34)|[35.Style Transfer(风格迁移)](#35)|[36.Scene Graph Generation(场景图生成)](#36)|
|[37.MC/KD/Pruning(模型压缩/知识蒸馏/剪枝)](#37)|[38.F/ZSL/DG/A(小/零样本/域泛化/适应)](#38)|[39.Machine learning(机器学习)](#39)|[40.Deep learning(深度学习)](#40)|
|[41.NAS(神经架构搜索)](#41)|[42.Vision Transformer](#42)|[43.Vision Language(视觉语言)](#43)|[44.Neural Radiance Fields](#44)|
|[45.Dataset](45#)|[46.Sound](46#)|[47.Animation(动画)](47#)|[48.Industrial Anomaly Detection(工业异常检测)](48#)|
|[49.biometric recognition(生物特征识别)](49#)|[50.Protecting copyright(保护版权)](50#)|[51.Visual Relationship Detection,VRD(视觉关系检测)](51#)|[52.Gaze](52#)|
|[53.Dense Prediction](53#)|[54.计算成像](54#)|





<a name="54"/>

## 54.计算成像
* [IM360 Large-scale Indoor Mapping with 360 Cameras](http://arxiv.org/abs/2502.12545)
* [Multispectral Demosaicing via Dual Cameras](http://arxiv.org/abs/2503.22026)
* [Processing and acquisition traces in visual encoders What does CLIP know about your camera](https://openaccess.thecvf.com/content/ICCV2025/papers/Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ryan-caesar-ramos/visual-encoder-traces)
* [Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras](http://arxiv.org/pdf/2506.22069v1)
* [Estimating 2D Camera Motion with Hybrid Motion Basis](https://arxiv.org/pdf/2507.22480v1)<br>:star:[code](https://lhaippp.github.io/CamFlow/)<br>:star:[code](https://github.com/lhaippp/camflow)
* [Image as an IMU Estimating Camera Motion from a Single Motion-Blurred Image](http://arxiv.org/abs/2503.17358)
* [AlignDiff Learning Physically-Grounded Camera Alignment via Diffusion](http://arxiv.org/abs/2503.21581)
* [TrajectoryCrafter Redirecting Camera Trajectory for Monocular Videos via Diffusion Models](http://arxiv.org/abs/2503.05638)
* [Super Resolved Imaging with Adaptive Optics](https://arxiv.org/pdf/2508.04648v1)<br>:house:[project](https://www.cs.toronto.edu/~robin/aosr/)
* [HccePose(BF) Predicting Front  Back Surfaces to Construct Ultra-Dense 2D-3D Correspondences for Pose Estimation](http://arxiv.org/abs/2510.10177)
* [RePoseD Efficient Relative Pose Estimation With Known Depth Information](https://openaccess.thecvf.com/content/ICCV2025/papers/Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/kocurvik/mdrp)
* [Scaling 3D Compositional Models for Robust Classification and Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Yuan_Scaling_3D_Compositional_Models_for_Robust_Classification_and_Pose_Estimation_ICCV_2025_paper.pdf)
* [DRaM-LHM A Quaternion Framework for Iterative Camera Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lin_DRaM-LHM_A_Quaternion_Framework_for_Iterative_Camera_Pose_Estimation_ICCV_2025_paper.pdf)
* [Epipolar Consistent Attention Aggregation Network for Unsupervised Light Field Disparity Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Gao_Epipolar_Consistent_Attention_Aggregation_Network_for_Unsupervised_Light_Field_Disparity_ICCV_2025_paper.pdf)
* [TESPEC Temporally-Enhanced Self-Supervised Pretraining for Event Cameras](http://arxiv.org/abs/2508.00913)<br>:house:[project](https://mhdmohammadi.github.io/TESPEC_webpage)
* [Simultaneous Motion And Noise Estimation with Event Cameras](http://arxiv.org/abs/2504.04029)<br>:star:[code](https://github.com/tub-rip/ESMD) :house:[project](https://github.com/tub-rip/ESMD)
* [EventUPS Uncalibrated Photometric Stereo Using an Event Camera](https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera_ICCV_2025_paper.pdf)
* [GenDoP Auto-regressive Camera Trajectory Generation as a Director of Photography](http://arxiv.org/abs/2504.07083)<br>:house:[project](https://kszpxxzmc.github.io/GenDoP)
* [Inverse Image-Based Rendering for Light Field Generation from Single Images](https://openaccess.thecvf.com/content/ICCV2025/papers/Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images_ICCV_2025_paper.pdf)
* [Princeton365 A Diverse Dataset with Accurate Camera Pose](http://arxiv.org/abs/2506.09035)
* [CF3 Compact and Fast 3D Feature Fields](http://arxiv.org/abs/2508.05254)
* [CCMNet Leveraging Calibrated Color Correction Matrices for Cross-Camera Color Constancy](http://arxiv.org/abs/2504.07959)

<a name="53"/>

## 53.Dense Prediction
* [Frequency-Dynamic Attention Modulation for Dense Prediction](https://arxiv.org/pdf/2507.12006v1)<br>:star:[code](https://github.com/Linwei-Chen/FDAM)
* [FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment](https://arxiv.org/pdf/2506.22509v1)<br>:star:[code](https://github.com/xuhang07/FreeDNA)
* [ATAS Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction](http://arxiv.org/abs/2506.08678)
* [Unbiased Region-Language Alignment for Open-Vocabulary Dense Prediction](http://arxiv.org/abs/2412.06244)<br>:star:[code](https://github.com/HVision-NKU/DenseVLM)
* [Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction](http://arxiv.org/abs/2508.20376)

<a name="52"/>

## 52.Gaze
* [Multi-view Gaze Target Estimation](https://arxiv.org/pdf/2508.05857v1)<br>:house:[project](https://www3.cs.stonybrook.edu/~cvl/multiview_gte.html)
* [Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction](https://arxiv.org/pdf/2507.23021v1)<br>:star:[code](https://aimagelab.github.io/ScanDiff)<br>:star:[code](https://github.com/aimagelab/scandiff)视觉注意力预测
* [Gaze-Language Alignment for Zero-Shot Prediction of Visual Search Targets from Human Gaze Scanpaths](https://openaccess.thecvf.com/content/ICCV2025/papers/Mondal_Gaze-Language_Alignment_for_Zero-Shot_Prediction_of_Visual_Search_Targets_from_ICCV_2025_paper.pdf)
* [What we need is explicit controllability Training 3D gaze estimator using only facial images](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_What_we_need_is_explicit_controllability_Training_3D_gaze_estimator_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ATinyBites/ControllableGaze)

<a name="51"/>

## 51.Visual Relationship Detection,VRD(视觉关系检测)
* [ART: Adaptive Relation Tuning for Generalized Relation Prediction](https://arxiv.org/pdf/2507.23543v1)

<a name="50"/>

## 50.Protecting copyright(保护版权)
* [TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity](https://arxiv.org/pdf/2506.23484v1)
* [Your Text Encoder Can Be An Object-Level Watermarking Controller](http://arxiv.org/abs/2503.11945)
* [SpecGuard Spectral Projection-based Advanced Invisible Watermarking](http://arxiv.org/abs/2510.07302)<br>:star:[code](https://github.com/inzamamulDU/SpecGuard_ICCV_2025)
* [Learning Robust Image Watermarking with Lossless Cover Recovery](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Learning_Robust_Image_Watermarking_with_Lossless_Cover_Recovery_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/chenoly/CRMark)
* [SynTag Enhancing the Geometric Robustness of Inversion-based Generative Image Watermarking](https://openaccess.thecvf.com/content/ICCV2025/papers/Fang_SynTag_Enhancing_the_Geometric_Robustness_of_Inversion-based_Generative_Image_Watermarking_ICCV_2025_paper.pdf)
* [PlugMark A Plug-in Zero-Watermarking Framework for Diffusion Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_PlugMark_A_Plug-in_Zero-Watermarking_Framework_for_Diffusion_Models_ICCV_2025_paper.pdf)
* [ROAR Reducing Inversion Error in Generative Image Watermarking](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking_ICCV_2025_paper.pdf)
* [SEAL Semantic Aware Image Watermarking](http://arxiv.org/abs/2503.12172)
* [Semantic Watermarking Reinvented Enhancing Robustness and Generation Quality with Fourier Integrity](http://arxiv.org/abs/2509.07647)<br>:star:[code](https://github.com/thomas11809/SFWMark)
* [Invisible Watermarks Visible Gains Steering Machine Unlearning with Bi-Level Watermarking Design](http://arxiv.org/abs/2508.10065)
* [TrustMark Robust Watermarking and Watermark Removal for Arbitrary Resolution Images](https://openaccess.thecvf.com/content/ICCV2025/papers/Bui_TrustMark_Robust_Watermarking_and_Watermark_Removal_for_Arbitrary_Resolution_Images_ICCV_2025_paper.pdf)
* [Attention to Neural Plagiarism Diffusion Models Can Plagiarize Your Copyrighted Images](https://openaccess.thecvf.com/content/ICCV2025/papers/Zou_Attention_to_Neural_Plagiarism_Diffusion_Models_Can_Plagiarize_Your_Copyrighted_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/zzzucf/Neural-Plagiarism)
* [From Imitation to Innovation The Emergence of AIs Unique Artistic Styles and the Challenge of Copyright Protection](https://openaccess.thecvf.com/content/ICCV2025/papers/Jia_From_Imitation_to_Innovation_The_Emergence_of_AIs_Unique_Artistic_ICCV_2025_paper.pdf)



<a name="49"/>


## 49.biometric recognition(生物特征识别)
* [DisenQ: Disentangling Q-Former for Activity-Biometrics](https://arxiv.org/pdf/2507.07262v1)
* [A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition](https://arxiv.org/pdf/2508.00053v1)
* 指纹
  * [Training-Free Personalization via Retrieval and Reasoning on Fingerprints](http://arxiv.org/abs/2503.18623)
  * [DiffIP Representation Fingerprints for Robust IP Protection of Diffusion Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_DiffIP_Representation_Fingerprints_for_Robust_IP_Protection_of_Diffusion_Models_ICCV_2025_paper.pdf)
  * [Riemannian-Geometric Fingerprints of Generative Models](http://arxiv.org/abs/2506.22802)

<a name="48"/>

## 48.Industrial Anomaly Detection(工业异常检测)
* [RareCLIP Rarity-aware Online Zero-shot Industrial Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/He_RareCLIP_Rarity-aware_Online_Zero-shot_Industrial_Anomaly_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/hjf02/RareCLIP)
* [ReMP-AD Retrieval-enhanced Multi-modal Prompt Fusion for Few-Shot Industrial Visual Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Ma_ReMP-AD_Retrieval-enhanced_Multi-modal_Prompt_Fusion_for_Few-Shot_Industrial_Visual_Anomaly_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/cshcma/ReMP-AD.git)
* [G2SF Geometry-Guided Score Fusion for Multimodal Industrial Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Tao_G2SF_Geometry-Guided_Score_Fusion_for_Multimodal_Industrial_Anomaly_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ctaoaa/G2SF)
* [Anomaly Detection of Integrated Circuits Package Substrates Using the Large Vision Model SAIC Dataset Construction Methodology and Application](https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_Anomaly_Detection_of_Integrated_Circuits_Package_Substrates_Using_the_Large_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Bingyang0410/CPS2D-AD)
* [SeaS Few-shot Industrial Anomaly Image Generation with Separation and Sharing Fine-tuning](http://arxiv.org/abs/2410.14987)<br>:star:[code](https://github.com/HUST-SLOW/SeaS)
* [Kaputt A Large-Scale Dataset for Visual Defect Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Hofer_Kaputt_A_Large-Scale_Dataset_for_Visual_Defect_Detection_ICCV_2025_paper.pdf)
* [Training-Free Industrial Defect Generation with Diffusion Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_Training-Free_Industrial_Defect_Generation_with_Diffusion_Models_ICCV_2025_paper.pdf)
* [DADet Safeguarding Image Conditional Diffusion Models against Adversarial and Backdoor Attacks via Diffusion Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor_ICCV_2025_paper.pdf)
* [Bridging 3D Anomaly Localization and Repair via High-Quality Continuous Geometric Representation](http://arxiv.org/abs/2505.24431)

<a name="47"/>

## 47.Animation(动画)
* [LayerAnimate: Layer-level Control for Animation](http://arxiv.org/abs/2501.08295)
* [Occlusion-robust Stylization for Drawing-based 3D Animation](https://arxiv.org/pdf/2508.00398v1)
* [Multi-Object Sketch Animation by Scene Decomposition and Motion Planning](http://arxiv.org/abs/2503.19351)
* [Animate Anyone 2 High-Fidelity Character Image Animation with Environment Affordance](http://arxiv.org/abs/2502.06145)
* [LongAnimation Long Animation Generation with Dynamic Global-Local Memory](http://arxiv.org/abs/2507.01945)
* [V2M4 4D Mesh Animation Reconstruction from a Single Monocular Video](http://arxiv.org/abs/2503.09631)
* [OmniHuman-1 Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models_ICCV_2025_paper.pdf)
* [Multi-identity Human Image Animation with Structural Video Diffusion](http://arxiv.org/abs/2504.04126)<br>:star:[code](https://github.com/zhenzhiwang/Multi-HumanVid)
* [Perception-as-Control Fine-grained Controllable Image Animation with 3D-aware Motion Representation](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Perception-as-Control_Fine-grained_Controllable_Image_Animation_with_3D-aware_Motion_Representation_ICCV_2025_paper.pdf)
* [DreamActor-M1 Holistic Expressive and Robust Human Image Animation with Hybrid Guidance](https://openaccess.thecvf.com/content/ICCV2025/papers/Luo_DreamActor-M1_Holistic_Expressive_and_Robust_Human_Image_Animation_with_Hybrid_ICCV_2025_paper.pdf)
* [Ponimator Unfolding Interactive Pose for Versatile Human-human Interaction Animation](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Ponimator_Unfolding_Interactive_Pose_for_Versatile_Human-human_Interaction_Animation_ICCV_2025_paper.pdf)<br>:house:[project](https://stevenlsw.github.io/ponimator)

<a name="46"/>

## 46.Sound
* [Music Grounding by Short Video](http://arxiv.org/abs/2408.16990)
* [VGGSounder Audio-Visual Evaluations for Foundation Models](http://arxiv.org/abs/2508.08237)
* [AV-Flow Transforming Text to Audio-Visual Human-like Interactions](https://openaccess.thecvf.com/content/ICCV2025/papers/Chatziagapi_AV-Flow_Transforming_Text_to_Audio-Visual_Human-like_Interactions_ICCV_2025_paper.pdf)
* [MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing](https://arxiv.org/pdf/2507.01384v1)<br>:star:[code](https://github.com/WangLY136/MUG)
* [What's Making That Sound Right Now? Video-centric Audio-Visual Localization](https://arxiv.org/pdf/2507.04667v1)<br>:star:[code](https://hahyeon610.github.io/Video-centric_Audio_Visual_Localization/)
* [Implicit Counterfactual Learning for Audio-Visual Segmentation](https://arxiv.org/pdf/2507.20740v1)
* [Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation](https://arxiv.org/pdf/2507.22886v1)<br>:house:[project](https://henghuiding.com/OmniAVS/)
* [Zero-AVSR Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations](https://openaccess.thecvf.com/content/ICCV2025/papers/Yeo_Zero-AVSR_Zero-Shot_Audio-Visual_Speech_Recognition_with_LLMs_by_Learning_Language-Agnostic_ICCV_2025_paper.pdf)
* [Not Only Vision Evolve Visual Speech Recognition via Peripheral Information](https://openaccess.thecvf.com/content/ICCV2025/papers/Yuan_Not_Only_Vision_Evolve_Visual_Speech_Recognition_via_Peripheral_Information_ICCV_2025_paper.pdf)
* [CogCM Cognition-Inspired Contextual Modeling for Audio-Visual Speech Enhancement](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_CogCM_Cognition-Inspired_Contextual_Modeling_for_Audio-Visual_Speech_Enhancement_ICCV_2025_paper.pdf)
* [How Do Optical Flow and Textual Prompts Collaborate to Assist in Audio-Visual Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_How_Do_Optical_Flow_and_Textual_Prompts_Collaborate_to_Assist_ICCV_2025_paper.pdf)
* [TAViS Text-bridged Audio-Visual Segmentation with Foundation Models](http://arxiv.org/abs/2506.11436)
* [AV-Link Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Haji-Ali_AV-Link_Temporally-Aligned_Diffusion_Features_for_Cross-Modal_Audio-Video_Generation_ICCV_2025_paper.pdf)
* [AURELIA Test-time Reasoning Distillation in Audio-Visual LLMs](http://arxiv.org/abs/2503.23219)
* [p-AVAS Can Physics-Integrated Audio-Visual Modeling Boost Neural Acoustic Synthesis](https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_p-AVAS_Can_Physics-Integrated_Audio-Visual_Modeling_Boost_Neural_Acoustic_Synthesis_ICCV_2025_paper.pdf)
* [TARO Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning for Synchronized Video-to-Audio Synthesis](http://arxiv.org/abs/2504.05684)
* [VAFlow Video-to-Audio Generation with Cross-Modality Flow Matching](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_VAFlow_Video-to-Audio_Generation_with_Cross-Modality_Flow_Matching_ICCV_2025_paper.pdf)
* [Shot-by-Shot Film-Grammar-Aware Training-Free Audio Description Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_Shot-by-Shot_Film-Grammar-Aware_Training-Free_Audio_Description_Generation_ICCV_2025_paper.pdf)
* [AVTrustBench Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs](http://arxiv.org/abs/2501.02135)
* 合成语音检测
  * [Intra-modal and Cross-modal Synchronization for Audio-visual Deepfake Detection and Temporal Localization](https://openaccess.thecvf.com/content/ICCV2025/papers/Anshul_Intra-modal_and_Cross-modal_Synchronization_for_Audio-visual_Deepfake_Detection_and_Temporal_ICCV_2025_paper.pdf)

<a name="45"/>

## 45.Dataset
* [Context-Aware Academic Emotion Dataset and Benchmark](https://arxiv.org/pdf/2507.00586v1)<br>:star:[code](https://zgsfer.github.io/CAER)
* [ROADWork A Dataset and Benchmark for Learning to Recognize Observe Analyze and Drive Through Work Zones](https://openaccess.thecvf.com/content/ICCV2025/papers/Ghosh_ROADWork_A_Dataset_and_Benchmark_for_Learning_to_Recognize_Observe_ICCV_2025_paper.pdf)
* [4D-Bench Benchmarking Multi-modal Large Language Models for 4D Object Understanding](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_4D-Bench_Benchmarking_Multi-modal_Large_Language_Models_for_4D_Object_Understanding_ICCV_2025_paper.pdf)
* [Bias in Gender Bias Benchmarks How Spurious Features Distort Evaluation](http://arxiv.org/abs/2509.07596)
* 基准
  * [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/pdf/2507.14449v1)
  * [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/pdf/2507.15028v1)<br>:star:[code](https://zhangyuanhan-ai.github.io/video-tt/)
  * [One Object Multiple Lies A Benchmark for Cross-task Adversarial Attack on Unified Vision-Language Models](http://arxiv.org/abs/2507.07709)
  * [Beyond the Destination A Novel Benchmark for Exploration-Aware Embodied Question Answering](http://arxiv.org/abs/2503.11117)
  * [JailbreakDiffBench A Comprehensive Benchmark for Jailbreaking Diffusion Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Jin_JailbreakDiffBench_A_Comprehensive_Benchmark_for_Jailbreaking_Diffusion_Models_ICCV_2025_paper.pdf)
  * [MMReason An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI](http://arxiv.org/abs/2506.23563)
  * [GRAB A Challenging GRaph Analysis Benchmark for Large Multimodal Models](http://arxiv.org/abs/2408.11817)
  * [INS-MMBench A Comprehensive Benchmark for Evaluating LVLMs Performance in Insurance](https://openaccess.thecvf.com/content/ICCV2025/papers/Lin_INS-MMBench_A_Comprehensive_Benchmark_for_Evaluating_LVLMs_Performance_in_Insurance_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/FDU-INS/INS-MMBench)
  * [MIEB Massive Image Embedding Benchmark](http://arxiv.org/abs/2504.10471)<br>:star:[code](https://github.com/embeddings-benchmark/mteb)
  * [LVBench An Extreme Long Video Understanding Benchmark](http://arxiv.org/abs/2406.08035)
  * [ProJudge A Multi-Modal Multi-Discipline Benchmark and Instruction-Tuning Dataset for MLLM-based Process Judges](http://arxiv.org/abs/2503.06553)
  * [From Abyssal Darkness to Blinding Glare A Benchmark on Extreme Exposure Correction in Real World](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_From_Abyssal_Darkness_to_Blinding_Glare_A_Benchmark_on_Extreme_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/juvenoia/REED)
  * [Beyond Walking A Large-Scale Image-Text Benchmark for Text-based Person Anomaly Search](http://arxiv.org/abs/2411.17776)
  * [MultiVerse A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_MultiVerse_A_Multi-Turn_Conversation_Benchmark_for_Evaluating_Large_Vision_and_ICCV_2025_paper.pdf)
  * [Extrapolated Urban View Synthesis Benchmark](http://arxiv.org/abs/2412.05256)
  * [WorldScore A Unified Evaluation Benchmark for World Generation](http://arxiv.org/abs/2504.00983)<br>:house:[project](https://haoyi-duan.github.io/WorldScore)
  * [ICE-Bench A Unified and Comprehensive Benchmark for Image Creating and Editing](https://openaccess.thecvf.com/content/ICCV2025/papers/Pan_ICE-Bench_A_Unified_and_Comprehensive_Benchmark_for_Image_Creating_and_ICCV_2025_paper.pdf)
  * [MVGBench a Comprehensive Benchmark for Multi-view Generation Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_MVGBench_a_Comprehensive_Benchmark_for_Multi-view_Generation_Models_ICCV_2025_paper.pdf)
* 数据集
  * [Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning](https://arxiv.org/pdf/2507.04790v1)
  * [ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users](https://arxiv.org/pdf/2507.10223v1)<br>:star:[code](https://github.com/pittisl/ProGait)<br>:house:[project](https://huggingface.co/datasets/ericyxy98/ProGait)
  * [DiffTell A High-Quality Dataset for Describing Image Manipulation Changes](https://openaccess.thecvf.com/content/ICCV2025/papers/Di_DiffTell_A_High-Quality_Dataset_for_Describing_Image_Manipulation_Changes_ICCV_2025_paper.pdf)
  * [CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling](https://arxiv.org/pdf/2507.12591v1)
  * [Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions](https://arxiv.org/pdf/2508.04681v1)<br>:star:[code](https://liangxuy.github.io/InterVLA/)<br>:star:[code](https://github.com/liangxuy/intervla)
  * [HumanOLAT: A Large-Scale Dataset for Full-Body Human Relighting and Novel-View Synthesis](https://arxiv.org/pdf/2508.09137v1)<br>:house:[project](https://vcai.mpi-inf.mpg.de/projects/HumanOLAT/)
  * [Dataset Ownership Verification for Pre-trained Masked Models](http://arxiv.org/abs/2507.12022)<br>:star:[code](https://github.com/xieyc99/DOV4MM)
  * [Asynchronous Event Error-Minimizing Noise for Safeguarding Event Dataset](http://arxiv.org/abs/2507.05728)<br>:star:[code](https://github.com/rfww/uevs)
  * [BlueNeg A 35mm Negative Film Dataset for Restoring Channel-Heterogeneous Deterioration](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_BlueNeg_A_35mm_Negative_Film_Dataset_for_Restoring_Channel-Heterogeneous_Deterioration_ICCV_2025_paper.pdf)
  * [CMB-ML A Cosmic Microwave Background Dataset for the Oldest Possible Computer Vision Task](https://openaccess.thecvf.com/content/ICCV2025/papers/Amato_CMB-ML_A_Cosmic_Microwave_Background_Dataset_for_the_Oldest_Possible_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/CMB-ML/cmb-ml)
  * [UAVScenes A Multi-Modal Dataset for UAVs](http://arxiv.org/abs/2507.22412)<br>:star:[code](https://github.com/sijieaaa/UAVScenes)
  * [UDC-VIT A Real-World Video Dataset for Under-Display Cameras](https://openaccess.thecvf.com/content/ICCV2025/papers/Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras_ICCV_2025_paper.pdf)
  * [Towards Comprehensive Lecture Slides Understanding Large-scale Dataset and Effective Method](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Towards_Comprehensive_Lecture_Slides_Understanding_Large-scale_Dataset_and_Effective_Method_ICCV_2025_paper.pdf)
  * [R-LiViT A LiDAR-Visual-Thermal Dataset Enabling Vulnerable Road User Focused Roadside Perception](https://openaccess.thecvf.com/content/ICCV2025/papers/Mirlach_R-LiViT_A_LiDAR-Visual-Thermal_Dataset_Enabling_Vulnerable_Road_User_Focused_Roadside_ICCV_2025_paper.pdf)
  * [MEH A Multi-Style Dataset and Toolkit for Advancing Egyptian Hieroglyph Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Golyadkin_MEH_A_Multi-Style_Dataset_and_Toolkit_for_Advancing_Egyptian_Hieroglyph_ICCV_2025_paper.pdf)
  * [3DRealCar An In-the-wild RGB-D Car Dataset with 360-degree Views](https://openaccess.thecvf.com/content/ICCV2025/papers/Du_3DRealCar_An_In-the-wild_RGB-D_Car_Dataset_with_360-degree_Views_ICCV_2025_paper.pdf)
  * [PBFG A New Physically-Based Dataset and Removal of Lens Flares and Glares](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_PBFG_A_New_Physically-Based_Dataset_and_Removal_of_Lens_Flares_ICCV_2025_paper.pdf)
  * [Feature Coding in the Era of Large Models Dataset Test Conditions and Benchmark](http://arxiv.org/abs/2412.04307)<br>:star:[code](https://github.com/chansongoal/LaMoFC)
  * [Modeling Saliency Dataset Bias](https://openaccess.thecvf.com/content/ICCV2025/papers/Kummerer_Modeling_Saliency_Dataset_Bias_ICCV_2025_paper.pdf)
  * [TrackVerse A Large-Scale Object-Centric Video Dataset for Image-Level Representation Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Wei_TrackVerse_A_Large-Scale_Object-Centric_Video_Dataset_for_Image-Level_Representation_Learning_ICCV_2025_paper.pdf)
  * [OpenSubstance A High-quality Measured Dataset of Multi-View and -Lighting Images and Shapes](https://openaccess.thecvf.com/content/ICCV2025/papers/Pei_OpenSubstance_A_High-quality_Measured_Dataset_of_Multi-View_and_-Lighting_Images_ICCV_2025_paper.pdf)<br>:house:[project](https://opensubstance.github.io/)
  * [MMAT-1M A Large Reasoning Dataset for Multimodal Agent Tuning](https://openaccess.thecvf.com/content/ICCV2025/papers/Gao_MMAT-1M_A_Large_Reasoning_Dataset_for_Multimodal_Agent_Tuning_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/VIS-MPU-Agent/MMAT-1M)
  * [ImageGem In-the-wild Generative Image Interaction Dataset for Generative Model Personalization](https://openaccess.thecvf.com/content/ICCV2025/papers/Guo_ImageGem_In-the-wild_Generative_Image_Interaction_Dataset_for_Generative_Model_Personalization_ICCV_2025_paper.pdf)
  * [LANGTRAJ Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation](http://arxiv.org/abs/2504.11521)<br>:house:[project](https://langtraj.github.io/)
  * [LightCity An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_LightCity_An_Urban_Dataset_for_Outdoor_Inverse_Rendering_and_Reconstruction_ICCV_2025_paper.pdf)
  * [CULTURE3D A Large-Scale and Diverse Dataset of Cultural Landmarks and Terrains for Gaussian-Based Scene Rendering](http://arxiv.org/abs/2501.06927)
  * [A Real-world Display Inverse Rendering Dataset](http://arxiv.org/abs/2508.14411)<br>:house:[project](https://michaelcsj.github.io/DIR)
* 数据蒸馏
  * [CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation](https://arxiv.org/pdf/2506.22637v1)<br>:star:[code](https://github.com/hatchetProject/CaO2)
  * [Dataset Distillation via Vision-Language Category Prototype](https://arxiv.org/pdf/2506.23580v1)<br>:star:[code](https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/)
  * [Dataset Distillation as Data Compression: A Rate-Utility Perspective](https://arxiv.org/pdf/2507.17221v1)
  * [Heavy Labels Out Dataset Distillation with Label Space Lightening](http://arxiv.org/abs/2408.08201)<br>:star:[code](https://github.com/Lexie-YU/HeLlO)
  * [Dataset Distillation via the Wasserstein Metric](http://arxiv.org/abs/2311.18531)<br>:star:[code](https://github.com/Liu-Hy/WMDD) :house:[project](https://liu-hy.github.io/WMDD)
  * [Diversity-Enhanced Distribution Alignment for Dataset Distillation](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Diversity-Enhanced_Distribution_Alignment_for_Dataset_Distillation_ICCV_2025_paper.pdf)
  * [Improving Noise Efficiency in Privacy-preserving Dataset Distillation](http://arxiv.org/abs/2508.01749)

<a name="44"/>

## 44.Neural Radiance Fields
* [UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields](http://arxiv.org/pdf/2506.21884v1)<br>:house:[project](https://www.factral.co/UnMix-NeRF)
* [LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling](https://arxiv.org/pdf/2507.02363v1)<br>:star:[code](https://wujh2001.github.io/LocalDyGS/)
* [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/pdf/2507.14596v1)
* [A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields](https://arxiv.org/pdf/2507.04408v1)
* [NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement](https://arxiv.org/pdf/2507.12714v1)<br>:star:[code](https://neuraleaf-yang.github.io/)
* [MuGS Multi-Baseline Generalizable Gaussian Splatting Reconstruction](http://arxiv.org/abs/2508.04297)<br>:star:[code](https://github.com/EuclidLou/MuGS)
* [UniVerse Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](http://arxiv.org/abs/2510.01669)
* 渲染
  * [BokehDiff: Neural Lens Blur with One-Step Diffusion](https://arxiv.org/pdf/2507.18060v1)
  * [OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene Reconstruction and Rendering](http://arxiv.org/abs/2503.16177)<br>:house:[project](https://occlugaussian.github.io)
  * [ReCamMaster Camera-Controlled Generative Rendering from A Single Video](http://arxiv.org/abs/2503.11647)
  * [Leveraging 2D Priors and SDF Guidance for Urban Scene Rendering](https://openaccess.thecvf.com/content/ICCV2025/papers/Tourani_Leveraging_2D_Priors_and_SDF_Guidance_for_Urban_Scene_Rendering_ICCV_2025_paper.pdf)
  * [Bokehlicious Photorealistic Bokeh Rendering with Controllable Apertures](http://arxiv.org/abs/2503.16067)
  * [UNIS A Unified Framework for Achieving Unbiased Neural Implicit Surfaces in Volume Rendering](https://openaccess.thecvf.com/content/ICCV2025/papers/Deng_UNIS_A_Unified_Framework_for_Achieving_Unbiased_Neural_Implicit_Surfaces_ICCV_2025_paper.pdf)
  * [Stochastic Gradient Estimation for Higher-Order Differentiable Rendering](http://arxiv.org/abs/2412.03489)
  * [Learning Null Geodesics for Gravitational Lensing Rendering in General Relativity](http://arxiv.org/abs/2507.15775)
  * [FonTS Text Rendering With Typography and Style Controls](http://arxiv.org/abs/2412.00136)
  * [Differentiable Room Acoustic Rendering with Multi-View Vision Priors](http://arxiv.org/abs/2504.21847)
* 逆向渲染
  * [Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues](https://arxiv.org/pdf/2507.23162v1)
  * [Ouroboros Single-step Diffusion Models for Cycle-consistent Forward and Inverse Rendering](http://arxiv.org/abs/2508.14461)
  * [Neural Inverse Rendering for High-Accuracy 3D Measurement of Moving Objects with Fewer Phase-Shifting Patterns](https://openaccess.thecvf.com/content/ICCV2025/papers/Urakawa_Neural_Inverse_Rendering_for_High-Accuracy_3D_Measurement_of_Moving_Objects_ICCV_2025_paper.pdf)
  * [InvRGB+L: Inverse Rendering of Complex Scenes with Unified Color and LiDAR Reflectance Modeling](https://arxiv.org/pdf/2507.17613v1)
  * [DNF-Intrinsic Deterministic Noise-Free Diffusion for Indoor Inverse Rendering](https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_DNF-Intrinsic_Deterministic_Noise-Free_Diffusion_for_Indoor_Inverse_Rendering_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/OnlyZZZZ/DNF-Intrinsic)
* NVS
  * [FVGen Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation](http://arxiv.org/abs/2508.06392)
  * [E-NeMF Event-based Neural Motion Field for Novel Space-time View Synthesis of Dynamic Scenes](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_E-NeMF_Event-based_Neural_Motion_Field_for_Novel_Space-time_View_Synthesis_ICCV_2025_paper.pdf)
  * [Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis](http://arxiv.org/abs/2411.00144)<br>:house:[project](https://sailor-z.github.io/projects)
  * [RayZer A Self-supervised Large View Synthesis Model](http://arxiv.org/abs/2505.00702)
  * [BillBoard Splatting (BBSplat) Learnable Textured Primitives for Novel View Synthesis](http://arxiv.org/abs/2411.08508)
  * [WAVE Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image](http://arxiv.org/abs/2506.23518)
  * [UniGS Modeling Unitary 3D Gaussians for Novel View Synthesis from Sparse-view Images](http://arxiv.org/abs/2410.13195)<br>:star:[code](https://github.com/jwubz123/UNIG)
  * [Scaling Transformer-Based Novel View Synthesis with Models Token Disentanglement and Synthetic Data](https://openaccess.thecvf.com/content/ICCV2025/papers/Nair_Scaling_Transformer-Based_Novel_View_Synthesis_with_Models_Token_Disentanglement_and_ICCV_2025_paper.pdf)
  * [SEHDR Single-Exposure HDR Novel View Synthesis via 3D Gaussian Bracketing](http://arxiv.org/abs/2509.20400)
  * [RayGaussX Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis](http://arxiv.org/abs/2509.07782)

<a name="43"/>

## 43.Vision Language(视觉语言)
* [Improving Large Vision and Language Models by Learning from a Panel of Peers](http://arxiv.org/abs/2509.01610)
* [DASH Detection and Assessment of Systematic Hallucinations of VLMs](http://arxiv.org/abs/2503.23573)
* [Vision-Language Models Cant See the Obvious](https://openaccess.thecvf.com/content/ICCV2025/papers/Huynh_Vision-Language_Models_Cant_See_the_Obvious_ICCV_2025_paper.pdf)
* [Web Artifact Attacks Disrupt Vision Language Models](http://arxiv.org/abs/2503.13652)<br>:star:[code](https://github.com/mqraitem/Web-Artifact-Attacks)
* [ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models](https://arxiv.org/pdf/2507.00898v1)<br>:star:[code](https://github.com/zifuwan/ONLY)<br>:star:[code](https://zifuwan.github.io/ONLY/)
* [VLM4D Towards Spatiotemporal Awareness in Vision Language Models](http://arxiv.org/abs/2508.02095)
* [WalkVLM Aid Visually Impaired People Walking by Vision Language Model](http://arxiv.org/abs/2412.20903)
* [ViLU: Learning Vision-Language Uncertainties for Failure Prediction](https://arxiv.org/pdf/2507.07620v1)<br>:star:[code](https://github.com/ykrmm/ViLU)
* [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/pdf/2507.08979v1)<br>:star:[code](https://github.com/MahdiyarMM/PRISM)
* [One Last Attention for Your Vision-Language Model](https://arxiv.org/pdf/2507.15480v1)<br>:star:[code](https://github.com/khufia/RAda/tree/main)
* [Hierarchical Cross-modal Prompt Learning for Vision-Language Models](https://arxiv.org/pdf/2507.14976v1)<br>:star:[code](https://github.com/zzeoZheng/HiCroPL)
* [METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models](https://arxiv.org/pdf/2507.20842v1)<br>:star:[code](https://github.com/YuchenLiu98/METEOR)
* [ATCTrack: Aligning Target-Context Cues with Dynamic Target States for Robust Vision-Language Tracking](https://arxiv.org/pdf/2507.19875v1)<br>:star:[code](https://github.com/XiaokunFeng/ATCTrack)
* [AgroBench: Vision-Language Model Benchmark in Agriculture](https://arxiv.org/pdf/2507.20519v1)<br>:star:[code](https://dahlian00.github.io/AgroBenchPage/)
* [MM-IFEngine Towards Multimodal Instruction Following](https://openaccess.thecvf.com/content/ICCV2025/papers/Ding_MM-IFEngine_Towards_Multimodal_Instruction_Following_ICCV_2025_paper.pdf)
* [Robustifying Zero-Shot Vision Language Models by Subspaces Alignment](https://openaccess.thecvf.com/content/ICCV2025/papers/Dong_Robustifying_Zero-Shot_Vision_Language_Models_by_Subspaces_Alignment_ICCV_2025_paper.pdf)
* [FDPT Federated Discrete Prompt Tuning for Black-Box Visual-Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_FDPT_Federated_Discrete_Prompt_Tuning_for_Black-Box_Visual-Language_Models_ICCV_2025_paper.pdf)
* [Griffon v2 Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring](http://arxiv.org/abs/2403.09333)<br>:star:[code](https://github.com/jefferyZhan/Griffon)
* [CLIP-GS Unifying Vision-Language Representation with 3D Gaussian Splatting](https://openaccess.thecvf.com/content/ICCV2025/papers/Jiao_CLIP-GS_Unifying_Vision-Language_Representation_with_3D_Gaussian_Splatting_ICCV_2025_paper.pdf)
* [Growing a Twig to Accelerate Large Vision-Language Models](http://arxiv.org/abs/2503.14075)
* [Test-Time Retrieval-Augmented Adaptation for Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_Test-Time_Retrieval-Augmented_Adaptation_for_Vision-Language_Models_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/xinqi-fan/TT-RAA)
* [Understanding Museum Exhibits using Vision-Language Reasoning](http://arxiv.org/abs/2412.01370)
* [One Perturbation is Enough On Generating Universal Adversarial Perturbations against Vision-Language Pre-training Models](http://arxiv.org/abs/2406.05491)
* [When Lighting Deceives Exposing Vision-Language Models Illumination Vulnerability Through Illumination Transformation Attack](http://arxiv.org/abs/2503.06903)
* [Target Bias Is All You Need Zero-Shot Debiasing of Vision-Language Models with Bias Corpus](https://openaccess.thecvf.com/content/ICCV2025/papers/Jang_Target_Bias_Is_All_You_Need_Zero-Shot_Debiasing_of_Vision-Language_ICCV_2025_paper.pdf)
* [TAB Transformer Attention Bottlenecks enable User Intervention and Debugging in Vision-Language Models](http://arxiv.org/abs/2412.18675)
* [Feather the Throttle Revisiting Visual Token Pruning for Vision-Language Model Acceleration](http://arxiv.org/abs/2412.13180)
* [Derm1M A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology](http://arxiv.org/abs/2503.14911)<br>:star:[code](https://github.com/SiyuanYan1/Derm1M)
* [ReCoT Reflective Self-Correction Training for Mitigating Confirmation Bias in Large Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Qu_ReCoT_Reflective_Self-Correction_Training_for_Mitigating_Confirmation_Bias_in_Large_ICCV_2025_paper.pdf)
* [AutoOcc Automatic Open-Ended Semantic Occupancy Annotation via Vision-Language Guided Gaussian Splatting](http://arxiv.org/abs/2502.04981)
* [D-Attn Decomposed Attention for Large Vision-and-Language Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Kuo_D-Attn_Decomposed_Attention_for_Large_Vision-and-Language_Model_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/bytedance/DecomposedAttention)
* [Deciphering Cross-Modal Alignment in Large Vision-Language Models via Modality Integration Rate](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_Deciphering_Cross-Modal_Alignment_in_Large_Vision-Language_Models_via_Modality_Integration_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/shikiw/Modality-Integration-Rate)
* [Fuzzy Contrastive Decoding to Alleviate Object Hallucination in Large Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_Fuzzy_Contrastive_Decoding_to_Alleviate_Object_Hallucination_in_Large_Vision-Language_ICCV_2025_paper.pdf)
* [IDEATOR Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves](http://arxiv.org/abs/2411.00827)
* [25 Years in Class A Multimodal Textbook for Vision-Language Pretraining](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining_ICCV_2025_paper.pdf)
* [Enhancing Few-Shot Vision-Language Classification with Large Multimodal Model Features](http://arxiv.org/abs/2412.00142)
* [FedMVP Federated Multimodal Visual Prompt Tuning for Vision-Language Models](http://arxiv.org/abs/2504.20860)<br>:star:[code](https://github.com/mainaksingha01/FedMVP)
* [Physics Context Builders A Modular Framework for Physical Reasoning in Vision-Language Models](http://arxiv.org/abs/2412.08619)
* [VLRMBench A Comprehensive and Challenging Benchmark for Vision-Language Reward Models](http://arxiv.org/abs/2503.07478)<br>:star:[code](https://github.com/JCruan519/VLRMBench)
* [ZipVL Accelerating Vision-Language Models through Dynamic Token Sparsity](https://openaccess.thecvf.com/content/ICCV2025/papers/He_ZipVL_Accelerating_Vision-Language_Models_through_Dynamic_Token_Sparsity_ICCV_2025_paper.pdf)
* [Skip-Vision Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping](https://openaccess.thecvf.com/content/ICCV2025/papers/Zeng_Skip-Vision_Efficient_and_Scalable_Acceleration_of_Vision-Language_Models_via_Adaptive_ICCV_2025_paper.pdf)
* [SAUCE Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders](http://arxiv.org/abs/2503.14530)
* [The Inter-Intra Modal Measure A Predictive Lens on Fine-Tuning Outcomes in Vision-Language Models](http://arxiv.org/abs/2407.15731)<br>:star:[code](https://github.com/mit-ll/IIMM)
* [MaTVLM Hybrid Mamba-Transformer for Efficient Vision-Language Modeling](http://arxiv.org/abs/2503.13440)<br>:star:[code](https://github.com/hustvl/MaTVLM)
* [Safeguarding Vision-Language Models Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks](http://arxiv.org/abs/2504.01308)<br>:star:[code](https://github.com/JarvisUSTC/DiffPure-RobustVLM)
* [Dynamic Multimodal Prototype Learning in Vision-Language Models](http://arxiv.org/abs/2507.03657)
* [GEOBench-VLM Benchmarking Vision-Language Models for Geospatial Tasks](https://openaccess.thecvf.com/content/ICCV2025/papers/Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks_ICCV_2025_paper.pdf)
* [Towards Cross-modal Backward-compatible Representation Learning for Vision-Language Models](http://arxiv.org/abs/2405.14715)
* [V2PE Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding](http://arxiv.org/abs/2412.09616)
* [DexVLG Dexterous Vision-Language-Grasp Model at Scale](http://arxiv.org/abs/2507.02747)
* [Vision-Language Neural Graph Featurization for Extracting Retinal Lesions](https://openaccess.thecvf.com/content/ICCV2025/papers/Hassan_Vision-Language_Neural_Graph_Featurization_for_Extracting_Retinal_Lesions_ICCV_2025_paper.pdf)
* [MotionCtrl A Real-time Controllable Vision-Language-Motion Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Cao_MotionCtrl_A_Real-time_Controllable_Vision-Language-Motion_Model_ICCV_2025_paper.pdf)
* [Breaking the Encoder Barrier for Seamless Video-Language Understanding](http://arxiv.org/abs/2503.18422)
* [OphCLIP Hierarchical Retrieval-Augmented Learning for Ophthalmic Surgical Video-Language Pretraining](http://arxiv.org/abs/2411.15421)
* [How Can Objects Help Video-Language Understanding](http://arxiv.org/abs/2504.07454)<br>:star:[code](https://github.com/brown-palm/ObjectMLLM)
* [Factorized Learning for Temporally Grounded Video-Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Zeng_Factorized_Learning_for_Temporally_Grounded_Video-Language_Models_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/nusnlp/d2vlm)
* [Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models](http://arxiv.org/abs/2508.01225)
* [AdvDreamer Unveils Are Vision-Language Models Truly Ready for Real-World 3D Variations](http://arxiv.org/abs/2412.03002)
* [HQ-CLIP Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Wei_HQ-CLIP_Leveraging_Large_Vision-Language_Models_to_Create_High-Quality_Image-Text_Datasets_ICCV_2025_paper.pdf)
* [Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation](http://arxiv.org/abs/2504.17207)
* [The Scalability of Simplicity Empirical Analysis of Vision-Language Learning with a Single Transformer](http://arxiv.org/abs/2504.10462)<br>:star:[code](https://github.com/bytedance/SAIL)
* [EVEv2 Improved Baselines for Encoder-Free Vision-Language Models](http://arxiv.org/abs/2502.06788)<br>:star:[code](https://github.com/baaivision/EVE)
* [TruthPrInt Mitigating Large Vision-Language Models Object Hallucination Via Latent Truthful-Guided Pre-Intervention](https://openaccess.thecvf.com/content/ICCV2025/papers/Duan_TruthPrInt_Mitigating_Large_Vision-Language_Models_Object_Hallucination_Via_Latent_Truthful-Guided_ICCV_2025_paper.pdf)
* [Structured Policy Optimization Enhance Large Vision-Language Model via Self-referenced Dialogue](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Structured_Policy_Optimization_Enhance_Large_Vision-Language_Model_via_Self-referenced_Dialogue_ICCV_2025_paper.pdf)
* [Causality-guided Prompt Learning for Vision-language Models via Visual Granulation](http://arxiv.org/abs/2509.03803)<br>:star:[code](https://github.com/GaoMY-521/CaPL_Code)
* [CalliReader Contextualizing Chinese Calligraphy via an Embedding-Aligned Vision-Language Model](http://arxiv.org/abs/2503.06472)
* [Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma](http://arxiv.org/abs/2503.12496)
* [Normal and Abnormal Pathology Knowledge-Augmented Vision-Language Model for Anomaly Detection in Pathology Images](http://arxiv.org/abs/2508.15256)
* [Uncertainty-Driven Expert Control Enhancing the Reliability of Medical Vision-Language Models](http://arxiv.org/abs/2507.09209)
* [Dynamic Multi-Layer Null Space Projection for Vision-Language Continual Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Kang_Dynamic_Multi-Layer_Null_Space_Projection_for_Vision-Language_Continual_Learning_ICCV_2025_paper.pdf)
* [Learning Beyond Still Frames Scaling Vision-Language Models with Video](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Learning_Beyond_Still_Frames_Scaling_Vision-Language_Models_with_Video_ICCV_2025_paper.pdf)
* [GLEAM Enhanced Transferable Adversarial Attacks for Vision-Language Pre-training Models via Global-Local Transformations](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_GLEAM_Enhanced_Transferable_Adversarial_Attacks_for_Vision-Language_Pre-training_Models_via_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/LuckAlex/GLEAM)
* [INTER Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling](http://arxiv.org/abs/2507.05056)<br>:star:[code](https://github.com/xxxxx313/INTER)
* [SmolDocling An ultra-compact vision-language model for end-to-end multi-modal document conversion](http://arxiv.org/abs/2503.11576)
* VLN
  * [Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities](https://arxiv.org/pdf/2507.13019v1)<br>:star:[code](https://crystalsixone.github.io/vln_pe.github.io/)
  * [monoVLN Bridging the Observation Gap between Monocular and Panoramic Vision and Language Navigation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_monoVLN_Bridging_the_Observation_Gap_between_Monocular_and_Panoramic_Vision_ICCV_2025_paper.pdf)
  * [NavQ Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_NavQ_Learning_a_Q-Model_for_Foresighted_Vision-and-Language_Navigation_ICCV_2025_paper.pdf)
  * [COSMO Combination of Selective Memorization for Low-cost Vision-and-Language Navigation](http://arxiv.org/abs/2503.24065)<br>:star:[code](https://github.com/siqiZ805/VLN-COSMO.git)
  * [NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/pdf/2506.23468v1)<br>:star:[code](https://github.com/Feliciaxyao/NavMorph)
  * [3D Gaussian Map with Open-Set Semantic Grouping for Vision-Language Navigation](https://openaccess.thecvf.com/content/ICCV2025/papers/Gao_3D_Gaussian_Map_with_Open-Set_Semantic_Grouping_for_Vision-Language_Navigation_ICCV_2025_paper.pdf)
* LLM
  * [LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching](https://arxiv.org/pdf/2506.23502v1)
  * [Aligning Information Capacity Between Vision and Language via Dense-to-Sparse Feature Distillation for Image-Text Matching](http://arxiv.org/abs/2503.14953)
  * [Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs](https://arxiv.org/pdf/2507.07990v1)<br>:house:[project](https://www.jshyun.me/projects/sttm)
  * [Why LVLMs Are More Prone to Hallucinations in Longer Responses The Role of Context](https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_Why_LVLMs_Are_More_Prone_to_Hallucinations_in_Longer_Responses_ICCV_2025_paper.pdf)
  * [Zeroth-Order Fine-Tuning of LLMs in Random Subspaces](http://arxiv.org/abs/2410.08989)<br>:star:[code](https://github.com/zimingyy/SubZero)
  * [Advancing Visual Large Language Model for Multi-granular Versatile Perception](https://arxiv.org/pdf/2507.16213v1)<br>:star:[code](https://github.com/xiangwentao666/MVP-LM)
  * [DisTime Distribution-based Time Representation for Video Large Language Models](http://arxiv.org/abs/2505.24329)<br>:star:[code](https://github.com/josephzpng/DisTime)
  * [Aligning Effective Tokens with Video Anomaly in Large Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Aligning_Effective_Tokens_with_Video_Anomaly_in_Large_Language_Models_ICCV_2025_paper.pdf)
  * [MeshLLM Empowering Large Language Models to Progressively Understand and Generate 3D Mesh](http://arxiv.org/abs/2508.01242)
  * [FOLDER Accelerating Multi-Modal Large Language Models with Enhanced Performance](http://arxiv.org/abs/2501.02430)<br>:star:[code](https://github.com/anakin-skywalker-Joseph/Folder)
  * [B-VLLM A Vision Large Language Model with Balanced Spatio-Temporal Tokens](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_B-VLLM_A_Vision_Large_Language_Model_with_Balanced_Spatio-Temporal_Tokens_ICCV_2025_paper.pdf)
  * [Robin3D Improving 3D Large Language Model via Robust Instruction Tuning](http://arxiv.org/abs/2410.00255)
  * [GenieBlue Integrating both Linguistic and Multimodal Capabilities for Large Language Models on Mobile Devices](http://arxiv.org/abs/2503.06019)
  * [CATP-LLM Empowering Large Language Models for Cost-Aware Tool Planning](https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_CATP-LLM_Empowering_Large_Language_Models_for_Cost-Aware_Tool_Planning_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/duowuyms/OpenCATP-LLM)
  * [Multimodal LLM Guided Exploration and Active Mapping using Fisher Information](http://arxiv.org/abs/2410.17422)
  * [Multimodal Large Language Model-Guided ISP Hyperparameter Optimization with Dynamic Preference Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Multimodal_Large_Language_Model-Guided_ISP_Hyperparameter_Optimization_with_Dynamic_Preference_ICCV_2025_paper.pdf)
  * [Aligning Vision to Language Annotation-Free Multimodal Knowledge Graph Construction for Enhanced LLMs Reasoning](http://arxiv.org/abs/2503.12972)<br>:star:[code](https://github.com/Wings-Of-Disaster/VaLiK)
* MLLM
  * [Token Activation Map to Visually Explain Multimodal LLMs](http://arxiv.org/abs/2506.23270)<br>:star:[code](https://github.com/xmed-lab/TAM)
  * [DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs](https://arxiv.org/pdf/2507.10302v1)<br>:star:[code](https://github.com/ZJHTerry18/DisCo)
  * [UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding](https://arxiv.org/pdf/2506.23219v1)<br>:star:[code](https://github.com/tsinghua-fib-lab/UrbanLLaVA)
  * [Kestrel 3D Multimodal LLM for Part-Aware Grounded Description](http://arxiv.org/abs/2405.18937)
  * [Are They the Same Exploring Visual Correspondence Shortcomings of Multimodal LLMs](http://arxiv.org/abs/2501.04670)
  * [Analyzing Finetuning Representation Shift for Multimodal LLMs Steering](http://arxiv.org/abs/2501.03012)
  * [Visual Chronicles Using Multimodal LLMs to Analyze Massive Collections of Images](http://arxiv.org/abs/2504.08727)
  * [Controlling Multimodal LLMs via Reward-guided Decoding](https://openaccess.thecvf.com/content/ICCV2025/papers/Manas_Controlling_Multimodal_LLMs_via_Reward-guided_Decoding_ICCV_2025_paper.pdf)
  * [TWIST  SCOUT Grounding Multimodal LLM-Experts by Forget-Free Tuning](http://arxiv.org/abs/2410.10491)
  * [FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging](https://arxiv.org/pdf/2508.04625v1)
  * [Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation](https://arxiv.org/pdf/2507.02859v1)
  * [BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models](https://arxiv.org/pdf/2508.06895v1)
  * [Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning](https://arxiv.org/pdf/2507.07424v1)<br>:star:[code](https://mm-vl.github.io/corvid)
  * [Instruction-Oriented Preference Alignment for Enhancing Multi-Modal Comprehension Capability of MLLMs](http://arxiv.org/abs/2503.20309)
  * [CompCap Improving Multimodal Large Language Models with Composite Captions](http://arxiv.org/abs/2412.05243)
  * [AVAM a Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering](https://openaccess.thecvf.com/content/ICCV2025/papers/Zeng_AVAM_a_Universal_Training-free_Adaptive_Visual_Anchoring_Embedded_into_Multimodal_ICCV_2025_paper.pdf)
  * [How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning Placing Them in An Extensible Escape Game](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_How_Do_Multimodal_Large_Language_Models_Handle_Complex_Multimodal_Reasoning_ICCV_2025_paper.pdf)
  * [LLaVA-KD A Framework of Distilling Multimodal Large Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Cai_LLaVA-KD_A_Framework_of_Distilling_Multimodal_Large_Language_Models_ICCV_2025_paper.pdf)
  * [LIRA Reasoning Reconstruction via Multimodal Large Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_LIRA_Reasoning_Reconstruction_via_Multimodal_Large_Language_Models_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/zhen6618/LIRA)
  * [MissRAG Addressing the Missing Modality Challenge in Multimodal Large Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Pipoli_MissRAG_Addressing_the_Missing_Modality_Challenge_in_Multimodal_Large_Language_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/aimagelab/MissRAG)
  * [Visual-Oriented Fine-Grained Knowledge Editing for MultiModal Large Language Models](http://arxiv.org/abs/2411.12790)<br>:star:[code](https://github.com/zeng-zhen/FGVEdit)
  * [Benchmarking Multimodal Large Language Models Against Image Corruptions](https://openaccess.thecvf.com/content/ICCV2025/papers/Qiu_Benchmarking_Multimodal_Large_Language_Models_Against_Image_Corruptions_ICCV_2025_paper.pdf)
  * [SHIFT Smoothing Hallucinations by Information Flow Tuning for Multimodal Large Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_SHIFT_Smoothing_Hallucinations_by_Information_Flow_Tuning_for_Multimodal_Large_ICCV_2025_paper.pdf)
  * [Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency](http://arxiv.org/abs/2501.04931)
  * [VisNumBench Evaluating Number Sense of Multimodal Large Language Models](http://arxiv.org/abs/2503.14939)
  * [ShortV Efficient Multimodal Large Language Models by Freezing Visual Tokens in Ineffective Layers](http://arxiv.org/abs/2504.00502)<br>:star:[code](https://github.com/icip-cas/ShortV)
  * [Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models](http://arxiv.org/abs/2412.05934)<br>:star:[code](https://github.com/MaTengSYSU/HIMRD-jailbreak)
  * [Learning to Inference Adaptively for Multimodal Large Language Models](http://arxiv.org/abs/2503.10905)
  * [FALCON Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers](http://arxiv.org/abs/2501.16297)
  * [R1-VL Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_R1-VL_Learning_to_Reason_with_Multimodal_Large_Language_Models_via_ICCV_2025_paper.pdf)
  * [Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles](https://openaccess.thecvf.com/content/ICCV2025/papers/Slyman_Calibrating_MLLM-as-a-judge_via_Multimodal_Bayesian_Prompt_Ensembles_ICCV_2025_paper.pdf)
  * [Boosting MLLM Reasoning with Text-Debiased Hint-GRPO](http://arxiv.org/abs/2503.23905)<br>:star:[code](https://github.com/hqhQAQ/Hint-GRPO)
  * [Information Density Principle for MLLM Benchmarks](http://arxiv.org/abs/2503.10079)
  * [Auto-Controlled Image Perception in MLLMs via Visual Perception Tokens](https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_Auto-Controlled_Image_Perception_in_MLLMs_via_Visual_Perception_Tokens_ICCV_2025_paper.pdf)
  * [VSP Diagnosing the Dual Challenges of Perception and Reasoning in Spatial Planning Tasks for MLLMs](https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_VSP_Diagnosing_the_Dual_Challenges_of_Perception_and_Reasoning_in_ICCV_2025_paper.pdf)
  * [MM-Spatial Exploring 3D Spatial Understanding in Multimodal LLMs](https://openaccess.thecvf.com/content/ICCV2025/papers/Daxberger_MM-Spatial_Exploring_3D_Spatial_Understanding_in_Multimodal_LLMs_ICCV_2025_paper.pdf)
  * [Spatial Preference Rewarding for MLLMs Spatial Understanding](https://openaccess.thecvf.com/content/ICCV2025/papers/Qiu_Spatial_Preference_Rewarding_for_MLLMs_Spatial_Understanding_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/hanqiu-hq/SPR)
  * [SparseMM Head Sparsity Emerges from Visual Concept Responses in MLLMs](http://arxiv.org/abs/2506.05344)<br>:star:[code](https://github.com/CR400AF-A/SparseMM)
  * [OrderChain Towards General Instruct-Tuning for Stimulating the Ordinal Understanding Ability of MLLM](http://arxiv.org/abs/2504.04801)<br>:house:[project](https://order-chain.github.io/)
  * [STI-Bench Are MLLMs Ready for Precise Spatial-Temporal World Understanding](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_STI-Bench_Are_MLLMs_Ready_for_Precise_Spatial-Temporal_World_Understanding_ICCV_2025_paper.pdf)
  * [ChartPoint Guiding MLLMs with Grounding Reflection for Chart Reasoning](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_ChartPoint_Guiding_MLLMs_with_Grounding_Reflection_for_Chart_Reasoning_ICCV_2025_paper.pdf)
  * [Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning](http://arxiv.org/abs/2507.17539)<br>:star:[code](https://github.com/MeteorElf/FundusExpert)
  * [p-MoD Building Mixture-of-Depths MLLMs via Progressive Ratio Decay](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_p-MoD_Building_Mixture-of-Depths_MLLMs_via_Progressive_Ratio_Decay_ICCV_2025_paper.pdf)
  * [LLaVA-SP Enhancing Visual Representation with Visual Spatial Tokens for MLLMs](https://openaccess.thecvf.com/content/ICCV2025/papers/Lou_LLaVA-SP_Enhancing_Visual_Representation_with_Visual_Spatial_Tokens_for_MLLMs_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/CnFaker/LLaVA-SP)
  * [Enhancing Numerical Prediction of MLLMs with Soft Labeling](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Enhancing_Numerical_Prediction_of_MLLMs_with_Soft_Labeling_ICCV_2025_paper.pdf)
  * [Creation-MMBench Assessing Context-Aware Creative Intelligence in MLLMs](https://openaccess.thecvf.com/content/ICCV2025/papers/Fang_Creation-MMBench_Assessing_Context-Aware_Creative_Intelligence_in_MLLMs_ICCV_2025_paper.pdf)
* Visual Grounding
  * [PropVG End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination](http://arxiv.org/abs/2509.04833)
  * [Move to Understand a 3D Scene Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation](http://arxiv.org/abs/2507.04047)
  * [MC-Bench A Benchmark for Multi-Context Visual Grounding in the Era of MLLMs](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_MC-Bench_A_Benchmark_for_Multi-Context_Visual_Grounding_in_the_Era_ICCV_2025_paper.pdf)<br>:house:[project](https://xuyunqiu.github.io/MC-Bench)
  * [AerialVG A Challenging Benchmark for Aerial Visual Grounding by Exploring Positional Relations](http://arxiv.org/abs/2504.07836)<br>:star:[code](https://github.com/Ideal-ljl/AerialVG)
  * [NAVER A Neuro-Symbolic Compositional Automaton for Visual Grounding with Explicit Logic Reasoning](http://arxiv.org/abs/2502.00372)<br>:star:[code](https://github.com/ControlNet/NAVER)
  * [VGMamba Attribute-to-Location Clue Reasoning for Quantity-Agnostic 3D Visual Grounding](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_VGMamba_Attribute-to-Location_Clue_Reasoning_for_Quantity-Agnostic_3D_Visual_Grounding_ICCV_2025_paper.pdf)
  * [Region-aware Anchoring Mechanism for Efficient Referring Visual Grounding](https://openaccess.thecvf.com/content/ICCV2025/papers/Ouyang_Region-aware_Anchoring_Mechanism_for_Efficient_Referring_Visual_Grounding_ICCV_2025_paper.pdf)
* REC
  * [Referring Expression Comprehension for Small Objects](http://arxiv.org/abs/2510.03701)
  * [Leveraging Debiased Cross-modal Attention Maps and Code-based Reasoning for Zero-shot Referring Expression Comprehension](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Leveraging_Debiased_Cross-modal_Attention_Maps_and_Code-based_Reasoning_for_Zero-shot_ICCV_2025_paper.pdf)

<a name="42"/>

## 42.Vision Transformer
* [Boosting Generative Adversarial Transferability with Self-supervised Vision Transformer Features](http://arxiv.org/pdf/2506.21046v1)<br>:star:[code](https://github.com/spencerwooo/dSVA)
* [Efficient Adaptation of Pre-trained Vision Transformer underpinned by Approximately Orthogonal Fine-Tuning Strategy](https://arxiv.org/pdf/2507.13260v1)
* [EA-ViT: Efficient Adaptation for Elastic Vision Transformer](https://arxiv.org/pdf/2507.19360v1)<br>:star:[code](https://github.com/zcxcf/EA-ViT)
* [MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective](https://arxiv.org/pdf/2507.19131v1)
* [OminiControl Minimal and Universal Control for Diffusion Transformer](http://arxiv.org/abs/2411.15098)
* [Pinco Position-induced Consistent Adapter for Diffusion Transformer in Foreground-conditioned Inpainting](http://arxiv.org/abs/2412.03812)
* [SAFER Sharpness Aware layer-selective Finetuning for Enhanced Robustness in vision transformers](http://arxiv.org/abs/2501.01529)
* [OmniCache A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Chu_OmniCache_A_Trajectory-Oriented_Global_Perspective_on_Training-Free_Cache_Reuse_for_ICCV_2025_paper.pdf)
* [Sparse Fine-Tuning of Transformers for Generative Tasks](http://arxiv.org/abs/2507.10855)
* [MaTe Images Are All You Need for Material Transfer via Diffusion Transformer](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_MaTe_Images_Are_All_You_Need_for_Material_Transfer_via_ICCV_2025_paper.pdf)
* [Hybrid Layout Control for Diffusion Transformer Fewer Annotations Superior Aesthetics](https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Hybrid_Layout_Control_for_Diffusion_Transformer_Fewer_Annotations_Superior_Aesthetics_ICCV_2025_paper.pdf)
* [UniCombine Unified Multi-Conditional Combination with Diffusion Transformer](http://arxiv.org/abs/2503.09277)
* [EasyControl Adding Efficient and Flexible Control for Diffusion Transformer](http://arxiv.org/abs/2503.07027)
* [Accelerating Diffusion Transformer via Gradient-Optimized Cache](http://arxiv.org/abs/2503.05156)<br>:star:[code](https://github.com/qiujx0520/GOC_ICCV2025.git)
* [LeGrad An Explainability Method for Vision Transformers via Feature Formation Sensitivity](http://arxiv.org/abs/2404.03214)
* [An Efficient Hybrid Vision Transformer for TinyML Applications](https://openaccess.thecvf.com/content/ICCV2025/papers/Zeng_An_Efficient_Hybrid_Vision_Transformer_for_TinyML_Applications_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/yuffeenn/TinyNeXt)
* [MixA A Mixed Attention approach with Stable Lightweight Linear Attention to enhance Efficiency of Vision Transformers at the Edge](https://openaccess.thecvf.com/content/ICCV2025/papers/Ahmed_MixA_A_Mixed_Attention_approach_with_Stable_Lightweight_Linear_Attention_ICCV_2025_paper.pdf)


<a name="41"/>

## 41.Neural Architecture Search(神经架构搜索)
* [Neural Architecture Search Driven by Locally Guided Diffusion for Personalized Federated Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Liao_Neural_Architecture_Search_Driven_by_Locally_Guided_Diffusion_for_Personalized_ICCV_2025_paper.pdf)
* [Loss Functions for Predictor-based Neural Architecture Search](http://arxiv.org/abs/2506.05869)
* [TRNAS A Training-Free Robust Neural Architecture Search](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_TRNAS_A_Training-Free_Robust_Neural_Architecture_Search_ICCV_2025_paper.pdf)

<a name="40"/>

## 40.Deep learning(深度学习)
* 胶囊网络
  * [EquiCaps Predictor-Free Pose-Aware Pre-Trained Capsule Networks](http://arxiv.org/abs/2506.09895)<br>:star:[code](http://github.com/AberdeenML/EquiCaps) :star:[code2](https://github.com/AberdeenML/EquiCaps)
* RNN
  * [ResQ: A Novel Framework to Implement Residual Neural Networks on Analog Rydberg Atom Quantum Computers](http://arxiv.org/pdf/2506.21537v1)

<a name="39"/>

## 39.Machine learning(机器学习)
* 机器遗忘
  * [MUNBa Machine Unlearning via Nash Bargaining](http://arxiv.org/abs/2411.15537)
  * [Robust Machine Unlearning for Quantized Neural Networks via Adaptive Gradient Reweighting with Similar Labels](http://arxiv.org/abs/2503.13917)
  * [Learning to Unlearn while Retaining Combating Gradient Conflicts in Machine Unlearning](http://arxiv.org/abs/2503.06339)
  * [Reminiscence Attack on Residuals Exploiting Approximate Machine Unlearning for Privacy](http://arxiv.org/abs/2507.20573)
* 主动学习
  * [To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models](https://arxiv.org/pdf/2507.15381v1)<br>:star:[code](https://github.com/juliamachnio/PALM)
  * [Consensus-Driven Active Model Selection](https://arxiv.org/pdf/2507.23771v1)<br>:star:[code](https://github.com/justinkay/coda)
* 对比学习
  * [Vector Contrastive Learning For Pixel-Wise Pretraining In Medical Vision](http://arxiv.org/pdf/2506.20850v1)  
  * [Selective Contrastive Learning for Weakly Supervised Affordance Grounding](https://arxiv.org/pdf/2508.07877v1)
  * [Fix-CLIP Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Fix-CLIP_Dual-Branch_Hierarchical_Contrastive_Learning_via_Synthetic_Captions_for_Better_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/bcwang-sjtu/Fix-CLIP)
  * [Robust Dataset Condensation using Supervised Contrastive Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_Robust_Dataset_Condensation_using_Supervised_Contrastive_Learning_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/DISL-Lab/RDC-ICCV2025)
  * [Differential-informed Sample Selection Accelerates Multimodal Contrastive Learning](http://arxiv.org/abs/2507.12998)<br>:star:[code](https://github.com/MediaBrain-SJTU/DISSect)
  * [Backdooring Self-Supervised Contrastive Learning by Noisy Alignment](http://arxiv.org/abs/2508.14015)<br>:star:[code](https://github.com/jsrdcht/Noisy-Alignment)
  * [Salvaging the Overlooked Leveraging Class-Aware Contrastive Learning for Multi-Class Anomaly Detection](http://arxiv.org/abs/2412.04769)
  * [AMD Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction](http://arxiv.org/abs/2507.01801)
* 强化学习
  * [RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment](http://arxiv.org/pdf/2506.21037v1)
  * [Reinforcement Learning-Guided Data Selection via Redundancy Assessment](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_Reinforcement_Learning-Guided_Data_Selection_via_Redundancy_Assessment_ICCV_2025_paper.pdf)
  * [RIPE: Reinforcement Learning on Unlabeled Image Pairs for Robust Keypoint Extraction](https://arxiv.org/pdf/2507.04839v1)<br>:star:[code](https://github.com/fraunhoferhhi/RIPE)
  * [DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding](https://arxiv.org/pdf/2508.08589v1)<br>:star:[code](https://github.com/wenwenyu/DocThinker)
  * [DeepMesh Auto-Regressive Artist-mesh Creation with Reinforcement Learning](http://arxiv.org/abs/2503.15265)
  * [ULTHO Ultra-Lightweight yet Efficient Hyperparameter Optimization in Deep Reinforcement Learning](http://arxiv.org/abs/2503.06101)
  * [Disentangled World Models Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement Learning](http://arxiv.org/abs/2503.08751)
  * [One Encoder to Rule them All Representation Learning for Model-free Visual Reinforcement Learning using Fourier Neural Operators](https://openaccess.thecvf.com/content/ICCV2025/papers/Dutta_One_Encoder_to_Rule_them_All_Representation_Learning_for_Model-free_ICCV_2025_paper.pdf)
  * [Diffusion Guided Adaptive Augmentation for Generalization in Visual Reinforcement Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_Diffusion_Guided_Adaptive_Augmentation_for_Generalization_in_Visual_Reinforcement_Learning_ICCV_2025_paper.pdf)
  * [GenFlowRL Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning](http://arxiv.org/abs/2508.11049)<br>:house:[project](https://colinyu1.github.io/genflowrl) :house:[project](https://colinyu1.github.io/genflowrl/)
* 持续学习
  * [CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization](http://arxiv.org/pdf/2506.21117v1)<br>:star:[code](https://cl-splats.github.io)
  * [PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning](https://arxiv.org/pdf/2507.12305v1)<br>:star:[code](https://github.com/anwarmaxsum/PROL)
  * [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/pdf/2507.09118v1)<br>:star:[code](https://github.com/linlany/MindtheGap)
  * [RainbowPrompt: Diversity-Enhanced Prompt-Evolving for Continual Learning](https://arxiv.org/pdf/2507.22553v1)
  * [Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models](https://arxiv.org/pdf/2508.00260v1)
  * [Divide-and-Conquer for Enhancing Unlabeled Learning, Stability, and Plasticity in Semi-supervised Continual Learning](https://arxiv.org/pdf/2508.05316v1)<br>:star:[code](https://github.com/NJUyued/USP4SSCL)
  * [Any-SSR How Recursive Least Squares Works in Continual Learning of Large Language Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Tong_Any-SSR_How_Recursive_Least_Squares_Works_in_Continual_Learning_of_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ZHUANGHP/Any-SSR)
  * [Joint Diffusion Models in Continual Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Skiers_Joint_Diffusion_Models_in_Continual_Learning_ICCV_2025_paper.pdf)
  * [PLAN Proactive Low-Rank Allocation for Continual Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_PLAN_Proactive_Low-Rank_Allocation_for_Continual_Learning_ICCV_2025_paper.pdf)
  * [Divide-and-Conquer for Enhancing Unlabeled Learning Stability and Plasticity in Semi-supervised Continual Learning](http://arxiv.org/abs/2508.05316)<br>:star:[code](https://github.com/NJUyued/USP4SSCL)
  * [CODE-CL Conceptor-Based Gradient Projection for Deep Continual Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Apolinario_CODE-CL_Conceptor-Based_Gradient_Projection_for_Deep_Continual_Learning_ICCV_2025_paper.pdf)
  * [FedAGC Federated Continual Learning with Asymmetric Gradient Correction](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_FedAGC_Federated_Continual_Learning_with_Asymmetric_Gradient_Correction_ICCV_2025_paper.pdf)
* 对抗学习
  * [TITAN Query-Token based Domain Adaptive Adversarial Learning](http://arxiv.org/abs/2506.21484)<br>:star:[code](https://github.com/Tajamul21/TITAN)
  * [ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models](https://arxiv.org/pdf/2507.21985v1)
  * [Pretend Benign A Stealthy Adversarial Attack by Exploiting Vulnerabilities in Cooperative Perception](https://openaccess.thecvf.com/content/ICCV2025/papers/Lin_Pretend_Benign_A_Stealthy_Adversarial_Attack_by_Exploiting_Vulnerabilities_in_ICCV_2025_paper.pdf)
  * [KOEnsAttack Towards Efficient Data-Free Black-Box Adversarial Attacks via Knowledge-Orthogonalized Substitute Ensembles](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_KOEnsAttack_Towards_Efficient_Data-Free_Black-Box_Adversarial_Attacks_via_Knowledge-Orthogonalized_Substitute_ICCV_2025_paper.pdf)
  * [SMP-Attack Boosting the Transferability of Feature Importance-based Adversarial Attack with Semantics-aware Multi-granularity Patchout](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_SMP-Attack_Boosting_the_Transferability_of_Feature_Importance-based_Adversarial_Attack_with_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/AdvML-Group/SMP-Attack)
  * [DISTIL: Data-Free Inversion of Suspicious Trojan Inputs via Latent Diffusion](https://arxiv.org/pdf/2507.22813v1)<br>:star:[code](https://github.com/AdaptiveMotorControlLab/DISTIL)
  * [Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights](https://arxiv.org/pdf/2508.00649v1)<br>:star:[code](https://github.com/Gandolfczjh/APDE)
  * [Towards a 3D Transfer-based Black-box Attack via Critical Feature Guidance](http://arxiv.org/abs/2508.15650)<br>:star:[code](https://github.com/AIASLab/CFG-ICCV2025)
  * [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/pdf/2508.05689v1)<br>:star:[code](https://github.com/ZezeTao/ResPA)
  * [Confound from All Sides Distill with Resilience Multi-Objective Adversarial Paths to Zero-Shot Robustness](https://openaccess.thecvf.com/content/ICCV2025/papers/Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths_ICCV_2025_paper.pdf)
  * [Adversarial Training for Probabilistic Robustness](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Adversarial_Training_for_Probabilistic_Robustness_ICCV_2025_paper.pdf)
  * [Mitigating Catastrophic Overfitting in Fast Adversarial Training via Label Information Elimination](https://openaccess.thecvf.com/content/ICCV2025/papers/Pan_Mitigating_Catastrophic_Overfitting_in_Fast_Adversarial_Training_via_Label_Information_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/fzjcdt/LIET)
  * [Towards Adversarial Robustness via Debiased High-Confidence Logit Alignment](http://arxiv.org/abs/2408.06079)<br>:star:[code](https://github.com/KejiaZhang-Robust/DHAT)
  * [Adversarial Exploitation of Data Diversity Improves Visual Localization](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Adversarial_Exploitation_of_Data_Diversity_Improves_Visual_Localization_ICCV_2025_paper.pdf)
  * [FedPall Prototype-based Adversarial and Collaborative Learning for Federated Learning with Feature Drift](http://arxiv.org/abs/2507.04781)
  * [Adversarial Robust Memory-Based Continual Learner](http://arxiv.org/abs/2311.17608)
  * [ViT-EnsembleAttack Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers](https://openaccess.thecvf.com/content/ICCV2025/papers/Cao_ViT-EnsembleAttack_Augmenting_Ensemble_Models_for_Stronger_Adversarial_Transferability_in_Vision_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Trustworthy-AI-Group/TransferAttack)
  * [CIARD Cyclic Iterative Adversarial Robustness Distillation](http://arxiv.org/abs/2509.12633)<br>:star:[code](https://github.com/CIARD2025/CIARD)
  * [Failure Cases Are Better Learned But Boundary Says Sorry Facilitating Smooth Perception Change for Accuracy-Robustness Trade-Off in Adversarial Training](http://arxiv.org/abs/2508.02186)<br>:star:[code](https://github.com/FlaAI/RPAT)
  * [Backdoor Mitigation by Distance-Driven Detoxification](http://arxiv.org/abs/2411.09585)
  * [Mind the Cost of Scaffold Benign Clients May Even Become Accomplices of Backdoor Attack](http://arxiv.org/abs/2411.16167)
  * [Prototype Guided Backdoor Defense via Activation Space Manipulation](https://openaccess.thecvf.com/content/ICCV2025/papers/Amula_Prototype_Guided_Backdoor_Defense_via_Activation_Space_Manipulation_ICCV_2025_paper.pdf)
  * [Leveraging Spatial Invariance to Boost Adversarial Transferability](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_Leveraging_Spatial_Invariance_to_Boost_Adversarial_Transferability_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/TheMoss7/SID)
  * [SPD Shallow Backdoor Protecting Deep Backdoor Against Backdoor Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Yuan_SPD_Shallow_Backdoor_Protecting_Deep_Backdoor_Against_Backdoor_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/YuanShunJie1/SPD)
  * [Backdoor Defense via Enhanced Splitting and Trap Isolation](https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_Backdoor_Defense_via_Enhanced_Splitting_and_Trap_Isolation_ICCV_2025_paper.pdf)
  * [Backdoor Attacks on Neural Networks via One-Bit Flip](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Backdoor_Attacks_on_Neural_Networks_via_One-Bit_Flip_ICCV_2025_paper.pdf)
  * [Seal Your Backdoor with Variational Defense](https://openaccess.thecvf.com/content/ICCV2025/papers/Sabolic_Seal_Your_Backdoor_with_Variational_Defense_ICCV_2025_paper.pdf)
  * [Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling](https://openaccess.thecvf.com/content/ICCV2025/papers/Niu_Enhancing_Adversarial_Transferability_by_Balancing_Exploration_and_Exploitation_with_Gradient-Guided_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/anuin-cat/GGS)
  * [Enhancing Transferability of Targeted Adversarial Examples via Inverse Target Gradient Competition and Spatial Distance Stretching](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Enhancing_Transferability_of_Targeted_Adversarial_Examples_via_Inverse_Target_Gradient_ICCV_2025_paper.pdf)
  * [Boosting Adversarial Transferability via Negative Hessian Trace Regularization](https://openaccess.thecvf.com/content/ICCV2025/papers/Long_Boosting_Adversarial_Transferability_via_Negative_Hessian_Trace_Regularization_ICCV_2025_paper.pdf)
  * [Unified Adversarial Augmentation for Improving Palmprint Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Jin_Unified_Adversarial_Augmentation_for_Improving_Palmprint_Recognition_ICCV_2025_paper.pdf)
  * [DIA The Adversarial Exposure of Deterministic Inversion in Diffusion Models](http://arxiv.org/abs/2510.00778)
  * [Generative Adversarial Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Jun_Generative_Adversarial_Diffusion_ICCV_2025_paper.pdf)
  * [ODDR Outlier Detection  Dimension Reduction Based Defense Against Adversarial Patches](http://arxiv.org/abs/2311.12084)
  * [Scaling and Taming Adversarial Training with Synthetic Data](https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Scaling_and_Taming_Adversarial_Training_with_Synthetic_Data_ICCV_2025_paper.pdf)
* 多模态学习
  * [G$^{2}$D: Boosting Multimodal Learning with Gradient-Guided Distillation](http://arxiv.org/pdf/2506.21514v1)<br>:star:[code](https://github.com/rAIson-Lab/G2D)
  * [Improving Multimodal Learning via Imbalanced Learning](https://arxiv.org/pdf/2507.10203v1)<br>:star:[code](https://github.com/shicaiwei123/ICCV2025-ARL)
  * [SimMLM: A Simple Framework for Multi-modal Learning with Missing Modality](https://arxiv.org/pdf/2507.19264v1)
  * [Unbiased Missing-modality Multimodal Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Dai_Unbiased_Missing-modality_Multimodal_Learning_ICCV_2025_paper.pdf)<br>:house:[project](https://crystal-punk.github.io/)
  * [Boosting Multimodal Learning via Disentangled Gradient Learning](https://arxiv.org/pdf/2507.10213v1)<br>:star:[code](https://github.com/shicaiwei123/ICCV2025-GDL)
  * [OpenVision A Fully-Open Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning](http://arxiv.org/abs/2505.04601)
* 多任务学习
  * [Resolving Token-Space Gradient Conflicts: Token Space Manipulation for Transformer-Based Multi-Task Learning](https://arxiv.org/pdf/2507.07485v1)
  * [Beyond Losses Reweighting Empowering Multi-Task Learning via the Generalization Perspective](http://arxiv.org/abs/2211.13723)
  * [Resolving Token-Space Gradient Conflicts Token Space Manipulation for Transformer-Based Multi-Task Learning](http://arxiv.org/abs/2507.07485)
  * [Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning](https://arxiv.org/pdf/2507.21049v1)<br>:star:[code](https://jacky1128.github.io/RepMTL/)
  * [TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction](https://arxiv.org/pdf/2508.04682v1)
  * [ModalTune Fine-Tuning Slide-Level Foundation Models with Multi-Modal Information for Multi-task Learning in Digital Pathology](http://arxiv.org/abs/2503.17564)
  * [Active Membership Inference Test (aMINT) Enhancing Model Auditability with Multi-Task Learning](http://arxiv.org/abs/2509.07879)<br>:star:[code](https://github.com/DanieldeAlcala/Membership-Inference-Test.git)
* 类增量学习
  * [Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning](https://arxiv.org/pdf/2507.09183v1)<br>:star:[code](https://github.com/Jywsuperman/LGSP)
  * [Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning](https://arxiv.org/pdf/2508.08165v1)<br>:star:[code](https://github.com/LAMDA-CL/ICCV2025-TUNA)
  * [Achieving More with Less Additive Prompt Tuning for Rehearsal-Free Class-Incremental Learning](http://arxiv.org/abs/2503.07979)
  * [Lark Low-Rank Updates After Knowledge Localization for Few-shot Class-Incremental Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Shi_Lark_Low-Rank_Updates_After_Knowledge_Localization_for_Few-shot_Class-Incremental_Learning_ICCV_2025_paper.pdf)
  * [A Tiny Change A Giant Leap Long-Tailed Class-Incremental Learning via Geometric Prototype Alignment](https://openaccess.thecvf.com/content/ICCV2025/papers/Lai_A_Tiny_Change_A_Giant_Leap_Long-Tailed_Class-Incremental_Learning_via_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/laixinyi023/Geometric-Prototype-Alignment)
  * [Task-Aware Prompt Gradient Projection for Parameter-Efficient Tuning Federated Class-Incremental Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Ke_Task-Aware_Prompt_Gradient_Projection_for_Parameter-Efficient_Tuning_Federated_Class-Incremental_Learning_ICCV_2025_paper.pdf)
  * [External Knowledge Injection for CLIP-Based Class-Incremental Learning](http://arxiv.org/abs/2503.08510)<br>:star:[code](https://github.com/LAMDA-CL/ICCV25-ENGINE)
  * [ESSENTIAL Episodic and Semantic Memory Integration for Video Class-Incremental Learning](http://arxiv.org/abs/2508.10896)
  * [Flexi-FSCIL Adaptive Knowledge Retention for Breaking the Stability-Plasticity Dilemma in Few-Shot Class-Incremental Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_Flexi-FSCIL_Adaptive_Knowledge_Retention_for_Breaking_the_Stability-Plasticity_Dilemma_in_ICCV_2025_paper.pdf)
  * [Seeing 3D Through 2D Lenses 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification](http://arxiv.org/abs/2509.14958)
  * [Feature Decomposition-Recomposition in Large Vision-Language Model for Few-Shot Class-Incremental Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Xue_Feature_Decomposition-Recomposition_in_Large_Vision-Language_Model_for_Few-Shot_Class-Incremental_Learning_ICCV_2025_paper.pdf)
* 增量学习
  * [Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning](https://arxiv.org/pdf/2507.21588v1)<br>:star:[code](https://github.com/ENJOY-Yin-jiong/PHP)
* 联邦学习
  * [Federated Representation Angle Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Yi_Federated_Representation_Angle_Learning_ICCV_2025_paper.pdf)
  * [Client2Vec Improving Federated Learning by Distribution Shifts Aware Client Indexing](http://arxiv.org/abs/2405.16233)<br>:star:[code](https://github.com/LINs-lab/client2vec)
  * [Geminio Language-Guided Gradient Inversion Attacks in Federated Learning](http://arxiv.org/abs/2411.14937)
  * [Sibai A Few-Shot Meta-Classifier for Poisoning Detection in Federated Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Gotz_Sibai_A_Few-Shot_Meta-Classifier_for_Poisoning_Detection_in_Federated_Learning_ICCV_2025_paper.pdf)
  * [You Are Your Own Best Teacher Achieving Centralized-level Performance in Federated Learning under Heterogeneous and Long-tailed Data](http://arxiv.org/abs/2503.06916)
  * [Personalized Federated Learning under Local Supervision](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Personalized_Federated_Learning_under_Local_Supervision_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/jqLi1626/FedSimSup)
  * [FedWSQ Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization](http://arxiv.org/abs/2506.23516)
  * [FedXDS Leveraging Model Attribution Methods to counteract Data Heterogeneity in Federated Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Hoefler_FedXDS_Leveraging_Model_Attribution_Methods_to_counteract_Data_Heterogeneity_in_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/MaxH1996/FedXDS)
  * [FLSeg Enhancing Privacy and Robustness in Federated Learning under Heterogeneous Data via Model Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Su_FLSeg_Enhancing_Privacy_and_Robustness_in_Federated_Learning_under_Heterogeneous_ICCV_2025_paper.pdf)
  * [Find a Scapegoat Poisoning Membership Inference Attack and Defense to Federated Learning](http://arxiv.org/abs/2507.00423)
  * [Forgetting Through Transforming Enabling Federated Unlearning via Class-Aware Representation Transformation](http://arxiv.org/abs/2410.06848)<br>:star:[code](https://github.com/zhentian777/FUCRT)
  * [Latte Collaborative Test-Time Adaptation of Vision-Language Models in Federated Learning](http://arxiv.org/abs/2507.21494)<br>:star:[code](https://github.com/baowenxuan/Latte)
  * 联邦遗忘学习
    * [Stealthy Backdoor Attack in Federated Learning via Adaptive Layer-wise Gradient Alignment](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_Stealthy_Backdoor_Attack_in_Federated_Learning_via_Adaptive_Layer-wise_Gradient_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/yqqhyqq/LGA)
* 元学习
  * [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/pdf/2508.06301v1)
  * [Meta-Unlearning on Diffusion Models Preventing Relearning Unlearned Concepts](http://arxiv.org/abs/2410.12777)
* Out-of-Distribution Detection(分布外检测)
  * [Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention](https://arxiv.org/pdf/2507.01417v1)
  * [NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection](https://arxiv.org/pdf/2507.09795v1)<br>:star:[code](https://github.com/ah-ansari/NegRefine)
  * [FEVER-OOD Free Energy Vulnerability Elimination for Robust Out-of-Distribution Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Isaac-Medina_FEVER-OOD_Free_Energy_Vulnerability_Elimination_for_Robust_Out-of-Distribution_Detection_ICCV_2025_paper.pdf)
  * [Beyond Pixel Uncertainty Bounding the OoD Objects in Road Scenes](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_Beyond_Pixel_Uncertainty_Bounding_the_OoD_Objects_in_Road_Scenes_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/huachao0124/DetSeg-official)
  * [ODP-Bench Benchmarking Out-of-Distribution Performance Prediction](https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_ODP-Bench_Benchmarking_Out-of-Distribution_Performance_Prediction_ICCV_2025_paper.pdf)
  * [A Unified Interpretation of Training-Time Out-of-Distribution Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection_ICCV_2025_paper.pdf)
  * [Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection](http://arxiv.org/abs/2507.10225)<br>:star:[code](https://github.com/Jarvisgivemeasuit/SynOOD)
  * [Activation Subspaces for Out-of-Distribution Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Zongur_Activation_Subspaces_for_Out-of-Distribution_Detection_ICCV_2025_paper.pdf)
  * [Diagnosing Pretrained Models for Out-of-distribution Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Xiong_Diagnosing_Pretrained_Models_for_Out-of-distribution_Detection_ICCV_2025_paper.pdf)
  * [Equipping Vision Foundation Model with Mixture of Experts for Out-of-Distribution Detection](http://arxiv.org/abs/2510.10584)
  * [DisCoPatch Taming Adversarially-driven Batch Statistics for Improved Out-of-Distribution Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Caetano_DisCoPatch_Taming_Adversarially-driven_Batch_Statistics_for_Improved_Out-of-Distribution_Detection_ICCV_2025_paper.pdf)
  * [Secure On-Device Video OOD Detection Without Backpropagation](http://arxiv.org/abs/2503.06166)<br>:star:[code](https://github.com/Dystopians/SecDOOD)
  * [FA Forced Prompt Learning of Vision-Language Models for Out-of-Distribution Detection](http://arxiv.org/abs/2507.04511)<br>:star:[code](https://github.com/0xFAFA/FA)
  * [Adaptive Prompt Learning via Gaussian Outlier Synthesis for Out-of-distribution Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Adaptive_Prompt_Learning_via_Gaussian_Outlier_Synthesis_for_Out-of-distribution_Detection_ICCV_2025_paper.pdf)
  * [Auxiliary Prompt Tuning of Vision-Language Models for Few-Shot Out-of-Distribution Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Miao_Auxiliary_Prompt_Tuning_of_Vision-Language_Models_for_Few-Shot_Out-of-Distribution_Detection_ICCV_2025_paper.pdf)
* 异常检测
  * [Toward Long-Tailed Online Anomaly Detection through Class-Agnostic Concepts](https://arxiv.org/pdf/2507.16946v1)<br>:house:[project](https://doi.org/10.5281/zenodo.16283852)
  * [DecAD Decoupling Anomalies in Latent Space for Multi-Class Unsupervised Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_DecAD_Decoupling_Anomalies_in_Latent_Space_for_Multi-Class_Unsupervised_Anomaly_ICCV_2025_paper.pdf)
  * [Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning](http://arxiv.org/abs/2508.02293)
  * [Wave-MambaAD Wavelet-driven State Space Model for Multi-class Unsupervised Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Wave-MambaAD_Wavelet-driven_State_Space_Model_for_Multi-class_Unsupervised_Anomaly_Detection_ICCV_2025_paper.pdf)
  * [Debiasing Trace Guidance Top-down Trace Distillation and Bottom-up Velocity Alignment for Unsupervised Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Debiasing_Trace_Guidance_Top-down_Trace_Distillation_and_Bottom-up_Velocity_Alignment_ICCV_2025_paper.pdf)
  * [MultiADS Defect-aware Supervision for Multi-type Anomaly Detection and Segmentation in Zero-Shot Learning](http://arxiv.org/abs/2504.06740)
  * [Triad Empowering LMM-based Anomaly Detection with Expert-guided Region-of-Interest Tokenizer and Manufacturing Process](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Triad_Empowering_LMM-based_Anomaly_Detection_with_Expert-guided_Region-of-Interest_Tokenizer_and_ICCV_2025_paper.pdf)
  * [SALAD -- Semantics-Aware Logical Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Fucka_SALAD_--_Semantics-Aware_Logical_Anomaly_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/MaticFuc/SALAD)
  * [SiM3D Single-instance Multiview Multimodal and Multisetup 3D Anomaly Detection Benchmark](http://arxiv.org/abs/2506.21549)
  * [Fine-grained Abnormality Prompt Learning for Zero-shot Anomaly Detection](http://arxiv.org/abs/2410.10289)
* 表征学习
  * [Multi-Modal Multi-Task Unified Embedding Model (M3T-UEM) A Task-Adaptive Representation Learning Framework](https://openaccess.thecvf.com/content/ICCV2025/papers/Sharma_Multi-Modal_Multi-Task_Unified_Embedding_Model_M3T-UEM_A_Task-Adaptive_Representation_Learning_ICCV_2025_paper.pdf)
  * [LayerLock Non-collapsing Representation Learning with Progressive Freezing](http://arxiv.org/abs/2509.10156)
  * [CARL Causality-guided Architecture Representation Learning for an Interpretable Performance Predictor](http://arxiv.org/abs/2506.04001)
  * [Pretrained Reversible Generation as Unsupervised Visual Representation Learning](http://arxiv.org/abs/2412.01787)<br>:house:[project](https://opendilab.github.io/PRG)
  * [Region-based Cluster Discrimination for Visual Representation Learning](https://arxiv.org/pdf/2507.20025v1)<br>:star:[code](https://github.com/deepglint/MVT)
  * [Gradient Extrapolation for Debiased Representation Learning](http://arxiv.org/abs/2503.13236)<br>:house:[project](https://gerne-debias.github.io/)
  * [Scaling Language-Free Visual Representation Learning](http://arxiv.org/abs/2504.01017)<br>:star:[code](https://github.com/facebookresearch/webssl)
  * [Q-Norm Robust Representation Learning via Quality-Adaptive Normalization](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Q-Norm_Robust_Representation_Learning_via_Quality-Adaptive_Normalization_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/IIP-Lab-XDU/Q-Norm)
  * [Scaling Omni-modal Pretraining with Multimodal Context Advancing Universal Representation Learning Across Modalities](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Scaling_Omni-modal_Pretraining_with_Multimodal_Context_Advancing_Universal_Representation_Learning_ICCV_2025_paper.pdf)
* 提示学习
  * [Advancing Textual Prompt Learning with Anchored Attributes](http://arxiv.org/abs/2412.09442)<br>:star:[code](https://github.com/zhengli97/ATPrompt)

<a name="38"/>

## 38.Few/Zero-Shot Learning/DG/Adaptation(小/零样本/域泛化/适应)
* 零样本
  * [Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model](https://arxiv.org/pdf/2506.23822v1)<br>:star:[code](https://github.com/shiming-chen/LaZSL)
  * [OBSER: Object-Based Sub-Environment Recognition for Zero-Shot Environmental Inference](https://arxiv.org/pdf/2507.02929v1)
  * [Language-Driven Multi-Label Zero-Shot Learning with Semantic Granularity](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Language-Driven_Multi-Label_Zero-Shot_Learning_with_Semantic_Granularity_ICCV_2025_paper.pdf)
  * [A Conditional Probability Framework for Compositional Zero-shot Learning](http://arxiv.org/abs/2507.17377)
  * [SVIP Semantically Contextualized Visual Patches for Zero-Shot Learning](http://arxiv.org/abs/2503.10252)<br>:star:[code](https://github.com/uqzhichen/SVIP)
  * [Learning Visual Proxy for Compositional Zero-Shot Learning](http://arxiv.org/abs/2501.13859)
  * [Verbalized Representation Learning for Interpretable Few-Shot Generalization](http://arxiv.org/abs/2411.18651)
  * [Hierarchical Variational Test-Time Prompt Generation for Zero-Shot Generalization](https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Hierarchical_Variational_Test-Time_Prompt_Generation_for_Zero-Shot_Generalization_ICCV_2025_paper.pdf)
* 小样本
  * [Towards Effective Foundation Model Adaptation for Extreme Cross-Domain Few-Shot Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_Towards_Effective_Foundation_Model_Adaptation_for_Extreme_Cross-Domain_Few-Shot_Learning_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/NWPUZhoufei/FMA)
  * [Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning](http://arxiv.org/abs/2508.03102)<br>:star:[code](https://github.com/tianjiao-j/CCA)
* AD 
  * [Graph Domain Adaptation with Dual-branch Encoder and Two-level Alignment for Whole Slide Image-based Survival Prediction](http://arxiv.org/abs/2411.14001)
* DG
  * [Bridging Domain Generalization to Multimodal Domain Generalization via Unified Representations](https://arxiv.org/pdf/2507.03304v1)
  * [Boosting Domain Generalized and Adaptive Detection with Diffusion Models Fitness Generalization and Transferability](http://arxiv.org/abs/2506.21042)
  * [Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization](https://arxiv.org/pdf/2507.04302v1)
  * [AdaDCP Learning an Adapter with Discrete Cosine Prior for Clear-to-Adverse Domain Generalization](https://openaccess.thecvf.com/content/ICCV2025/papers/Bi_AdaDCP_Learning_an_Adapter_with_Discrete_Cosine_Prior_for_Clear-to-Adverse_ICCV_2025_paper.pdf)
  * [ConstStyle Robust Domain Generalization with Unified Style Transformation](http://arxiv.org/abs/2509.05975)
  * [Split-and-Combine Enhancing Style Augmentation for Single Domain Generalization](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Split-and-Combine_Enhancing_Style_Augmentation_for_Single_Domain_Generalization_ICCV_2025_paper.pdf)
  * [Federated Domain Generalization with Domain-specific Soft Prompts Generation](http://arxiv.org/abs/2509.20807)
  * [Whats in a Latent Leveraging Diffusion Latent Space for Domain Generalization](https://openaccess.thecvf.com/content/ICCV2025/papers/Thomas_Whats_in_a_Latent_Leveraging_Diffusion_Latent_Space_for_Domain_ICCV_2025_paper.pdf)<br>:house:[project](https://xthomasbu.github.io/GUIDE)
  * [Customizing Domain Adapters for Domain Generalization](https://openaccess.thecvf.com/content/ICCV2025/papers/Ji_Customizing_Domain_Adapters_for_Domain_Generalization_ICCV_2025_paper.pdf)
* 无监督
  * [Soft Separation and Distillation Toward Global Uniformity in Federated Unsupervised Learning](http://arxiv.org/abs/2508.01251)<br>:house:[project](https://ssd-uniformity.github.io/)
* 自监督
  * [Prototype-based Contrastive Learning with Stage-wise Progressive Augmentation for Self-Supervised Fine-Grained Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Tan_Prototype-based_Contrastive_Learning_with_Stage-wise_Progressive_Augmentation_for_Self-Supervised_Fine-Grained_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/SEU-VIPGroup/PAPN)
  * [MoSiC Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning](http://arxiv.org/abs/2506.08694)<br>:star:[code](https://github.com/SMSD75/MoSiC)
  * [Self-supervised Learning of Hybrid Part-aware 3D Representations of 2D Gaussians and Superquadrics](http://arxiv.org/abs/2408.10789)
  * [Adversarial Robustness of Discriminative Self-Supervised Learning in Vision](https://openaccess.thecvf.com/content/ICCV2025/papers/Cagatan_Adversarial_Robustness_of_Discriminative_Self-Supervised_Learning_in_Vision_ICCV_2025_paper.pdf)
* 弱监督
  * [Weakly-Supervised Learning of Dense Functional Correspondences](http://arxiv.org/abs/2509.03893)
  * [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://openaccess.thecvf.com/content/ICCV2025/papers/Tang_Closed-Loop_Transfer_for_Weakly-supervised_Affordance_Grounding_ICCV_2025_paper.pdf)
* 半监督
  * [Semi-ViM Bidirectional State Space Model for Mitigating Label Imbalance in Semi-Supervised Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/He_Semi-ViM_Bidirectional_State_Space_Model_for_Mitigating_Label_Imbalance_in_ICCV_2025_paper.pdf)
  * [CaliMatch Adaptive Calibration for Improving Safe Semi-supervised Learning](http://arxiv.org/abs/2508.00922)
  * [Learnable Logit Adjustment for Imbalanced Semi-Supervised Learning under Class Distribution Mismatch](https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_Learnable_Logit_Adjustment_for_Imbalanced_Semi-Supervised_Learning_under_Class_Distribution_ICCV_2025_paper.pdf)
  * [SemiVisBooster Boosting Semi-Supervised Learning for Fine-Grained Classification through Pseudo-Label Semantic Guidance](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_SemiVisBooster_Boosting_Semi-Supervised_Learning_for_Fine-Grained_Classification_through_Pseudo-Label_Semantic_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/wenjinzhang/SemiVisBooster)
  * [Semi-supervised Deep Transfer for Regression without Domain Alignment](http://arxiv.org/abs/2509.05092)
  * [Semi-supervised Concept Bottleneck Models](http://arxiv.org/abs/2406.18992)



<a name="37"/>

## 37.Model Compression/Knowledge Distillation/Pruning(模型压缩/知识蒸馏/剪枝)
* [Representation Shift: Unifying Token Compression with FlashAttention](https://arxiv.org/pdf/2508.00367v1)<br>:star:[code](https://github.com/mlvlab/Representation-Shift)
* [Dynamic-VLM Simple Dynamic Visual Token Compression for VideoLLM](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Dynamic-VLM_Simple_Dynamic_Visual_Token_Compression_for_VideoLLM_ICCV_2025_paper.pdf)
* [AirCache Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference](http://arxiv.org/abs/2503.23956)
* [DiTFastAttnV2 Head-wise Attention Compression for Multi-Modality Diffusion Transformers](http://arxiv.org/abs/2503.22796)
* 剪枝
  * [Partial Forward Blocking: A Novel Data Pruning Paradigm for Lossless Training Acceleration](https://arxiv.org/pdf/2506.23674v1)
  * [Variance-Based Pruning for Accelerating and Compressing Trained Networks](https://arxiv.org/pdf/2507.12988v1)
  * [VFlowOpt A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization](http://arxiv.org/abs/2508.05211)
  * [WINS Winograd Structured Pruning for Fast Winograd Convolution](https://openaccess.thecvf.com/content/ICCV2025/papers/Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution_ICCV_2025_paper.pdf)
  * [Keyframe-oriented Vision Token Pruning Enhancing Efficiency of Large Vision Language Models on Long-Form Video Processing](http://arxiv.org/abs/2503.10742)
  * [Pruning All-Rounder Rethinking and Improving Inference Efficiency for Large Vision Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Suo_Pruning_All-Rounder_Rethinking_and_Improving_Inference_Efficiency_for_Large_Vision_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ASGO-MM/Pruning-All-Rounder)
  * [AIM Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning](http://arxiv.org/abs/2412.03248)<br>:star:[code](https://github.com/LaVi-Lab/AIM)
  * [FastVAR Linear Visual Autoregressive Modeling via Cached Token Pruning](http://arxiv.org/abs/2503.23367)
  * [Beyond Text-Visual Attention Exploiting Visual Cues for Effective Token Pruning in VLMs](http://arxiv.org/abs/2412.01818)<br>:star:[code](https://github.com/Theia-4869/VisPruner)
  * [MosaicDiff Training-free Structural Pruning for Diffusion Model Acceleration Reflecting Pretraining Dynamics](https://openaccess.thecvf.com/content/ICCV2025/papers/Guo_MosaicDiff_Training-free_Structural_Pruning_for_Diffusion_Model_Acceleration_Reflecting_Pretraining_ICCV_2025_paper.pdf)
  * 量化
  * [AHCPTQ Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model](http://arxiv.org/abs/2503.03088)
  * [Moment Quantization for Video Temporal Grounding](http://arxiv.org/abs/2504.02286)<br>:star:[code](https://github.com/TensorsSun/MQVTG)
  * [MSQ Memory-Efficient Bit Sparsification Quantization](http://arxiv.org/abs/2507.22349)
  * [Task Vector Quantization for Memory-Efficient Model Merging](http://arxiv.org/abs/2503.06921)<br>:house:[project](https://aim-skku.github.io/TVQ)
  * [OuroMamba A Data-Free Quantization Framework for Vision Mamba](http://arxiv.org/abs/2503.10959)<br>:star:[code](https://github.com/georgia-tech-synergy-lab/ICCV-OuroMamba)
  * [Semantic Alignment and Reinforcement for Data-Free Quantization of Vision Transformers](http://arxiv.org/abs/2412.16553)
  * [HUST High-Fidelity Unbiased Skin Tone Estimation via Texture Quantization](https://openaccess.thecvf.com/content/ICCV2025/papers/Ran_HUST_High-Fidelity_Unbiased_Skin_Tone_Estimation_via_Texture_Quantization_ICCV_2025_paper.pdf)
  * [D3QE Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_D3QE_Learning_Discrete_Distribution_Discrepancy-aware_Quantization_Error_for_Autoregressive-Generated_Image_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Zhangyr2022/D3QE)
  * [Scheduling Weight Transitions for Quantization-Aware Training](http://arxiv.org/abs/2404.19248)
  * [SSVQ Unleashing the Potential of Vector Quantization with Sign-Splitting](http://arxiv.org/abs/2503.08668)<br>:star:[code](https://github.com/list0830/SSVQ)
  * [ViM-VQ Efficient Post-Training Vector Quantization for Visual Mamba](https://openaccess.thecvf.com/content/ICCV2025/papers/Deng_ViM-VQ_Efficient_Post-Training_Vector_Quantization_for_Visual_Mamba_ICCV_2025_paper.pdf)
  * [Scalable Image Tokenization with Index Backpropagation Quantization](http://arxiv.org/abs/2412.02692)
  * [Allowing Oscillation Quantization Overcoming Solution Space Limitation in Low Bit-Width Quantization](https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_Allowing_Oscillation_Quantization_Overcoming_Solution_Space_Limitation_in_Low_Bit-Width_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/muzenc/AOQ)
  * [QuEST Low-bit Diffusion Model Quantization via Efficient Selective Finetuning](http://arxiv.org/abs/2402.03666)
  * [Memory-Efficient Generative Models via Product Quantization](https://openaccess.thecvf.com/content/ICCV2025/papers/Shao_Memory-Efficient_Generative_Models_via_Product_Quantization_ICCV_2025_paper.pdf)
  * [DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization](https://arxiv.org/pdf/2507.12933v1)<br>:star:[code](https://github.com/LeeDongYeun/dmq)
* KD
  * [Frequency-Aligned Knowledge Distillation for Lightweight Spatiotemporal Forecasting](https://arxiv.org/pdf/2507.02939v1)<br>:star:[code](https://github.com/itsnotacie/SDKD)
  * [Local Dense Logit Relations for Enhanced Knowledge Distillation](https://arxiv.org/pdf/2507.15911v1)
  * [Inference-Time Diffusion Model Distillation](http://arxiv.org/abs/2412.08871)
  * [Cross-Architecture Distillation Made Simple with Redundancy Suppression](https://arxiv.org/pdf/2507.21844v1)
  * [Evidential Knowledge Distillation](https://openaccess.thecvf.com/content/ICCV2025/papers/Xiang_Evidential_Knowledge_Distillation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/lyxiang-casia/EKD)
  * [EA-KD Entropy-based Adaptive Knowledge Distillation](https://openaccess.thecvf.com/content/ICCV2025/papers/Su_EA-KD_Entropy-based_Adaptive_Knowledge_Distillation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/cpsu00/EA-KD)
  * [CleanPose Category-Level Object Pose Estimation via Causal Learning and Knowledge Distillation](http://arxiv.org/abs/2502.01312)<br>:star:[code](https://github.com/chrislin0621/CleanPose)
  * [Knowledge Distillation with Refined Logits](http://arxiv.org/abs/2408.07703)<br>:star:[code](https://github.com/zju-SWJ/RLD)
  * [What to Distill Fast Knowledge Distillation with Adaptive Sampling](https://openaccess.thecvf.com/content/ICCV2025/papers/Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling_ICCV_2025_paper.pdf)
  * [VRM Knowledge Distillation via Virtual Relation Matching](http://arxiv.org/abs/2502.20760)
  * [Photolithography Overlay Map Generation with Implicit Knowledge Distillation Diffusion Transformer](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_Photolithography_Overlay_Map_Generation_with_Implicit_Knowledge_Distillation_Diffusion_Transformer_ICCV_2025_paper.pdf)
  * [Coupling the Generator with Teacher for Effective Data-Free Knowledge Distillation](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Coupling_the_Generator_with_Teacher_for_Effective_Data-Free_Knowledge_Distillation_ICCV_2025_paper.pdf)
  * [ACAM-KD Adaptive and Cooperative Attention Masking for Knowledge Distillation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lan_ACAM-KD_Adaptive_and_Cooperative_Attention_Masking_for_Knowledge_Distillation_ICCV_2025_paper.pdf)
  * [Enhanced Event-based Dense Stereo via Cross-Sensor Knowledge Distillation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Enhanced_Event-based_Dense_Stereo_via_Cross-Sensor_Knowledge_Distillation_ICCV_2025_paper.pdf)
  * [Perspective-Aware Teaching Adapting Knowledge for Heterogeneous Distillation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lin_Perspective-Aware_Teaching_Adapting_Knowledge_for_Heterogeneous_Distillation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/jimmylin0979/PAT.git)
  * [Fuse Before Transfer Knowledge Fusion for Heterogeneous Distillation](http://arxiv.org/abs/2410.12342)<br>:star:[code](https://github.com/liguopeng0923/FBT)
  * [A Good Teacher Adapts Their Knowledge for Distillation](https://openaccess.thecvf.com/content/ICCV2025/papers/Qian_A_Good_Teacher_Adapts_Their_Knowledge_for_Distillation_ICCV_2025_paper.pdf)


<a name="36"/>

## 36.Scene Graph Generation(场景图生成)
* [SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning](https://arxiv.org/pdf/2507.05798v1)
* [Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images](https://arxiv.org/pdf/2508.06546v1)<br>:star:[code](https://qixun1.github.io/projects/SCRSSG)<br>:star:[code](https://github.com/qixun1/scrsssg)
* [FROSS Faster-Than-Real-Time Online 3D Semantic Scene Graph Generation from RGB-D Images](https://openaccess.thecvf.com/content/ICCV2025/papers/Hou_FROSS_Faster-Than-Real-Time_Online_3D_Semantic_Scene_Graph_Generation_from_RGB-D_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Howardkhh/FROSS)
* [TRKT Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring](http://arxiv.org/abs/2508.04943)<br>:star:[code](https://github.com/XZPKU/TRKT.git)
* [End-to-End Entity-Predicate Association Reasoning for Dynamic Scene Graph Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_End-to-End_Entity-Predicate_Association_Reasoning_for_Dynamic_Scene_Graph_Generation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/wlw951226/ARN)
* [Vision-Language Interactive Relation Mining for Open-Vocabulary Scene Graph Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Min_Vision-Language_Interactive_Relation_Mining_for_Open-Vocabulary_Scene_Graph_Generation_ICCV_2025_paper.pdf)


<a name="35"/>


## 35.Style Transfer(风格迁移)
* [Domain Generalizable Portrait Style Transfer](https://arxiv.org/pdf/2507.04243v1)<br>:star:[code](https://github.com/wangxb29/DGPST)
* [Tune-Your-Style Intensity-tunable 3D Style Transfer with Gaussian Splatting](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_Tune-Your-Style_Intensity-tunable_3D_Style_Transfer_with_Gaussian_Splatting_ICCV_2025_paper.pdf)<br>:house:[project](https://zhao-yian.github.io/TuneStyle)

<a name="34"/>

## 34.Object Pose Estimation(物体姿态估计)
* [Deterministic Object Pose Confidence Region Estimation](https://arxiv.org/pdf/2506.22720v1)
* [BoxDreamer Dreaming Box Corners for Generalizable Object Pose Estimation](http://arxiv.org/abs/2504.07955)
* [MixRI Mixing Features of Reference Images for Novel Object Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_MixRI_Mixing_Features_of_Reference_Images_for_Novel_Object_Pose_ICCV_2025_paper.pdf)
* [Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures](https://arxiv.org/pdf/2507.10265v1)<br>:star:[code](https://wakuwu.github.io/KBA)
* [SDFit 3D Object Pose and Shape by Fitting a Morphable SDF to a Single Image](https://openaccess.thecvf.com/content/ICCV2025/papers/Antic_SDFit_3D_Object_Pose_and_Shape_by_Fitting_a_Morphable_ICCV_2025_paper.pdf)<br>:house:[project](https://anticdimi.github.io/sdfit)
* 计数
  * [Enhancing Zero-shot Object Counting via Text-guided Local Ranking and Number-evoked Global Attention](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Enhancing_Zero-shot_Object_Counting_via_Text-guided_Local_Ranking_and_Number-evoked_ICCV_2025_paper.pdf)
  * [CAPTURE Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting](http://arxiv.org/abs/2504.15485)
  * [CountSE Soft Exemplar Open-set Object Counting](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/pppppz22/CountSE)
  * [Counting Stacked Objects](https://openaccess.thecvf.com/content/ICCV2025/papers/Dumery_Counting_Stacked_Objects_ICCV_2025_paper.pdf)
* 重识别
  * [Generalizable Object Re-Identification via Visual In-Context Prompting](http://arxiv.org/abs/2508.21222)<br>:star:[code](https://github.com/Hzzone/VICP)
* 6DoF
  * [GraspCoT Integrating Physical Property Reasoning for 6-DoF Grasping under Flexible Language Instructions](https://openaccess.thecvf.com/content/ICCV2025/papers/Chu_GraspCoT_Integrating_Physical_Property_Reasoning_for_6-DoF_Grasping_under_Flexible_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/cxmomo/GraspCoT)
  * [Environment-Agnostic Pose Generating Environment-independent Object Representations for 6D Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Environment-Agnostic_Pose_Generating_Environment-independent_Object_Representations_for_6D_Pose_Estimation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/acmff22/EA6D) :house:[project](https://github.com/acmff22/EA6D)
  * [Ultra-Precision 6DoF Pose Estimation Using 2-D Interpolated Discrete Fourier Transform](https://openaccess.thecvf.com/content/ICCV2025/papers/Shi_Ultra-Precision_6DoF_Pose_Estimation_Using_2-D_Interpolated_Discrete_Fourier_Transform_ICCV_2025_paper.pdf)
  * [Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation](http://arxiv.org/abs/2510.04125)
  * [RayPose Ray Bundling Diffusion for Template Views in Unseen 6D Object Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_RayPose_Ray_Bundling_Diffusion_for_Template_Views_in_Unseen_6D_ICCV_2025_paper.pdf)
  * [6DOPE-GS Online 6D Object Pose Estimation using Gaussian Splatting](https://openaccess.thecvf.com/content/ICCV2025/papers/Jin_6DOPE-GS_Online_6D_Object_Pose_Estimation_using_Gaussian_Splatting_ICCV_2025_paper.pdf)

<a name="33"/>

## 33.Keypoint Detection(关键点检测)
* 关键点检测
  * [Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection](https://arxiv.org/pdf/2507.07994v1)<br>:house:[project](https://subhajitmaity.me/DYKp)
  * [ZeroKey Point-Level Reasoning and Zero-Shot 3D Keypoint Detection from Large Language Models](http://arxiv.org/abs/2412.06292)

<a name="32"/>


## 32.Image Registration(图像配准)
* [EDFFDNet Towards Accurate and Efficient Unsupervised Multi-Grid Image Registration](http://arxiv.org/abs/2509.07662)

<a name="31"/>


## 31.Image Matching(图像匹配)
* [HOMO-Feature Cross-Arbitrary-Modal Image Matching with Homomorphism of Organized Major Orientation](https://openaccess.thecvf.com/content/ICCV2025/papers/Gao_HOMO-Feature_Cross-Arbitrary-Modal_Image_Matching_with_Homomorphism_of_Organized_Major_Orientation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/MrPingQi/HOMO_Feature_ImgMatching)
* [Towards Open-World Generation of Stereo Images and Unsupervised Matching](http://arxiv.org/abs/2503.12720)<br>:house:[project](https://qjizhi.github.io/genstereo)
* [ArgMatch Adaptive Refinement Gathering for Efficient Dense Matching](https://openaccess.thecvf.com/content/ICCV2025/papers/Deng_ArgMatch_Adaptive_Refinement_Gathering_for_Efficient_Dense_Matching_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ACuOoOoO/argmatch)
* [CoMatch Dynamic Covisibility-Aware Transformer for Bilateral Subpixel-Level Semi-Dense Image Matching](http://arxiv.org/abs/2503.23925)<br>:star:[code](https://github.com/ZizhuoLi/CoMatch)
* [Balanced Image Stylization with Style Matching Score](http://arxiv.org/abs/2503.07601)
* Feature Matching(特征匹配)
  * [Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching](https://arxiv.org/pdf/2507.10318v1)
  * [CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance](https://arxiv.org/pdf/2507.17312v1)<br>:star:[code](https://github.com/pq-chen/CasP)
  * [DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in Collaborative Perception](https://arxiv.org/pdf/2507.18237v1)<br>:star:[code](https://github.com/ChengchangTian/DATA)
  * [Focal Plane Visual Feature Generation and Matching on a Pixel Processor Array](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Focal_Plane_Visual_Feature_Generation_and_Matching_on_a_Pixel_ICCV_2025_paper.pdf)
  * [Towards Efficient General Feature Prediction in Masked Skeleton Modeling](http://arxiv.org/abs/2509.03609)
  * [Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space](http://arxiv.org/abs/2507.00392)
  * [SGAD Semantic and Geometric-aware Descriptor for Local Feature Matching](http://arxiv.org/abs/2508.02278)
  * [EDM Efficient Deep Feature Matching](http://arxiv.org/abs/2503.05122)<br>:star:[code](https://github.com/chicleee/EDM)

<a name="30"/>


## 30.Image Fusion(图像融合)
* [DreamFuse Adaptive Image Fusion with Diffusion Transformer](http://arxiv.org/abs/2504.08291)
* [MMAIF Multi-task and Multi-degradation All-in-One for Image Fusion with Language Guidance](http://arxiv.org/abs/2503.14944)
* [Balancing Task-invariant Interaction and Task-specific Adaptation for Unified Image Fusion](http://arxiv.org/abs/2504.05164)
* [Revisiting Image Fusion for Multi-Illuminant White-Balance Correction](http://arxiv.org/abs/2503.14774)
* [Retinex-MEF Retinex-based Glare Effects Aware Unsupervised Multi-Exposure Image Fusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Bai_Retinex-MEF_Retinex-based_Glare_Effects_Aware_Unsupervised_Multi-Exposure_Image_Fusion_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/HaowenBai/Retinex-MEF)
* [Highlight What You Want Weakly-Supervised Instance-Level Controllable Infrared-Visible Image Fusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Highlight_What_You_Want_Weakly-Supervised_Instance-Level_Controllable_Infrared-Visible_Image_Fusion_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/GMY628/RIS-Fuse)
* [AMDANet Attention-Driven Multi-Perspective Discrepancy Alignment for RGB-Infrared Image Fusion and Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhong_AMDANet_Attention-Driven_Multi-Perspective_Discrepancy_Alignment_for_RGB-Infrared_Image_Fusion_and_ICCV_2025_paper.pdf)
* [LUT-Fuse Towards Extremely Fast Infrared and Visible Image Fusion via Distillation to Learnable Look-Up Tables](https://openaccess.thecvf.com/content/ICCV2025/papers/Yi_LUT-Fuse_Towards_Extremely_Fast_Infrared_and_Visible_Image_Fusion_via_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/zyb5/LUT-Fuse)
* [The Source Image is the Best Attention for Infrared and Visible Image Fusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_The_Source_Image_is_the_Best_Attention_for_Infrared_and_ICCV_2025_paper.pdf)

<a name="29"/>


## 29.Deepfake Detection/AI生成图像检测
* [Seeing Through Deepfakes A Human-Inspired Framework for Multi-Face Detection](http://arxiv.org/abs/2507.14807)
* [Generalization-Preserved Learning Closing the Backdoor to Catastrophic Forgetting in Continual Deepfake Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Generalization-Preserved_Learning_Closing_the_Backdoor_to_Catastrophic_Forgetting_in_Continual_ICCV_2025_paper.pdf)
* [Open-Unfairness Adversarial Mitigation for Generalized Deepfake Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Open-Unfairness_Adversarial_Mitigation_for_Generalized_Deepfake_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/liacaaa/AdvOU)
* 图像伪造定位/检测
  * [M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization](http://arxiv.org/pdf/2506.20922v1)
  * [Spatial-Temporal Forgery Trace based Forgery Image Identification](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Spatial-Temporal_Forgery_Trace_based_Forgery_Image_Identification_ICCV_2025_paper.pdf)
  * [ForgeLens Data-Efficient Forgery Focus for Generalizable Forgery Image Detection](http://arxiv.org/abs/2408.13697)
  * [Semantic Discrepancy-aware Detector for Image Forgery Identification](http://arxiv.org/abs/2508.12341)<br>:star:[code](https://github.com/wzy1111111/SSD)
  * [FakeRadar Probing Forgery Outliers to Detect Unknown Deepfake Videos](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_FakeRadar_Probing_Forgery_Outliers_to_Detect_Unknown_Deepfake_Videos_ICCV_2025_paper.pdf)
  * [ADCD-Net Robust Document Image Forgery Localization via Adaptive DCT Feature and Hierarchical Content Disentanglement](https://openaccess.thecvf.com/content/ICCV2025/papers/Wong_ADCD-Net_Robust_Document_Image_Forgery_Localization_via_Adaptive_DCT_Feature_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/KAHIMWONG/ACDC-Net)
* AI生成图片检测
  * [AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models](https://arxiv.org/pdf/2507.02664v1)
  * [Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful Illusions](https://arxiv.org/pdf/2507.22617v1)
  * [D3 Training-Free AI-Generated Video Detection Using Second-Order Features](http://arxiv.org/abs/2508.00701)<br>:star:[code](https://github.com/Zig-HS/D3)* [Bridging the Gap Between Ideal and Real-world Evaluation Benchmarking AI-Generated Image Detection in Challenging Scenarios](http://arxiv.org/abs/2509.09172)
  * [LOTA Bit-Planes Guided AI-Generated Image Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_LOTA_Bit-Planes_Guided_AI-Generated_Image_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/hongsong-wang/LOTA)
* 视频伪造检测
  * [HumanSAM: Classifying Human-centric Forgery Videos in Human Spatial, Appearance, and Motion Anomaly](https://arxiv.org/pdf/2507.19924v1)<br>:star:[code](https://dejian-lc.github.io/humansam/)
  * [Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection](https://arxiv.org/pdf/2507.02398v1)<br>:star:[code](https://github.com/rama0126/PwTF-DVD)
  * [Vulnerability-Aware Spatio-Temporal Learning for Generalizable Deepfake Video Detection](http://arxiv.org/abs/2501.01184)<br>:star:[code](https://github.com/10Ring/FakeSTormer)
  * [DeepShield Fortifying Deepfake Video Detection with Local and Global Forgery Analysis](https://openaccess.thecvf.com/content/ICCV2025/papers/Cai_DeepShield_Fortifying_Deepfake_Video_Detection_with_Local_and_Global_Forgery_ICCV_2025_paper.pdf)
* 合成图像检测
  * [LEGION Learning to Ground and Explain for Synthetic Image Detection](http://arxiv.org/abs/2503.15264)<br>:house:[project](https://opendatalab.github.io/LEGION)
  * [Forensic-MoE Exploring Comprehensive Synthetic Image Detection Traces with Mixture of Experts](https://openaccess.thecvf.com/content/ICCV2025/papers/Fang_Forensic-MoE_Exploring_Comprehensive_Synthetic_Image_Detection_Traces_with_Mixture_of_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/fangmq77/Forensic-MoE)
  * [MCID Multi-aspect Copyright Infringement Detection for Generated Images](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_MCID_Multi-aspect_Copyright_Infringement_Detection_for_Generated_Images_ICCV_2025_paper.pdf)
  * [Diffusion Epistemic Uncertainty with Asymmetric Learning for Diffusion-Generated Image Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_Diffusion_Epistemic_Uncertainty_with_Asymmetric_Learning_for_Diffusion-Generated_Image_Detection_ICCV_2025_paper.pdf)
* 复制图片检测
  * [Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_Tracing_Copied_Pixels_and_Regularizing_Patch_Affinity_in_Copy_Detection_ICCV_2025_paper.pdf)

<a name="28"/>


## 28.Optical Flow Estimation(光流估计)
* [MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation](https://arxiv.org/pdf/2506.23151v1)<br>:star:[code](https://github.com/msu-video-group/memfof)
* [FlowSeek Optical Flow Made Easier with Depth Foundation Models and Motion Bases](http://arxiv.org/abs/2509.05297)
* [PriOr-Flow Enhancing Primitive Panoramic Optical Flow with Orthogonal View](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View_ICCV_2025_paper.pdf)
* [Flow4Agent Long-form Video Understanding via Motion Prior from Optical Flow](http://arxiv.org/abs/2510.05836)
* [EMatch A Unified Framework for Event-based Optical Flow and Stereo Matching](http://arxiv.org/abs/2407.21735)
* [Removing Cost Volumes from Optical Flow Estimators](https://openaccess.thecvf.com/content/ICCV2025/papers/Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators_ICCV_2025_paper.pdf)
* [Unsupervised Joint Learning of Optical Flow and Intensity with Event Cameras](http://arxiv.org/abs/2503.17262)<br>:star:[code](https://github.com/tub-rip/E2FAI) :house:[project](https://github.com/tub-rip/E2FAI)

<a name="27"/>


## 27.Visual Question Answering(视觉问答)
* [SplatTalk 3D VQA with Gaussian Splatting](http://arxiv.org/abs/2503.06271)
* [ReasonVQA: A Multi-hop Reasoning Benchmark with Structural Knowledge for Visual Question Answering](https://arxiv.org/pdf/2507.16403v1)
* [ToolVQA A Dataset for Multi-step Reasoning VQA with External Tools](http://arxiv.org/abs/2508.03284)<br>:star:[code](https://github.com/Fugtemypt123/ToolVQA-release)
* [SimpleVQA Multimodal Factuality Evaluation for Multimodal Large Language Models](http://arxiv.org/abs/2502.13059)
* [Overcoming Dual Drift for Continual Long-Tailed Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Overcoming_Dual_Drift_for_Continual_Long-Tailed_Visual_Question_Answering_ICCV_2025_paper.pdf)
* [Ask and Remember A Questions-Only Replay Strategy for Continual Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2025/papers/Marouf_Ask_and_Remember_A_Questions-Only_Replay_Strategy_for_Continual_Visual_ICCV_2025_paper.pdf)
* Video-QA
  * [Object-centric Video Question Answering with Visual Grounding and Referring](http://arxiv.org/abs/2507.19599)
  * [TOGA Temporally Grounded Open-Ended Video QA with Weak Supervision](http://arxiv.org/abs/2506.09445)
* 数学问题解决
  * [VisionMath Vision-Form Mathematical Problem-Solving](https://openaccess.thecvf.com/content/ICCV2025/papers/Ma_VisionMath_Vision-Form_Mathematical_Problem-Solving_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/mengqiDyangge/VisionMath)

<a name="26"/>


## 26.Robot
* [GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization](https://arxiv.org/pdf/2507.10935v1)<br>:star:[code](https://github.com/tongshw/GeoDistill)
* [AR-1-to-3 Single Image to Consistent 3D Object via Next-View Prediction](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_AR-1-to-3_Single_Image_to_Consistent_3D_Object_via_Next-View_Prediction_ICCV_2025_paper.pdf)
* [RoboFactory Exploring Embodied Agent Collaboration with Compositional Constraints](http://arxiv.org/abs/2503.16408)
* [Embodied Representation Alignment with Mirror Neurons](http://arxiv.org/abs/2509.21136)
* [UnrealZoo Enriching Photo-realistic Virtual Worlds for Embodied AI](http://arxiv.org/abs/2412.20977)
* [NormalLoc Visual Localization on Textureless 3D Models using Surface Normals](https://openaccess.thecvf.com/content/ICCV2025/papers/Abe_NormalLoc_Visual_Localization_on_Textureless_3D_Models_using_Surface_Normals_ICCV_2025_paper.pdf)
* [Semantic-guided Camera Ray Regression for Visual Localization](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Semantic-guided_Camera_Ray_Regression_for_Visual_Localization_ICCV_2025_paper.pdf)
* 虚拟试穿
  * [OmniVTON: Training-Free Universal Virtual Try-On](https://arxiv.org/pdf/2507.15037v1)<br>:star:[code](https://github.com/Jerome-Young/OmniVTON)
  * [PromptDresser Improving the Quality and Controllability of Virtual Try-On via Generative Textual Prompt and Prompt-aware Mask](http://arxiv.org/abs/2412.16978)
  * [Learning Implicit Features with Flow-Infused Transformations for Realistic Virtual Try-On](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Learning_Implicit_Features_with_Flow-Infused_Transformations_for_Realistic_Virtual_Try-On_ICCV_2025_paper.pdf)
  * [All Parts Matter A Unified Mask-Free Virtual Try-On Framework](https://openaccess.thecvf.com/content/ICCV2025/papers/Du_All_Parts_Matter_A_Unified_Mask-Free_Virtual_Try-On_Framework_ICCV_2025_paper.pdf)
  * [TryOn-Refiner Conditional Rectified-flow-based TryOn Refiner for More Accurate Detail Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Qian_TryOn-Refiner_Conditional_Rectified-flow-based_TryOn_Refiner_for_More_Accurate_Detail_Reconstruction_ICCV_2025_paper.pdf)
* 机器人
  * [RoboPearls: Editable Video Simulation for Robot Manipulation](https://arxiv.org/pdf/2506.22756v1)
  * [AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning](https://arxiv.org/pdf/2508.07626v1)<br>:star:[code](https://github.com/idejie/ar)
  * [Adaptive Articulated Object Manipulation On The Fly with Foundation Model Reasoning and Part Grounding](https://arxiv.org/pdf/2507.18276v1)
  * [Recognizing Actions from Robotic View for Natural Human-Robot Interaction](https://arxiv.org/pdf/2507.22522v1)<br>:star:[code](https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View)
  * [Moto Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos](http://arxiv.org/abs/2412.04445)
  * [IRASim A Fine-Grained World Model for Robot Manipulation](http://arxiv.org/abs/2406.14540)<br>:house:[project](https://gen-irasim.github.io/)
  * [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/pdf/2508.00697v1)
  * [RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping](https://arxiv.org/pdf/2507.23734v1)<br>:star:[code](https://github.com/wudongming97/AffordanceNet)
  * [PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation](https://arxiv.org/pdf/2508.05976v1)
  * [Learning Precise Affordances from Egocentric Videos for Robotic Manipulation](http://arxiv.org/abs/2408.10123)<br>:house:[project](https://reagan1311.github.io/affgrasp)
  * [iManip Skill-Incremental Learning for Robotic Manipulation](http://arxiv.org/abs/2503.07087)
  * [RoBridge A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation](http://arxiv.org/abs/2505.01709)
  * [EC-Flow Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_EC-Flow_Enabling_Versatile_Robotic_Manipulation_from_Action-Unlabeled_Videos_via_Embodiment-Centric_ICCV_2025_paper.pdf)<br>:house:[project](https://ec-flow1.github.io/)
  * [A0 An Affordance-Aware Hierarchical Model for General Robotic Manipulation](http://arxiv.org/abs/2504.12636)
  * [Rethinking Bimanual Robotic Manipulation Learning with Decoupled Interaction Framework](http://arxiv.org/abs/2503.09186)
  * [GWM Towards Scalable Gaussian World Models for Robotic Manipulation](http://arxiv.org/abs/2508.17600)
  * [FedVLA Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation](http://arxiv.org/abs/2508.02190)
  * [Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control](http://arxiv.org/abs/2505.15304)
  * [Learning 4D Embodied World Models](http://arxiv.org/abs/2504.20995)
  * [VLABench A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks](http://arxiv.org/abs/2412.18194)
  * [RobAVA A Large-scale Dataset and Baseline Towards Video based Robotic Arm Action Understanding](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_RobAVA_A_Large-scale_Dataset_and_Baseline_Towards_Video_based_Robotic_ICCV_2025_paper.pdf)
  * [RoboAnnotatorX A Comprehensive and Universal Annotation Framework for Accurate Understanding of Long-horizon Robot Demonstration](https://openaccess.thecvf.com/content/ICCV2025/papers/Kou_RoboAnnotatorX_A_Comprehensive_and_Universal_Annotation_Framework_for_Accurate_Understanding_ICCV_2025_paper.pdf)
  * [4D Visual Pre-training for Robot Learning](http://arxiv.org/abs/2508.17230)<br>:house:[project](https://4d-visual-pretraining.github.io/)
  * [G-DexGrasp Generalizable Dexterous Grasping Synthesis Via Part-Aware Prior Retrieval and Prior-Assisted Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Jian_G-DexGrasp_Generalizable_Dexterous_Grasping_Synthesis_Via_Part-Aware_Prior_Retrieval_and_ICCV_2025_paper.pdf)
  * [DexH2R A Benchmark for Dynamic Dexterous Grasping in Human-to-Robot Handover](http://arxiv.org/abs/2506.23152)
  * [OVA-Fields Weakly Supervised Open-Vocabulary Affordance Fields for Robot Operational Part Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Su_OVA-Fields_Weakly_Supervised_Open-Vocabulary_Affordance_Fields_for_Robot_Operational_Part_ICCV_2025_paper.pdf)* [AnyBimanual Transferring Unimanual Policy for General Bimanual Manipulation](http://arxiv.org/abs/2412.06779)
  * [DyWA Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation](http://arxiv.org/abs/2503.16806)
  * [SD2Actor Continuous State Decomposition via Diffusion Embeddings for Robotic Manipulation](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_SD2Actor_Continuous_State_Decomposition_via_Diffusion_Embeddings_for_Robotic_Manipulation_ICCV_2025_paper.pdf)
  * [Diffusion-Based Imaginative Coordination for Bimanual Manipulation](http://arxiv.org/abs/2507.11296)<br>:star:[code](https://github.com/return-sleep/Diffusion_based_imaginative_Coordination)
  * [RoboTron-Mani All-in-One Multimodal Large Model for Robotic Manipulation](https://openaccess.thecvf.com/content/ICCV2025/papers/Yan_RoboTron-Mani_All-in-One_Multimodal_Large_Model_for_Robotic_Manipulation_ICCV_2025_paper.pdf)
  * Object Discovery
    * [Ensemble Foreground Management for Unsupervised Object Discovery](https://arxiv.org/pdf/2507.20860v1)<br>:star:[code](https://github.com/YFaris/UnionCut)
* SLAM
  * [Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps](https://arxiv.org/pdf/2507.03737v1)<br>:star:[code](https://3dagentworld.github.io/S3PO-GS/)
  * [DyGS-SLAM Real-Time Accurate Localization and Gaussian Reconstruction for Dynamic Scenes](https://openaccess.thecvf.com/content/ICCV2025/papers/Hu_DyGS-SLAM_Real-Time_Accurate_Localization_and_Gaussian_Reconstruction_for_Dynamic_Scenes_ICCV_2025_paper.pdf)
  * [4D Gaussian Splatting SLAM](http://arxiv.org/abs/2503.16710)
  * [ToF-Splatting Dense SLAM using Sparse Time-of-Flight Depth and Multi-Frame Integration](https://openaccess.thecvf.com/content/ICCV2025/papers/Conti_ToF-Splatting_Dense_SLAM_using_Sparse_Time-of-Flight_Depth_and_Multi-Frame_Integration_ICCV_2025_paper.pdf)
  * [Benchmarking Egocentric Visual-Inertial SLAM at City Scale](http://arxiv.org/abs/2509.26639)
  * [SuperEvent Cross-Modal Learning of Event-based Keypoint Detection for SLAM](http://arxiv.org/abs/2504.00139)<br>:house:[project](https://ethz-mrl.github.io/SuperEvent)
  * [SEGS-SLAM Structure-enhanced 3D Gaussian Splatting SLAM with Appearance Embedding](https://openaccess.thecvf.com/content/ICCV2025/papers/Wen_SEGS-SLAM_Structure-enhanced_3D_Gaussian_Splatting_SLAM_with_Appearance_Embedding_ICCV_2025_paper.pdf)<br>:house:[project](https://segs-slam.github.io/)
  * [Underwater Visual SLAM with Depth Uncertainty and Medium Modeling](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling_ICCV_2025_paper.pdf)
* 导航
  * [IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation](https://arxiv.org/pdf/2508.00823v1)<br>:star:[code](https://gwxuan.github.io/IGL-Nav/)<br>:star:[code](https://github.com/gwxuan/igl-nav)
  * [Embodied Navigation with Auxiliary Task of Action Description Prediction](https://openaccess.thecvf.com/content/ICCV2025/papers/Kondoh_Embodied_Navigation_with_Auxiliary_Task_of_Action_Description_Prediction_ICCV_2025_paper.pdf)
  * [EmbodiedSplat Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device](http://arxiv.org/abs/2509.17430)<br>:house:[project](https://gchhablani.github.io/embodied-splat)
  * [GUIOdyssey A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices](http://arxiv.org/abs/2406.08451)
  * [Learning on the Go A Meta-learning Object Navigation Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Learning_on_the_Go_A_Meta-learning_Object_Navigation_Model_ICCV_2025_paper.pdf)
  * [RoboTrom-Nav A Unified Framework for Embodied Navigation Integrating Perception Planning and Prediction](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhong_RoboTrom-Nav_A_Unified_Framework_for_Embodied_Navigation_Integrating_Perception_Planning_ICCV_2025_paper.pdf)
  * [MoMa-Kitchen A 100K Benchmark for Affordance-Grounded Last-Mile Navigation in Mobile Manipulation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_MoMa-Kitchen_A_100K_Benchmark_for_Affordance-Grounded_Last-Mile_Navigation_in_Mobile_ICCV_2025_paper.pdf)
  * [Active Perception Meets Rule-Guided RL A Two-Phase Approach for Precise Object Navigation in Complex Environments](https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Active_Perception_Meets_Rule-Guided_RL_A_Two-Phase_Approach_for_Precise_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/qinliangql/APRR)
  * [SAME Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts](http://arxiv.org/abs/2412.05552)
  * [Collaborative Instance Object Navigation Leveraging Uncertainty-Awareness to Minimize Human-Agent Dialogues](http://arxiv.org/abs/2412.01250)
  * [CogNav Cognitive Process Modeling for Object Goal Navigation with LLMs](http://arxiv.org/abs/2412.10439)
  * [DialNav Multi-turn Dialog Navigation with a Remote Guide](http://arxiv.org/abs/2509.12894)<br>:house:[project](https://happilee12.github.io/DialNav)
  * [LookOut Real-World Humanoid Egocentric Navigation](https://openaccess.thecvf.com/content/ICCV2025/papers/Pan_LookOut_Real-World_Humanoid_Egocentric_Navigation_ICCV_2025_paper.pdf)
  * [CityNav A Large-Scale Dataset for Real-World Aerial Navigation](http://arxiv.org/abs/2406.14240)
  * [Function-centric Bayesian Network for Zero-Shot Object Goal Navigation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Function-centric_Bayesian_Network_for_Zero-Shot_Object_Goal_Navigation_ICCV_2025_paper.pdf)
* 视觉位置识别
  * [VPR-Cloak A First Look at Privacy Cloak Against Visual Place Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Dong_VPR-Cloak_A_First_Look_at_Privacy_Cloak_Against_Visual_Place_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Dilemma-CMZ/VPR-Cloak)
  * [A Hyperdimensional One Place Signature to Represent Them All Stackable Descriptors For Visual Place Recognition](http://arxiv.org/abs/2412.06153)



<a name="25"/>


## 25.Human-Object Interaction Detection(人机交互)
* [Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss](https://arxiv.org/pdf/2507.01630v1)<br>:star:[code](https://github.com/YuxiaoWang-AI/P3HOT)
* [Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection](https://arxiv.org/pdf/2507.06510v1)
* [Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers](https://arxiv.org/pdf/2507.11287v1)<br>:star:[code](https://hcis-lab.github.io/TOHGS/)
* [HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation](https://arxiv.org/pdf/2507.15542v1)<br>:star:[code](https://github.com/ChelsieLei/HOLa)
* [Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting](https://arxiv.org/pdf/2508.00427v1)
* [SyncDiff Synchronized Motion Diffusion for Multi-Body Human-Object Interaction Synthesis](http://arxiv.org/abs/2412.20104)
* [PrimHOI Compositional Human-Object Interaction via Reusable Primitives](https://openaccess.thecvf.com/content/ICCV2025/papers/Jia_PrimHOI_Compositional_Human-Object_Interaction_via_Reusable_Primitives_ICCV_2025_paper.pdf)
* [No More Sibling Rivalry Debiasing Human-Object Interaction Detection](http://arxiv.org/abs/2509.00760)
* [Human-Object Interaction from Human-Level Instructions](http://arxiv.org/abs/2406.17840)
* [Open-Vocabulary HOI Detection with Interaction-aware Prompt and Concept Calibration](http://arxiv.org/abs/2508.03207)<br>:star:[code](https://github.com/ltttpku/INP-CC)
* [Visual Relation Diffusion for Human-Object Interaction Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Cao_Visual_Relation_Diffusion_for_Human-Object_Interaction_Detection_ICCV_2025_paper.pdf)
* [HUMOTO A 4D Dataset of Mocap Human Object Interactions](http://arxiv.org/abs/2504.10414)<br>:house:[project](https://jiaxin-lu.github.io/humoto/) :house:[project](https://jiaxin-lu.github.io/humoto)
* [ScoreHOI Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion](http://arxiv.org/abs/2509.07920)
* 手物交互
  * [Dynamic Reconstruction of Hand-Object Interaction with Distributed Force-aware Contact Representation](http://arxiv.org/abs/2411.09572)
  * [MagicHOI Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips](http://arxiv.org/abs/2508.05506)
  * [HORT Monocular Hand-held Objects Reconstruction with Transformers](http://arxiv.org/abs/2503.21313)
* 与场景交互
  * [SceneMI Motion In-betweening for Modeling Human-Scene Interaction](http://arxiv.org/abs/2503.16289)

<a name="24"/>

## 24.Autonomous Driving(自动驾驶)
* [CoDa-4DGS Dynamic Gaussian Splatting with Context and Deformation Awareness for Autonomous Driving](https://openaccess.thecvf.com/content/ICCV2025/papers/Song_CoDa-4DGS_Dynamic_Gaussian_Splatting_with_Context_and_Deformation_Awareness_for_ICCV_2025_paper.pdf)
* [Epona: Autoregressive Diffusion World Model for Autonomous Driving](https://arxiv.org/pdf/2506.24113v1)<br>:star:[code](https://kevin-thu.github.io/Epona/)<br>:star:[code](https://github.com/Kevin-thu/Epona/)
* [AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving](https://arxiv.org/pdf/2507.12137v1)
* [World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model](https://arxiv.org/pdf/2507.00603v1)<br>:star:[code](https://github.com/ucaszyp/World4Drive)
* [3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation](https://arxiv.org/pdf/2507.01367v1)<br>:star:[code](https://github.com/TRLou/PGA)
* [Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge](https://arxiv.org/pdf/2507.04123v1)
* [GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous Driving with Gaussian Splatting](https://arxiv.org/pdf/2507.19451v1)<br>:star:[code](https://gs-occ3d.github.io/)
* [From Gaze to Movement Predicting Visual Attention for Autonomous Driving Human-Machine Interaction based on Programmatic Imitation Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_From_Gaze_to_Movement_Predicting_Visual_Attention_for_Autonomous_Driving_ICCV_2025_paper.pdf)
* [MamV2XCalib: V2X-based Target-less Infrastructure Camera Calibration with State Space Model](https://arxiv.org/pdf/2507.23595v1)<br>:star:[code](https://github.com/zhuyaoye/MamV2XCalib)
* [VLR-Driver Large Vision-Language-Reasoning Models for Embodied Autonomous Driving](https://openaccess.thecvf.com/content/ICCV2025/papers/Kong_VLR-Driver_Large_Vision-Language-Reasoning_Models_for_Embodied_Autonomous_Driving_ICCV_2025_paper.pdf)
* [RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case](https://arxiv.org/pdf/2508.04642v1)<br>:star:[code](https://stars79689.github.io/RoboTron-Sim/)<br>:star:[code](https://github.com/stars79689/robotron-sim)
* [OD-RASE Ontology-Driven Risk Assessment and Safety Enhancement for Autonomous Driving](https://openaccess.thecvf.com/content/ICCV2025/papers/Shimomura_OD-RASE_Ontology-Driven_Risk_Assessment_and_Safety_Enhancement_for_Autonomous_Driving_ICCV_2025_paper.pdf)
* [UniMLVG Unified Framework for Multi-view Long Video Generation with Comprehensive Control Capabilities for Autonomous Driving](http://arxiv.org/abs/2412.04842)
* [MagicDrive-V2 High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control](https://openaccess.thecvf.com/content/ICCV2025/papers/Gao_MagicDrive-V2_High-Resolution_Long_Video_Generation_for_Autonomous_Driving_with_Adaptive_ICCV_2025_paper.pdf)<br>:house:[project](https://flymin.github.io/magicdrive-v2/) :house:[project](https://flymin.github.io/magicdrive-v2)
* [UniOcc A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving](http://arxiv.org/abs/2503.24381)<br>:house:[project](https://uniocc.github.io/)
* [ConsistentCity Semantic Flow-guided Occupancy DiT for Temporally Consistent Driving Scene Synthesis](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_ConsistentCity_Semantic_Flow-guided_Occupancy_DiT_for_Temporally_Consistent_Driving_Scene_ICCV_2025_paper.pdf)
* [U-ViLAR Uncertainty-Aware Visual Localization for Autonomous Driving via Differentiable Association and Registration](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_U-ViLAR_Uncertainty-Aware_Visual_Localization_for_Autonomous_Driving_via_Differentiable_Association_ICCV_2025_paper.pdf)
* [Towards Visual Localization Interoperability Cross-Feature for Collaborative Visual Localization and Mapping](https://openaccess.thecvf.com/content/ICCV2025/papers/Jaenal_Towards_Visual_Localization_Interoperability_Cross-Feature_for_Collaborative_Visual_Localization_and_ICCV_2025_paper.pdf)
* [SynAD Enhancing Real-World End-to-End Autonomous Driving Models through Synthetic Data Integration](https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_SynAD_Enhancing_Real-World_End-to-End_Autonomous_Driving_Models_through_Synthetic_Data_ICCV_2025_paper.pdf)
* [Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving](http://arxiv.org/abs/2503.21505)<br>:star:[code](https://github.com/Depth2World/VLADBench)
* [ORION A Holistic End-to-End Autonomous Driving Framework by Vision-Language Instructed Action Generation](http://arxiv.org/abs/2503.19755)
* [Unraveling the Effects of Synthetic Data on End-to-End Autonomous Driving](http://arxiv.org/abs/2503.18108)
* [DriveX Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving](http://arxiv.org/abs/2505.19239)
* [CoLMDriver LLM-based Negotiation Benefits Cooperative Autonomous Driving](http://arxiv.org/abs/2503.08683)<br>:star:[code](https://github.com/cxliu0314/CoLMDriver)
* [AdaDrive Self-Adaptive Slow-Fast System for Language-Grounded Autonomous Driving](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_AdaDrive_Self-Adaptive_Slow-Fast_System_for_Language-Grounded_Autonomous_Driving_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ReaFly/AdaDrive)
* [HiP-AD Hierarchical and Multi-Granularity Planning with Deformable Attention for Autonomous Driving in a Single Decoder](https://openaccess.thecvf.com/content/ICCV2025/papers/Tang_HiP-AD_Hierarchical_and_Multi-Granularity_Planning_with_Deformable_Attention_for_Autonomous_ICCV_2025_paper.pdf)
* [TAD-E2E A Large-scale End-to-end Autonomous Driving Dataset](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_TAD-E2E_A_Large-scale_End-to-end_Autonomous_Driving_Dataset_ICCV_2025_paper.pdf)
* [DistillDrive End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model](http://arxiv.org/abs/2508.05402)<br>:star:[code](https://github.com/YuruiAI/DistillDrive)
* [CARIM Caption-Based Autonomous Driving Scene Retrieval via Inclusive Text Matching](https://openaccess.thecvf.com/content/ICCV2025/papers/Ki_CARIM_Caption-Based_Autonomous_Driving_Scene_Retrieval_via_Inclusive_Text_Matching_ICCV_2025_paper.pdf)
* [Passing the Driving Knowledge Test](http://arxiv.org/abs/2508.21824)
* [DriveArena A Closed-loop Generative Simulation Platform for Autonomous Driving](http://arxiv.org/abs/2408.00415)
* [Hints of Prompt Enhancing Visual Representation for Multimodal LLMs in Autonomous Driving](http://arxiv.org/abs/2411.13076)
* [Are VLMs Ready for Autonomous Driving An Empirical Study from the Reliability Data and Metric Perspectives](http://arxiv.org/abs/2501.04003)
* [VLDrive Vision-Augmented Lightweight MLLMs for Efficient Language-grounded Autonomous Driving](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_VLDrive_Vision-Augmented_Lightweight_MLLMs_for_Efficient_Language-grounded_Autonomous_Driving_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ReaFly/VLDrive)
* [RoboTron-Drive All-in-One Large Multimodal Model for Autonomous Driving](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_RoboTron-Drive_All-in-One_Large_Multimodal_Model_for_Autonomous_Driving_ICCV_2025_paper.pdf)
* [ReAL-AD Towards Human-Like Reasoning in End-to-End Autonomous Driving](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_ReAL-AD_Towards_Human-Like_Reasoning_in_End-to-End_Autonomous_Driving_ICCV_2025_paper.pdf)
* [V2XPnP Vehicle-to-Everything Spatio-Temporal Fusion for Multi-Agent Perception and Prediction](http://arxiv.org/abs/2412.01812)
* [ETA Efficiency through Thinking Ahead A Dual Approach to Self-Driving with Large Models](http://arxiv.org/abs/2506.07725)
* [DrivingGPT Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers](http://arxiv.org/abs/2412.18607)
* [V2XScenes A Multiple Challenging Traffic Conditions Dataset for Large-Range Vehicle-Infrastructure Collaborative Perception](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_V2XScenes_A_Multiple_Challenging_Traffic_Conditions_Dataset_for_Large-Range_Vehicle-Infrastructure_ICCV_2025_paper.pdf)
* [Hydra-NeXt Robust Closed-Loop Driving with Open-Loop Training](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Hydra-NeXt_Robust_Closed-Loop_Driving_with_Open-Loop_Training_ICCV_2025_paper.pdf)
* [Driving View Synthesis on Free-form Trajectories with Generative Prior](http://arxiv.org/abs/2412.01717)
* [DiST-4D Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Guo_DiST-4D_Disentangled_Spatiotemporal_Diffusion_with_Metric_Depth_for_4D_Driving_ICCV_2025_paper.pdf)<br>:house:[project](https://royalmelon0505.github.io/DiST-4D)
* 三维占据
  * [Occupancy Learning with Spatiotemporal Memory](https://arxiv.org/pdf/2508.04705v1)<br>:star:[code](https://matthew-leng.github.io/stocc)
  * [Semantic Causality-Aware Vision-Based 3D Occupancy Prediction](http://arxiv.org/abs/2509.08388)
  * [GaussRender Learning 3D Occupancy with Gaussian Rendering](http://arxiv.org/abs/2502.05040)
  * [SA-Occ Satellite-Assisted 3D Occupancy Prediction in Real World](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_SA-Occ_Satellite-Assisted_3D_Occupancy_Prediction_in_Real_World_ICCV_2025_paper.pdf)
  * [EmbodiedOcc Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding](http://arxiv.org/abs/2412.04380)<br>:star:[code](https://github.com/YkiWu/EmbodiedOcc)
  * [AGO Adaptive Grounding for Open World 3D Occupancy Prediction](http://arxiv.org/abs/2504.10117)<br>:star:[code](https://github.com/EdwardLeeLPZ/AGO)
  * [GaussianFlowOcc Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow](http://arxiv.org/abs/2502.17288)
  * [GaussianOcc Fully Self-supervised and Efficient 3D Occupancy Estimation with Gaussian Splatting](http://arxiv.org/abs/2408.11447)
* 轨迹预测
  * [Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics](https://arxiv.org/pdf/2507.12083v1)
  * [Generative Active Learning for Long-tail Trajectory Prediction via Controllable Diffusion Model](https://arxiv.org/pdf/2507.22615v1)
  * [End-to-End Driving with Online Trajectory Evaluation via BEV World Model](http://arxiv.org/abs/2504.01941)<br>:star:[code](https://github.com/liyingyanUCAS/WoTE)
  * [TOTP Transferable Online Pedestrian Trajectory Prediction with Temporal-Adaptive Mamba Latent Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Ren_TOTP_Transferable_Online_Pedestrian_Trajectory_Prediction_with_Temporal-Adaptive_Mamba_Latent_ICCV_2025_paper.pdf)
  * [Resonance Learning to Predict Social-Aware Pedestrian Trajectories as Co-Vibrations](http://arxiv.org/abs/2412.02447)
  * [NATRA Noise-Agnostic Framework for Trajectory Prediction with Noisy Observations](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_NATRA_Noise-Agnostic_Framework_for_Trajectory_Prediction_with_Noisy_Observations_ICCV_2025_paper.pdf)
  * [DONUT A Decoder-Only Model for Trajectory Prediction](http://arxiv.org/abs/2506.06854)
* VLA
  * [CoA-VLA Improving Vision-Language-Action Models via Visual-Text Chain-of-Affordance](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_CoA-VLA_Improving_Vision-Language-Action_Models_via_Visual-Text_Chain-of-Affordance_ICCV_2025_paper.pdf)
  * [VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers](https://arxiv.org/pdf/2507.01016v1)<br>:star:[code](https://xiaoxiao0406.github.io/vqvla.github.io)
  * [CombatVLA An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games](http://arxiv.org/abs/2503.09527)<br>:house:[project](https://combatvla.github.io/)
  * [Dita Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy](http://arxiv.org/abs/2503.19757)
  * [Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics](http://arxiv.org/abs/2411.13587)
  * [Towards Long-Horizon Vision-Language-Action System Reasoning Acting and Memory](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Towards_Long-Horizon_Vision-Language-Action_System_Reasoning_Acting_and_Memory_ICCV_2025_paper.pdf)
* 占用预测
  * [Language Driven Occupancy Prediction](http://arxiv.org/abs/2411.16072)
  * [MergeOcc Bridge the Domain Gap between Different LiDARs for Robust Occupancy Prediction](http://arxiv.org/abs/2403.08512)
  * [MCOP Multi-UAV Collaborative Occupancy Prediction](https://openaccess.thecvf.com/content/ICCV2025/papers/Lin_MCOP_Multi-UAV_Collaborative_Occupancy_Prediction_ICCV_2025_paper.pdf)
  * [RIOcc Efficient Cross-Modal Fusion Transformer with Collaborative Feature Refinement for 3D Semantic Occupancy Prediction](https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_RIOcc_Efficient_Cross-Modal_Fusion_Transformer_with_Collaborative_Feature_Refinement_for_ICCV_2025_paper.pdf)
  * [ALOcc Adaptive Lifting-Based 3D Semantic Occupancy and Cost Volume-Based Flow Predictions](http://arxiv.org/abs/2411.07725)
* 重识别
  * [VehicleMAE View-asymmetry Mutual Learning for Vehicle Re-identification Pre-training via Masked AutoEncoders](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_VehicleMAE_View-asymmetry_Mutual_Learning_for_Vehicle_Re-identification_Pre-training_via_Masked_ICCV_2025_paper.pdf)
* 车道线检测
  * [When Anchors Meet Cold Diffusion A Multi-Stage Approach to Lane Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_When_Anchors_Meet_Cold_Diffusion_A_Multi-Stage_Approach_to_Lane_ICCV_2025_paper.pdf)
  * [SC-Lane Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Park_SC-Lane_Slope-aware_and_Consistent_Road_Height_Estimation_Framework_for_3D_ICCV_2025_paper.pdf)<br>:house:[project](https://parkchaesong.github.io/sclane) :house:[project](https://parkchaesong.github.io/sclane/)
  * [SparseLaneSTP Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Pittner_SparseLaneSTP_Leveraging_Spatio-Temporal_Priors_with_Sparse_Transformers_for_3D_Lane_ICCV_2025_paper.pdf)
* 车辆监控
  * [TrafficLoc Localizing Traffic Surveillance Cameras in 3D Scenes](http://arxiv.org/abs/2412.10308)<br>:house:[project](https://tum-luk.github.io/projects)



<a name="23"/>

## 23.Point Cloud(点云)
* [HVPUNet Hybrid-Voxel Point-cloud Upsampling Network](https://openaccess.thecvf.com/content/ICCV2025/papers/Ha_HVPUNet_Hybrid-Voxel_Point-cloud_Upsampling_Network_ICCV_2025_paper.pdf)
* [GAP: Gaussianize Any Point Clouds with Text Guidance](https://arxiv.org/pdf/2508.05631v1)<br>:star:[code](https://weiqi-zhang.github.io/GAP)<br>:star:[code](https://github.com/weiqi-zhang/gap)
* [StruMamba3D: Exploring Structural Mamba for Self-supervised Point Cloud Representation Learning](http://arxiv.org/pdf/2506.21541v1)
* [PointGAC: Geometric-Aware Codebook for Masked Point Cloud Modeling](https://arxiv.org/pdf/2507.04801v1)<br>:star:[code](https://github.com/LAB123-tech/PointGAC)
* [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/pdf/2507.09102v1)<br>:star:[code](https://github.com/wdttt/PointSD)
* [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/pdf/2507.15686v1)<br>:star:[code](https://huangwenjie2023.github.io/LINR-PCGC/)
* [Blended Point Cloud Diffusion for Localized Text-guided Shape Editing](https://arxiv.org/pdf/2507.15399v1)<br>:star:[code](https://tau-vailab.github.io/BlendedPC/)
* [UPP: Unified Point-Level Prompting for Robust Point Cloud Analysis](https://arxiv.org/pdf/2507.18997v1)<br>:star:[code](https://github.com/zhoujiahuan1991/ICCV2025-UPP)
* [Efficient Spiking Point Mamba for Point Cloud Analysis](http://arxiv.org/abs/2504.14371)<br>:star:[code](https://github.com/PeppaWu/SPM)
* [Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints](https://arxiv.org/pdf/2508.00558v1)
* [Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs](https://arxiv.org/pdf/2508.00169v1)<br>:star:[code](https://bhavyagoyal.github.io/ppc)<br>:star:[code](https://github.com/bhavyagoyal/ppc)
* [UST-SSM Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_UST-SSM_Unified_Spatio-Temporal_State_Space_Models_for_Point_Cloud_Video_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/wangzy01/UST-SSM)
* [Omni-scene Perception-oriented Point Cloud Geometry Enhancement for Coordinate Quantization](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Omni-scene_Perception-oriented_Point_Cloud_Geometry_Enhancement_for_Coordinate_Quantization_ICCV_2025_paper.pdf)
* [A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds](http://arxiv.org/abs/2504.09129)
* [GenFlow3D Generative Scene Flow Estimation and Prediction on Point Cloud Sequences](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_GenFlow3D_Generative_Scene_Flow_Estimation_and_Prediction_on_Point_Cloud_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ustc-hlli/GenFlow3D)
* [Feature Extraction and Representation of Pre-training Point Cloud Based on Diffusion Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Qiu_Feature_Extraction_and_Representation_of_Pre-training_Point_Cloud_Based_on_ICCV_2025_paper.pdf)
* [Leaps and Bounds An Improved Point Cloud Winding Number Formulation for Fast Normal Estimation and Surface Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Koneputugodage_Leaps_and_Bounds_An_Improved_Point_Cloud_Winding_Number_Formulation_ICCV_2025_paper.pdf)
* [Serialization based Point Cloud Oversegmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_Serialization_based_Point_Cloud_Oversegmentation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/CHL-glitch/SPCNet)
* [Towards More Diverse and Challenging Pre-training for Point Cloud Learning Self-Supervised Cross Reconstruction with Decoupled Views](http://arxiv.org/abs/2509.01250)<br>:star:[code](https://github.com/aHapBean/Point-PQAE)
* [Liberated-GS 3D Gaussian Splatting Independent from SfM Point Clouds](https://openaccess.thecvf.com/content/ICCV2025/papers/Pan_Liberated-GS_3D_Gaussian_Splatting_Independent_from_SfM_Point_Clouds_ICCV_2025_paper.pdf)
* [CAD-Recode Reverse Engineering CAD Code from Point Clouds](https://openaccess.thecvf.com/content/ICCV2025/papers/Rukhovich_CAD-Recode_Reverse_Engineering_CAD_Code_from_Point_Clouds_ICCV_2025_paper.pdf)
* [Constraint-Aware Feature Learning for Parametric Point Cloud](http://arxiv.org/abs/2411.07747)
* [Egocentric Action-aware Inertial Localization in Point Clouds with Vision-Language Guidance](http://arxiv.org/abs/2505.14346)
* [DiffPCI Large Motion Point Cloud frame Interpolation with Diffusion Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_DiffPCI_Large_Motion_Point_Cloud_frame_Interpolation_with_Diffusion_Model_ICCV_2025_paper.pdf)
* [Mixed Signals A Diverse Point Cloud Dataset for Heterogeneous LiDAR V2X Collaboration](http://arxiv.org/abs/2502.14156)
* [DAP-MAE Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/CVI-SZU/DAP-MAE)
* [Interpretable point cloud classification using multiple instance learning](https://openaccess.thecvf.com/content/ICCV2025/papers/De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning_ICCV_2025_paper.pdf)
* [CounterPC Counterfactual Feature Realignment for Unsupervised Domain Adaptation on Point Clouds](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point_ICCV_2025_paper.pdf)
* [Partially Matching Submap Helps Uncertainty Modeling and Propagation for Text to Point Cloud Localization](https://openaccess.thecvf.com/content/ICCV2025/papers/Feng_Partially_Matching_Submap_Helps_Uncertainty_Modeling_and_Propagation_for_Text_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Afoolbird/PMSH)
* [DiffRefine Diffusion-based Proposal Specific Point Cloud Densification for Cross-Domain Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object_ICCV_2025_paper.pdf)
* [Point Cloud Self-supervised Learning via 3D to Multi-view Masked Learner](http://arxiv.org/abs/2311.10887)
* [RARE Refine Any Registration of Pairwise Point Clouds via Zero-Shot Learning](http://arxiv.org/abs/2507.19950)<br>:star:[code](https://github.com/zhengcy-lambo/RARE.git)
* 3D 点云
  * [Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation](http://arxiv.org/pdf/2506.22375v1)
  * [FastPoint: Accelerating 3D Point Cloud Model Inference via Sample Point Distance Prediction](https://arxiv.org/pdf/2507.23480v1)
  * [ForestFormer3D A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds](http://arxiv.org/abs/2506.16991)<br>:house:[project](https://bxiang233.github.io/FF3D)
  * [Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds](http://arxiv.org/abs/2508.11265)<br>:star:[code](https://github.com/ChicalH/DCGL)
  * [GroundFlow A Plug-in Module for Temporal Reasoning on 3D Point Cloud Sequential Grounding](http://arxiv.org/abs/2506.21188)
  * [Tree Skeletonization from 3D Point Clouds by Denoising Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Marks_Tree_Skeletonization_from_3D_Point_Clouds_by_Denoising_Diffusion_ICCV_2025_paper.pdf)
  * [TrackAny3D Transferring Pretrained 3D Models for Category-unified 3D Point Cloud Tracking](http://arxiv.org/abs/2507.19908)
* 点云配准
  * [TurboReg: TurboClique for Robust and Efficient Point Cloud Registration](https://arxiv.org/pdf/2507.01439v1)<br>:star:[code](https://github.com/Laka-3DV/TurboReg)
  * [Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior](https://arxiv.org/pdf/2507.06651v1)
  * [Unsupervised RGB-D Point Cloud Registration for Scenes with Low Overlap and Photometric Inconsistency](https://openaccess.thecvf.com/content/ICCV2025/papers/Shou_Unsupervised_RGB-D_Point_Cloud_Registration_for_Scenes_with_Low_Overlap_ICCV_2025_paper.pdf)
  * [BUFFER-X Towards Zero-Shot Point Cloud Registration in Diverse Scenes](https://openaccess.thecvf.com/content/ICCV2025/papers/Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/MIT-SPARK/BUFFER-X)
* 点云分割
  * [All in One: Visual-Description-Guided Unified Point Cloud Segmentation](https://arxiv.org/pdf/2507.05211v1)<br>:star:[code](https://github.com/Hanzy1996/VDG-Uni3DSeg)
  * [Generalized Few-Shot Point Cloud Segmentation via LLM-Assisted Hyper-Relation Matching](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Generalized_Few-Shot_Point_Cloud_Segmentation_via_LLM-Assisted_Hyper-Relation_Matching_ICCV_2025_paper.pdf)
  * [Mitigating Geometric Degradation in Fast DownSampling via FastAdapter for Point Cloud Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Mitigating_Geometric_Degradation_in_Fast_DownSampling_via_FastAdapter_for_Point_ICCV_2025_paper.pdf)
* 点云补全
  * [Revisiting Point Cloud Completion Are We Ready For The Real-World](http://arxiv.org/abs/2411.17580)
  * [Geometric Alignment and Prior Modulation for View-Guided Point Cloud Completion on Unseen Categories](https://openaccess.thecvf.com/content/ICCV2025/papers/Xiu_Geometric_Alignment_and_Prior_Modulation_for_View-Guided_Point_Cloud_Completion_ICCV_2025_paper.pdf)
* 点云分类
  * [Purge-Gate Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token purging](https://openaccess.thecvf.com/content/ICCV2025/papers/Yazdanpanah_Purge-Gate_Backpropagation-Free_Test-Time_Adaptation_for_Point_Clouds_Classification_via_Token_ICCV_2025_paper.pdf)
* 点云去噪
  * [Noise2Score3D Tweedies Approach for Unsupervised Point Cloud Denoising](https://openaccess.thecvf.com/content/ICCV2025/papers/Wei_Noise2Score3D_Tweedies_Approach_for_Unsupervised_Point_Cloud_Denoising_ICCV_2025_paper.pdf)

<a name="22"/>

## 22.3D
* [VertexRegen: Mesh Generation with Continuous Level of Detail](https://arxiv.org/pdf/2508.09062v1)<br>:star:[code](https://vertexregen.github.io/)
* [GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields](https://arxiv.org/pdf/2506.23352v1)<br>:star:[code](https://snskysk.github.io/GeoProg3D/)
* [ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition](https://arxiv.org/pdf/2507.11261v1)<br>:star:[code](https://github.com/visualjason/ViewSRD)
* [Top2Pano: Learning to Generate Indoor Panoramas from Top-Down View](https://arxiv.org/pdf/2507.21371v1)<br>:star:[code](https://top2pano.github.io/)
* [PanoSplatt3R: Leveraging Perspective Pretraining for Generalized Unposed Wide-Baseline Panorama Reconstruction](https://arxiv.org/pdf/2507.21960v1)<br>:star:[code](https://npucvr.github.io/PanoSplatt3R)
* [Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion](https://arxiv.org/pdf/2507.23483v1)<br>:star:[code](https://mutianxu.github.io/stable-sim2real/)<br>:star:[code](https://github.com/gap-lab-cuhk-sz/stable-sim2real)
* [OmniDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment](https://arxiv.org/pdf/2508.04611v1)<br>:star:[code](https://github.com/aeolusguan/OmniDepth)
* [LightSwitch: Multi-view Relighting with Material-guided Diffusion](https://arxiv.org/pdf/2508.06494v1)<br>:star:[code](https://yehonathanlitman.github.io/light_switch/)<br>:star:[code](https://github.com/yehonathanlitman/lightswitch)
* [How Far are AI-generated Videos from Simulating the 3D Visual World: A Learned 3D Evaluation Approach](http://arxiv.org/abs/2406.19568)
* [Diorama Unleashing Zero-shot Single-view 3D Indoor Scene Modeling](http://arxiv.org/abs/2411.19492)
* [Articulate3D Holistic Understanding of 3D Scenes as Universal Scene Description](http://arxiv.org/abs/2412.01398)
* [HouseCrafter Lifting Floorplans to 3D Scenes with 2D Diffusion Models](http://arxiv.org/abs/2406.20077)
* [Uncertainty-Aware Diffusion-Guided Refinement of 3D Scenes](http://arxiv.org/abs/2503.15742)
* [Learning 3D Scene Analogies with Neural Contextual Scene Maps](http://arxiv.org/abs/2503.15897)<br>:house:[project](https://82magnolia.github.io/3d_scene_analogies)
* [SuperDec 3D Scene Decomposition with Superquadrics Primitives](http://arxiv.org/abs/2504.00992)
* [SAS Segment Any 3D Scene with Integrated 2D Priors](http://arxiv.org/abs/2503.08512)
* [Bolt3D Generating 3D Scenes in Seconds](http://arxiv.org/abs/2503.14445)
* [GLEAM Learning Generalizable Exploration Policy for Active Mapping in Complex 3D Indoor Scene](http://arxiv.org/abs/2505.20294)
* [Event-Driven Storytelling with Multiple Lifelike Humans in a 3D Scene](http://arxiv.org/abs/2507.19232)
* [Can3Tok Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians](http://arxiv.org/abs/2508.01464)
* [Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable Single-stage Image-to-3D Generation and Reconstruction](http://arxiv.org/abs/2411.14384)<br>:house:[project](https://caiyuanhao1998.github.io/project)
* [Generative Gaussian Splatting Generating 3D Scenes with Video Diffusion Priors](https://openaccess.thecvf.com/content/ICCV2025/papers/Schwarz_Generative_Gaussian_Splatting_Generating_3D_Scenes_with_Video_Diffusion_Priors_ICCV_2025_paper.pdf)
* [MaGS Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed Gaussian Splatting](http://arxiv.org/abs/2406.01593)
* [GS-Occ3D Scaling Vision-only Occupancy Reconstruction with Gaussian Splatting](https://openaccess.thecvf.com/content/ICCV2025/papers/Ye_GS-Occ3D_Scaling_Vision-only_Occupancy_Reconstruction_with_Gaussian_Splatting_ICCV_2025_paper.pdf)<br>:house:[project](https://gs-occ3d.github.io/)
* [PhysSplat Efficient Physics Simulation for 3D Scenes via MLLM-Guided Gaussian Splatting](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_PhysSplat_Efficient_Physics_Simulation_for_3D_Scenes_via_MLLM-Guided_Gaussian_ICCV_2025_paper.pdf)
* [CCL-LGS Contrastive Codebook Learning for 3D Language Gaussian Splatting](https://openaccess.thecvf.com/content/ICCV2025/papers/Tian_CCL-LGS_Contrastive_Codebook_Learning_for_3D_Language_Gaussian_Splatting_ICCV_2025_paper.pdf)<br>:house:[project](https://epsilontl.github.io/CCL-LGS)
* [InsideOut Integrated RGB-Radiative Gaussian Splatting for Comprehensive 3D Object Representation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_InsideOut_Integrated_RGB-Radiative_Gaussian_Splatting_for_Comprehensive_3D_Object_Representation_ICCV_2025_paper.pdf)
* 表面重建
  * [SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies](https://arxiv.org/pdf/2508.00366v1)<br>:star:[code](https://hanl2010.github.io/SparseRecon/)
  * [RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians](https://arxiv.org/pdf/2508.09830v1)<br>:star:[code](https://github.com/vLAR-group/RayletDF)
  * [PolGS Polarimetric Gaussian Splatting for Fast Reflective Surface Reconstruction](http://arxiv.org/abs/2509.19726)
  * [Quadratic Gaussian Splatting High Quality Surface Reconstruction with Second-order Geometric Primitives](http://arxiv.org/abs/2411.16392)
  * [Drawing Developmental Trajectory from Cortical Surface Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Drawing_Developmental_Trajectory_from_Cortical_Surface_Reconstruction_ICCV_2025_paper.pdf)
  * [SurfaceSplat Connecting Surface Reconstruction and Gaussian Splatting](http://arxiv.org/abs/2507.15602)<br>:star:[code](https://github.com/aim-uofa/SurfaceSplat)
  * [QuickSplat Fast 3D Surface Reconstruction via Learned Gaussian Initialization](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_QuickSplat_Fast_3D_Surface_Reconstruction_via_Learned_Gaussian_Initialization_ICCV_2025_paper.pdf)
  * [GSRecon Efficient Generalizable Gaussian Splatting for Surface Reconstruction from Sparse Views](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_GSRecon_Efficient_Generalizable_Gaussian_Splatting_for_Surface_Reconstruction_from_Sparse_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/hyangwinter/GSRecon)
  * [GCRayDiffusion Pose-Free Surface Reconstruction via Geometric Consistent Ray Diffusion](http://arxiv.org/abs/2503.22349)
  * [MGSR 2D3D Mutual-boosted Gaussian Splatting for High-fidelity Surface Reconstruction under Various Light Conditions](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_MGSR_2D3D_Mutual-boosted_Gaussian_Splatting_for_High-fidelity_Surface_Reconstruction_under_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/TsingyuanChou/MGSR)
* 三维重建
  * [Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction](http://arxiv.org/pdf/2506.21401v1)<br>:star:[code](https://github.com/zhirui-gao/Curve-Gaussian)
  * [InstaScene: Towards Complete 3D Instance Decomposition and Reconstruction from Cluttered Scenes](https://arxiv.org/pdf/2507.08416v1)<br>:star:[code](https://zju3dv.github.io/instascene/)
  * [Image-Guided Shape-from-Template Using Mesh Inextensibility Constraints](https://arxiv.org/pdf/2507.22699v1)<br>:star:[code](https://github.com/dvttran/nsft)
  * [MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction](https://arxiv.org/pdf/2507.15212v1)
  * [LONG3R: Long Sequence Streaming 3D Reconstruction](https://arxiv.org/pdf/2507.18255v1)<br>:star:[code](https://zgchen33.github.io/LONG3R/)
  * [H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction](https://arxiv.org/pdf/2508.03118v1)<br>:star:[code](https://github.com/JiaHeng-DLUT/H3R)
  * [Ross3D Reconstructive Visual Instruction Tuning with 3D-Awareness](http://arxiv.org/abs/2504.01901)
  * [FlowR Flowing from Sparse to Dense 3D Reconstructions](https://openaccess.thecvf.com/content/ICCV2025/papers/Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions_ICCV_2025_paper.pdf)
  * [Dream-to-Recon Monocular 3D Reconstruction with Diffusion-Depth Distillation from Single Images](https://openaccess.thecvf.com/content/ICCV2025/papers/Wulff_Dream-to-Recon_Monocular_3D_Reconstruction_with_Diffusion-Depth_Distillation_from_Single_Images_ICCV_2025_paper.pdf)
  * [POMATO Marrying Pointmap Matching with Temporal Motions for Dynamic 3D Reconstruction](http://arxiv.org/abs/2504.05692)<br>:star:[code](https://github.com/wyddmw/POMATO)
  * [Amodal3R Amodal 3D Reconstruction from Occluded 2D Images](http://arxiv.org/abs/2503.13439)
  * [DeGauss Dynamic-Static Decomposition with Gaussian Splatting for Distractor-free 3D Reconstruction](http://arxiv.org/abs/2503.13176)
  * [Hi-Gaussian Hierarchical Gaussians under Normalized Spherical Projection for Single-View 3D Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_Hi-Gaussian_Hierarchical_Gaussians_under_Normalized_Spherical_Projection_for_Single-View_3D_ICCV_2025_paper.pdf)
  * [Explaining Human Preferences via Metrics for Structured 3D Reconstruction](http://arxiv.org/abs/2503.08208)
  * [Boost 3D Reconstruction using Diffusion-based Monocular Camera Calibration](http://arxiv.org/abs/2411.17240)
  * [Dynamic Point Maps A Versatile Representation for Dynamic 3D Reconstruction](http://arxiv.org/abs/2503.16318)
  * [RadarSplat Radar Gaussian Splatting for High-Fidelity Data Synthesis and 3D Reconstruction of Autonomous Driving Scenes](http://arxiv.org/abs/2506.01379)
  * [HAMSt3R Human-Aware Multi-view Stereo 3D Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Rojas_HAMSt3R_Human-Aware_Multi-view_Stereo_3D_Reconstruction_ICCV_2025_paper.pdf)
  * [ArchiSet Benchmarking Editable and Consistent Single-View 3D Reconstruction of Buildings with Specific Window-to-Wall Ratios](https://openaccess.thecvf.com/content/ICCV2025/papers/Yin_ArchiSet_Benchmarking_Editable_and_Consistent_Single-View_3D_Reconstruction_of_Buildings_ICCV_2025_paper.pdf)
  * [FreeSplatter Pose-free Gaussian Splatting for Sparse-view 3D Reconstruction](http://arxiv.org/abs/2412.09573)
  * [SketchSplat 3D Edge Reconstruction via Differentiable Multi-view Sketch Splatting](http://arxiv.org/abs/2503.14786)
  * [Inverse 3D Microscopy Rendering for Cell Shape Inference with Active Mesh](http://arxiv.org/abs/2303.10440)
  * [Real3D Towards Scaling Large Reconstruction Models with Real Images](https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_Real3D_Towards_Scaling_Large_Reconstruction_Models_with_Real_Images_ICCV_2025_paper.pdf)
  * [TimeFormer Capturing Temporal Relationships of Deformable 3D Gaussians for Robust Reconstruction](http://arxiv.org/abs/2411.11941)
  * [Mamba-3VL Taming State Space Model for 3D Vision Language Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Mamba-3VL_Taming_State_Space_Model_for_3D_Vision_Language_Learning_ICCV_2025_paper.pdf)
  * [AAA-Gaussians Anti-Aliased and Artifact-Free 3D Gaussian Rendering](https://openaccess.thecvf.com/content/ICCV2025/papers/Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering_ICCV_2025_paper.pdf)
* 场景重建
  * [BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting](http://arxiv.org/pdf/2506.22099v1)<br>:star:[code](https://github.com/fudan-zvg/BezierGS)
  * [ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting](https://arxiv.org/pdf/2507.15454v1)<br>:star:[code](https://ruijiezhu94.github.io/ObjectGS_page)
  * [DepR: Depth Guided Single-view Scene Reconstruction with Instance-level Diffusion](https://arxiv.org/pdf/2507.22825v1)
  * [Momentum-GS Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_Momentum-GS_Momentum_Gaussian_Self-Distillation_for_High-Quality_Large_Scene_Reconstruction_ICCV_2025_paper.pdf)
  * [Splat-based 3D Scene Reconstruction with Extreme Motion-blur](https://openaccess.thecvf.com/content/ICCV2025/papers/Jang_Splat-based_3D_Scene_Reconstruction_with_Extreme_Motion-blur_ICCV_2025_paper.pdf)
  * [Puzzle Similarity A Perceptually-guided Cross-Reference Metric for Artifact Detection in 3D Scene Reconstructions](http://arxiv.org/abs/2411.17489)<br>:house:[project](https://nihermann.github.io/puzzlesim)
  * [ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors](https://arxiv.org/pdf/2508.06014v1)<br>:star:[code](https://exploregs.github.io)<br>:star:[code](https://github.com/minsu1206/exploregs)
  * [Self-Supervised Monocular 4D Scene Reconstruction for Egocentric Videos](http://arxiv.org/abs/2411.09145)<br>:house:[project](https://egomono4d.github.io/)
  * [BezierGS Dynamic Urban Scene Reconstruction with Bezier Curve Gaussian Splatting](http://arxiv.org/abs/2506.22099)
  * [S3R-GS Streamlining the Pipeline for Large-Scale Street Scene Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_S3R-GS_Streamlining_the_Pipeline_for_Large-Scale_Street_Scene_Reconstruction_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Tom-zgt/S3R-GS)
  * [RGE-GS Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors](https://openaccess.thecvf.com/content/ICCV2025/papers/Du_RGE-GS_Reward-Guided_Expansive_Driving_Scene_Reconstruction_via_Diffusion_Priors_ICCV_2025_paper.pdf)
  * [SpatialCrafter Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations](http://arxiv.org/abs/2505.11992)
  * [ClaraVid A Holistic Scene Reconstruction Benchmark From Aerial Perspective With Delentropy-Based Complexity Profiling](http://arxiv.org/abs/2503.17856)<br>:house:[project](https://rdbch.github.com/claravid)
  * [Diffusion-Based Extreme High-speed Scenes Reconstruction with the Complementary Vision Sensor](https://openaccess.thecvf.com/content/ICCV2025/papers/Meng_Diffusion-Based_Extreme_High-speed_Scenes_Reconstruction_with_the_Complementary_Vision_Sensor_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Tianmouc/GenRec)
  * [CityGS-X A Scalable Architecture for Efficient and Geometrically Accurate Large-Scale Scene Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Gao_CityGS-X_A_Scalable_Architecture_for_Efficient_and_Geometrically_Accurate_Large-Scale_ICCV_2025_paper.pdf)
  * [Event-boosted Deformable 3D Gaussians for Dynamic Scene Reconstruction](http://arxiv.org/abs/2411.16180)
  * [VistaDream Sampling multiview consistent images for single-view scene reconstruction](http://arxiv.org/abs/2410.16892)
  * [Hierarchy UGP Hierarchy Unified Gaussian Primitive for Large-Scale Dynamic Scene Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Hierarchy_UGP_Hierarchy_Unified_Gaussian_Primitive_for_Large-Scale_Dynamic_Scene_ICCV_2025_paper.pdf)
  * [Back on Track Bundle Adjustment for Dynamic Scene Reconstruction](http://arxiv.org/abs/2504.14516)
  * [Geo4D Leveraging Video Generators for Geometric 4D Scene Reconstruction](http://arxiv.org/abs/2504.07961)
  * [Humans as a Calibration Pattern Dynamic 3D Scene Reconstruction from Unsynchronized and Uncalibrated Videos](http://arxiv.org/abs/2412.19089)
  * [Proactive Scene Decomposition and Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Proactive_Scene_Decomposition_and_Reconstruction_ICCV_2025_paper.pdf)
  * [Scene Coordinate Reconstruction Priors](https://openaccess.thecvf.com/content/ICCV2025/papers/Bian_Scene_Coordinate_Reconstruction_Priors_ICCV_2025_paper.pdf)
* 三维场景理解
  * [VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding](https://arxiv.org/pdf/2506.22799v1)<br>:star:[code](https://sy-ja.github.io/votesplat/)
  * [Open-Vocabulary Octree-Graph for 3D Scene Understanding](http://arxiv.org/abs/2411.16253)<br>:star:[code](https://github.com/yifeisu/OV-Octree-Graph)
  * [HERMES A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation](http://arxiv.org/abs/2501.14729)<br>:star:[code](https://github.com/LMD0311/HERMES)
  * [3DGraphLLM Combining Semantic Graphs and Large Language Models for 3D Scene Understanding](http://arxiv.org/abs/2412.18450)<br>:star:[code](https://github.com/CognitiveAISystems/3DGraphLLM)
  * [ExCap3D Expressive 3D Scene Understanding via Object Captioning with Varying Detail](http://arxiv.org/abs/2503.17044)
  * [NuPlanQA A Large-Scale Dataset and Benchmark for Multi-View Driving Scene Understanding in Multi-Modal Large Language Models](http://arxiv.org/abs/2503.12772)<br>:star:[code](https://github.com/sungyeonparkk/NuPlanQA)
  * [Hierarchical 3D Scene Graphs Construction Outdoors](https://openaccess.thecvf.com/content/ICCV2025/papers/Nyffeler_Hierarchical_3D_Scene_Graphs_Construction_Outdoors_ICCV_2025_paper.pdf)
  * [AG2aussian Anchor-Graph Structured Gaussian Splatting for Instance-Level 3D Scene Understanding and Editing](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_AG2aussian_Anchor-Graph_Structured_Gaussian_Splatting_for_Instance-Level_3D_Scene_Understanding_ICCV_2025_paper.pdf)
  * [Embodied VideoAgent Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding](http://arxiv.org/abs/2501.00358)
  * [OURO A Self-Bootstrapped Framework for Enhancing Multimodal Scene Understanding](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_OURO_A_Self-Bootstrapped_Framework_for_Enhancing_Multimodal_Scene_Understanding_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/tinnel123666888/OURO.git)
  * [SceneSplat Gaussian Splatting-based Scene Understanding with Vision-Language Pretraining](http://arxiv.org/abs/2503.18052)
* 深度估计
  * [DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation](https://arxiv.org/pdf/2507.01603v1)
  * [Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation](http://arxiv.org/abs/2510.09320)
  * [One Look is Enough Seamless Patchwise Refinement for Zero-Shot Monocular Depth Estimation on High-Resolution Images](http://arxiv.org/abs/2503.22351)
  * [FiffDepth Feed-forward Transformation of Diffusion-Based Generators for Detailed Depth Estimation](http://arxiv.org/abs/2412.00671)
  * [GVDepth Zero-Shot Monocular Depth Estimation for Ground Vehicles based on Probabilistic Cue Fusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Koledic_GVDepth_Zero-Shot_Monocular_Depth_Estimation_for_Ground_Vehicles_based_on_ICCV_2025_paper.pdf)<br>:house:[project](https://unizgfer-lamor.github.io/gvdepth/) :house:[project](https://unizgfer-lamor.github.io/gvdepth)
  * [Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens](http://arxiv.org/abs/2508.04928)<br>:star:[code](https://github.com/JungHeeKim29/calibration-token)
  * [Depth Any Event Stream Enhancing Event-based Monocular Depth Estimation via Dense-to-Sparse Distillation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_Depth_Any_Event_Stream_Enhancing_Event-based_Monocular_Depth_Estimation_via_ICCV_2025_paper.pdf)
  * [Depth AnyEvent A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation](http://arxiv.org/abs/2509.15224)
  * [StableDepth Scene-Consistent and Scale-Invariant Monocular Depth](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth_ICCV_2025_paper.pdf)
  * [Spherical Epipolar Rectification for Deep Two-View Absolute Depth Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Brousseau_Spherical_Epipolar_Rectification_for_Deep_Two-View_Absolute_Depth_Estimation_ICCV_2025_paper.pdf)
  * [Seeing and Seeing Through the Glass Real and Synthetic Data for Multi-Layer Depth Estimation](http://arxiv.org/abs/2503.11633)
  * [S2M2 Scalable Stereo Matching Model for Reliable Depth Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Min_S2M2_Scalable_Stereo_Matching_Model_for_Reliable_Depth_Estimation_ICCV_2025_paper.pdf)
  * [Hyper-Depth Hypergraph-based Multi-Scale Representation Fusion for Monocular Depth Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Bie_Hyper-Depth_Hypergraph-based_Multi-Scale_Representation_Fusion_for_Monocular_Depth_Estimation_ICCV_2025_paper.pdf)
  * [FlashDepth Real-time Streaming Video Depth Estimation at 2K Resolution](http://arxiv.org/abs/2504.07093)<br>:star:[code](https://github.com/Eyeline-Research/FlashDepth)
  * [Simulating Dual-Pixel Images From Ray Tracing For Depth Estimation](http://arxiv.org/abs/2503.11213)
  * [Amodal Depth Anything Amodal Depth Estimation in the Wild](http://arxiv.org/abs/2412.02336)
* 深度补全
  * [PacGDC: Label-Efficient Generalizable Depth Completion with Projection Ambiguity and Consistency](https://arxiv.org/pdf/2507.07374v1)<br>:star:[code](https://github.com/Wang-xjtu/PacGDC)
  * [Test-Time Prompt Tuning for Zero-Shot Depth Completion](https://openaccess.thecvf.com/content/ICCV2025/papers/Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion_ICCV_2025_paper.pdf)
  * [ETA Energy-based Test-time Adaptation for Depth Completion](http://arxiv.org/abs/2508.05989)
  * [OMNI-DC Highly Robust Depth Completion with Multiresolution Depth Integration](https://openaccess.thecvf.com/content/ICCV2025/papers/Zuo_OMNI-DC_Highly_Robust_Depth_Completion_with_Multiresolution_Depth_Integration_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/princeton-vl/OMNI-DC)
  * [HFD-Teacher High-Frequency Depth Distillation from Depth Foundation Models for Enhanced Depth Completion](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_HFD-Teacher_High-Frequency_Depth_Distillation_from_Depth_Foundation_Models_for_Enhanced_ICCV_2025_paper.pdf)
  * [Marigold-DC Zero-Shot Monocular Depth Completion with Guided Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Viola_Marigold-DC_Zero-Shot_Monocular_Depth_Completion_with_Guided_Diffusion_ICCV_2025_paper.pdf)<br>:house:[project](https://MarigoldDepthCompletion.github.io/)
* SM
  * [RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather](https://arxiv.org/pdf/2507.01653v1)
  * [BANet Bilateral Aggregation Network for Mobile Stereo Matching](http://arxiv.org/abs/2503.03259)
  * [ZeroStereo Zero-shot Stereo Matching from Single Images](http://arxiv.org/abs/2501.08654)<br>:star:[code](https://github.com/Windsrain/ZeroStereo)
  * [Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts](https://arxiv.org/pdf/2507.04631v1)<br>:star:[code](https://github.com/cocowy1/SMoE-Stereo)
  * [Diving into the Fusion of Monocular Priors for Generalized Stereo Matching](http://arxiv.org/abs/2505.14414)
  * [Global Regulation and Excitation via Attention Tuning for Stereo Matching](http://arxiv.org/abs/2509.15891)<br>:star:[code](https://github.com/JarvisLee0423/GREAT-Stereo)
  * [MDP-Omni Parameter-free Multimodal Depth Prior-based Sampling for Omnidirectional Stereo Matching](https://openaccess.thecvf.com/content/ICCV2025/papers/Son_MDP-Omni_Parameter-free_Multimodal_Depth_Prior-based_Sampling_for_Omnidirectional_Stereo_Matching_ICCV_2025_paper.pdf)
* MVS
  * [MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network](https://arxiv.org/pdf/2507.11333v1)<br>:star:[code](https://github.com/JianfeiJ/MonoMVSNet)
* 3DGS
  * [RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration](https://arxiv.org/pdf/2507.08136v1)<br>:star:[code](https://3dagentworld.github.io/reggs/)
  * [PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations](https://arxiv.org/pdf/2507.13891v1)
  * [GaussianUpdate: Continual 3D Gaussian Splatting Update for Changing Environments](https://arxiv.org/pdf/2508.08867v1)
  * [ResGS Residual Densification of 3D Gaussian for Efficient Detail Recovery](http://arxiv.org/abs/2412.07494)
  * [RobustSplat Decoupling Densification and Dynamics for Transient-Free 3DGS](http://arxiv.org/abs/2506.02751)<br>:house:[project](https://fcyycf.github.io/RobustSplat)
  * [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](http://arxiv.org/abs/2507.11061)<br>:house:[project](https://janeyeon.github.io/romap)
  * [OCSplats Observation Completeness Quantification and Label Noise Separation in 3DGS](http://arxiv.org/abs/2508.01239)
  * [GSV3D Gaussian Splatting-based Geometric Distillation with Stable Video Diffusion for Single-Image 3D Object Generation](http://arxiv.org/abs/2503.06136)<br>:star:[code](https://github.com/MOMOYATW/GSV3D)
  * [GauUpdate New Object Insertion in 3D Gaussian Fields with Consistent Global Illumination](https://openaccess.thecvf.com/content/ICCV2025/papers/Ren_GauUpdate_New_Object_Insertion_in_3D_Gaussian_Fields_with_Consistent_ICCV_2025_paper.pdf)
  * [A Lesson in Splats Teacher-Guided Diffusion for 3D Gaussian Splats Generation with 2D Supervision](http://arxiv.org/abs/2412.00623)
  * [A3GS Arbitrary Artistic Style into Arbitrary 3D Gaussian Splatting](https://openaccess.thecvf.com/content/ICCV2025/papers/Fang_A3GS_Arbitrary_Artistic_Style_into_Arbitrary_3D_Gaussian_Splatting_ICCV_2025_paper.pdf)
  * [StochasticSplats Stochastic Rasterization for Sorting-Free 3D Gaussian Splatting](http://arxiv.org/abs/2503.24366)
  * [InterGSEdit Interactive 3D Gaussian Splatting Editing with 3D Geometry-Consistent Attention Prior](http://arxiv.org/abs/2507.04961)
  * [GaRe Relightable 3D Gaussian Splatting for Outdoor Scenes from Unconstrained Photo Collections](http://arxiv.org/abs/2507.20512)
  * [NeRF Is a Valuable Assistant for 3D Gaussian Splatting](http://arxiv.org/abs/2507.23374)
  * [Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction](http://arxiv.org/abs/2507.23006)<br>:house:[project](https://yzslab.github.io/REUrbanGS)
  * [GazeGaussian High-Fidelity Gaze Redirection with 3D Gaussian Splatting](http://arxiv.org/abs/2411.12981)<br>:house:[project](https://ucwxb.github.io/GazeGaussian)
  * [LongSplat Robust Unposed 3D Gaussian Splatting for Casual Long Videos](http://arxiv.org/abs/2508.14041)<br>:house:[project](https://linjohnss.github.io/longsplat/) :house:[project](https://linjohnss.github.io/longsplat)
  * [Lightweight Gradient-Aware Upscaling of 3D Gaussian Splatting Images](http://arxiv.org/abs/2503.14171)
  * [StealthAttack Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](http://arxiv.org/abs/2510.02314)
  * [SU-RGS Relightable 3D Gaussian Splatting from Sparse Views under Unconstrained Illuminations](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_SU-RGS_Relightable_3D_Gaussian_Splatting_from_Sparse_Views_under_Unconstrained_ICCV_2025_paper.pdf)
  * [MEGA Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes](http://arxiv.org/abs/2410.13613)<br>:star:[code](https://github.com/Xinjie-Q/MEGA)
  * [SplArt Articulation Estimation and Part-Level Reconstruction with 3D Gaussian Splatting](http://arxiv.org/abs/2506.03594)<br>:star:[code](https://github.com/ripl/splart)
  * [CATSplat Context-Aware Transformer with Spatial Guidance for Generalizable 3D Gaussian Splatting from A Single-View Image](http://arxiv.org/abs/2412.12906)
  * [AccidentalGS 3D Gaussian Splatting from Accidental Camera Motion](https://openaccess.thecvf.com/content/ICCV2025/papers/Mao_AccidentalGS_3D_Gaussian_Splatting_from_Accidental_Camera_Motion_ICCV_2025_paper.pdf)
  * [No Pose at All Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views](http://arxiv.org/abs/2508.01171)<br>:house:[project](https://ranrhuang.github.io/spfsplat/) :house:[project](https://ranrhuang.github.io/spfsplat)
* Semantic Scene Completion(语义场景补全)
  * [Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion](https://arxiv.org/pdf/2507.06230v1)<br>:star:[code](https://visinf.github.io/scenedino)<br>:star:[code](https://github.com/tum-vision/scenedino)
  * [Disentangling Instance and Scene Contexts for 3D Semantic Scene Completion](https://arxiv.org/pdf/2507.08555v1)<br>:star:[code](https://github.com/Enyu-Liu/DISC)
  * [Monocular Semantic Scene Completion via Masked Recurrent Networks](https://arxiv.org/pdf/2507.17661v1)<br>:star:[code](https://github.com/alanWXZ/MonoMRN)
  * [SDFormer Vision-based 3D Semantic Scene Completion via SAM-assisted Dual-channel Voxel Transformer](https://openaccess.thecvf.com/content/ICCV2025/papers/Xue_SDFormer_Vision-based_3D_Semantic_Scene_Completion_via_SAM-assisted_Dual-channel_Voxel_ICCV_2025_paper.pdf)
  * [Global-Aware Monocular Semantic Scene Completion with State Space Models](http://arxiv.org/abs/2503.06569)
  * [VisHall3D Monocular Semantic Scene Completion from Reconstructing the Visible Regions to Hallucinating the Invisible Regions](http://arxiv.org/abs/2507.19188)
* Scene Completion(场景补全)  
  * [Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion](http://arxiv.org/abs/2412.03515)<br>:star:[code](https://github.com/happyw1nd/ScoreLiDAR)
* 4D重建
  * [MonoFusion: Sparse-View 4D Reconstruction via Monocular Fusion](https://arxiv.org/pdf/2507.23782v1)<br>:star:[code](https://imnotprepared.github.io/research/25_DSR/)<br>:star:[code](https://github.com/ImNotPrepared/MonoFusion)
  * [I2-World Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting](https://openaccess.thecvf.com/content/ICCV2025/papers/Liao_I2-World_Intra-Inter_Tokenization_for_Efficient_Dynamic_4D_Scene_Forecasting_ICCV_2025_paper.pdf)
  * [St4RTrack Simultaneous 4D Reconstruction and Tracking in the World](http://arxiv.org/abs/2504.13152)
  * [Shape of Motion 4D Reconstruction from a Single Video](http://arxiv.org/abs/2407.13764)
* 场景生成
  * [ScenePainter Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment](http://arxiv.org/abs/2507.19058)
  * [Free4D Tuning-free 4D Scene Generation with Spatial-Temporal Consistency](http://arxiv.org/abs/2503.20785)
  * [Controllable 3D Outdoor Scene Generation via Scene Graphs](http://arxiv.org/abs/2503.07152)<br>:star:[code](https://github.com/yuhengliu02/control-3d-scene)
  * [InfiniCube Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models](http://arxiv.org/abs/2412.03934)
  * [MiDSummer Multi-Guidance Diffusion for Controllable Zero-Shot Immersive Gaussian Splatting Scene Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Hu_MiDSummer_Multi-Guidance_Diffusion_for_Controllable_Zero-Shot_Immersive_Gaussian_Splatting_Scene_ICCV_2025_paper.pdf)
  * [WonderPlay Dynamic 3D Scene Generation from a Single Image and Actions](http://arxiv.org/abs/2505.18151)
  * [VMem Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory](http://arxiv.org/abs/2506.18903)
  * [AutoScape Geometry-Consistent Long-Horizon Scene Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_AutoScape_Geometry-Consistent_Long-Horizon_Scene_Generation_ICCV_2025_paper.pdf)
  * [PersonaCraft Personalized and Controllable Full-Body Multi-Human Scene Generation Using Occlusion-Aware 3D-Conditioned Diffusion](http://arxiv.org/abs/2411.18068)
  * [Decoupled Diffusion Sparks Adaptive Scene Generation](http://arxiv.org/abs/2504.10485)
  * [Large Scene Generation with Cube-Absorb Discrete Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Hu_Large_Scene_Generation_with_Cube-Absorb_Discrete_Diffusion_ICCV_2025_paper.pdf)
* 场景流估计
  * [TARS Traffic-Aware Radar Scene Flow Estimation](http://arxiv.org/abs/2503.10210)

<a name="21"/>

## 21.UAV/RS/Satellite Image(无人机/遥感/卫星图像)
* [HUG Hierarchical Urban Gaussian Splatting with Block-Based Reconstruction for Large-Scale Aerial Scenes](http://arxiv.org/abs/2504.16606)
* [UAVScenes: A Multi-Modal Dataset for UAVs](https://arxiv.org/pdf/2507.22412v1)<br>:star:[code](https://github.com/sijieaaa/UAVScenes)
* [MMGeo Multimodal Compositional Geo-Localization for UAVs](https://openaccess.thecvf.com/content/ICCV2025/papers/Ji_MMGeo_Multimodal_Compositional_Geo-Localization_for_UAVs_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Yux1angJi/MMGeo)
* [Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method](http://arxiv.org/pdf/2506.22027v1)<br>:star:[code](https://github.com/Alioth2000/Hoss-ReID)
* [LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment](https://arxiv.org/pdf/2507.00659v1)
* [SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing](https://arxiv.org/pdf/2507.13812v1)
* [Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with Weak Supervision](https://arxiv.org/pdf/2507.20976v1)<br>:star:[code](https://humansensinglab.github.io/AGenDA)
* [OpenRSD Towards Open-prompts for Object Detection in Remote Sensing Images](http://arxiv.org/abs/2503.06146)
* [Dual Domain Control via Active Learning for Remote Sensing Domain Incremental Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Dual_Domain_Control_via_Active_Learning_for_Remote_Sensing_Domain_ICCV_2025_paper.pdf)
* [Active Learning Meets Foundation Models Fast Remote Sensing Data Annotation for Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Burges_Active_Learning_Meets_Foundation_Models_Fast_Remote_Sensing_Data_Annotation_ICCV_2025_paper.pdf)
* [LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing Object Detection](http://arxiv.org/abs/2509.16970)
* [Fusion Meets Diverse Conditions A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Fusion_Meets_Diverse_Conditions_A_High-diversity_Benchmark_and_Baseline_for_ICCV_2025_paper.pdf)
* [RS-vHeat Heat Conduction Guided Efficient Remote Sensing Foundation Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model_ICCV_2025_paper.pdf)
* [SMARTIES Spectrum-Aware Multi-Sensor Auto-Encoder for Remote Sensing Images](http://arxiv.org/abs/2506.19585)<br>:house:[project](https://gsumbul.github.io/SMARTIES)
* [HoliTracer Holistic Vectorization of Geographic Objects from Large-Size Remote Sensing Imagery](http://arxiv.org/abs/2507.16251)<br>:star:[code](https://github.com/vvangfaye/HoliTracer)
* [Towards Privacy-preserved Pre-training of Remote Sensing Foundation Models with Federated Mutual-guidance Learning](http://arxiv.org/abs/2503.11051)
* [When Large Vision-Language Model Meets Large Remote Sensing Imagery Coarse-to-Fine Text-Guided Token Pruning](http://arxiv.org/abs/2503.07588)
* 卫星
  * [Harnessing Massive Satellite Imagery with Efficient Masked Image Modeling](http://arxiv.org/abs/2406.11933)
  * [WildSAT Learning Satellite Image Representations from Wildlife Observations](http://arxiv.org/abs/2412.14428)
  * [Sat2City: 3D City Generation from A Single Satellite Image with Cascaded Latent Diffusion](https://arxiv.org/pdf/2507.04403v1)
  * [MagicCity Geometry-Aware 3D City Generation from Satellite Imagery with Multi-View Consistency](https://openaccess.thecvf.com/content/ICCV2025/papers/Yao_MagicCity_Geometry-Aware_3D_City_Generation_from_Satellite_Imagery_with_Multi-View_ICCV_2025_paper.pdf)
* 变化检测
  * [Information-Bottleneck Driven Binary Neural Network for Change Detection](https://arxiv.org/pdf/2507.03504v1)
  * [RTMap Real-Time Recursive Mapping with Change Detection and Localization](http://arxiv.org/abs/2507.00980)<br>:star:[code](https://github.com/CN-ADLab/RTMap)
  * [What Changed Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models](http://arxiv.org/abs/2505.20405)<br>:house:[project](https://aimagelab.github.io/DICE)
* 目标检测
  * [Measuring the Impact of Rotation Equivariance on Aerial Object Detection](https://arxiv.org/pdf/2507.09896v1)
* 分割
  * [SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation](https://arxiv.org/pdf/2507.12857v1)<br>:star:[code](https://github.com/HuangShiqi128/SCORE)
  * [Dynamic Dictionary Learning for Remote Sensing Image Segmentation](http://arxiv.org/abs/2503.06683)<br>:star:[code](https://github.com/XavierJiezou/D2LS)
* 无人机
  * [Video2BEV Transforming Drone Videos to BEVs for Video-based Geo-localization](http://arxiv.org/abs/2411.13610)<br>:star:[code](https://github.com/HaoDot/Video2BEV-Open)
  * [Video Individual Counting for Moving Drones](http://arxiv.org/abs/2503.10701)

<a name="20"/>

## 20.OCR
* [Beyond Isolated Words: Diffusion Brush for Handwritten Text-Line Generation](https://arxiv.org/pdf/2508.03256v1)<br>:star:[code](https://github.com/dailenson/DiffBrush)
* [CTC Transcription Alignment of the Bullinger Letters: Automatic Improvement of Annotation Quality](https://arxiv.org/pdf/2508.07904v1)<br>:star:[code](https://github.com/andreas-fischer-unifr/nntp)
* [OCR Hinders RAG Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation](http://arxiv.org/abs/2412.02592)<br>:star:[code](https://github.com/opendatalab/OHR-Bench)
* [CC-OCR A Comprehensive and Challenging OCR Benchmark for Evaluating Large Multimodal Models in Literacy](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_CC-OCR_A_Comprehensive_and_Challenging_OCR_Benchmark_for_Evaluating_Large_ICCV_2025_paper.pdf)
* [MSA2 Multi-task Framework with Structure-aware and Style-adaptive Character Representation for Open-set Chinese Text Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_MSA2_Multi-task_Framework_with_Structure-aware_and_Style-adaptive_Character_Representation_for_ICCV_2025_paper.pdf)
* [A Token-level Text Image Foundation Model for Document Understanding](http://arxiv.org/abs/2503.02304)<br>:star:[code](https://github.com/Token-family/TokenFD)
* 文本生成
  * [UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis](https://arxiv.org/pdf/2507.00992v1)
* 甲骨文解读
  * [OracleFusion: Assisting the Decipherment of Oracle Bone Script with Structurally Constrained Semantic Typography](http://arxiv.org/pdf/2506.21101v1)
* 文档矫正
  * [ForCenNet: Foreground-Centric Network for Document Image Rectification](https://arxiv.org/pdf/2507.19804v1)<br>:star:[code](https://github.com/caipeng328/ForCenNet)
* 表格理解
  * [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/pdf/2508.06492v1)<br>:star:[code](https://github.com/yuweiyang-anu/ECD)
* 场景文本检索
  * [MonSTeR a Unified Model for Motion Scene Text Retrieval](http://arxiv.org/abs/2510.03200)<br>:star:[code](https://github.com/colloroneluca/MonSTeR)
* 场景文本识别
  * [SVTRv2 CTC Beats Encoder-Decoder Models in Scene Text Recognition](http://arxiv.org/abs/2411.15858)<br>:star:[code](https://github.com/Topdu/OpenOCR)
  * [TextSSR Diffusion-based Data Synthesis for Scene Text Recognition](http://arxiv.org/abs/2412.01137)<br>:star:[code](https://github.com/YesianRohn/TextSSR)

<a name="19"/>

## 19.Video
* [SciVid: Cross-Domain Evaluation of Video Models in Scientific Applications](https://arxiv.org/pdf/2507.03578v1)<br>:star:[code](https://github.com/google-deepmind/scivid)
* [Fine-grained Spatiotemporal Grounding on Egocentric Videos](https://arxiv.org/pdf/2508.00518v1)<br>:star:[code](https://github.com/LaVi-Lab/EgoMask)
* [LV-MAE Learning Long Video Representations through Masked-Embedding Autoencoders](https://openaccess.thecvf.com/content/ICCV2025/papers/Naiman_LV-MAE_Learning_Long_Video_Representations_through_Masked-Embedding_Autoencoders_ICCV_2025_paper.pdf)
* [Learning Streaming Video Representation via Multitask Training](http://arxiv.org/abs/2504.20041)
* [Tree-NeRV Efficient Non-Uniform Sampling for Neural Video Representation via Tree-Structured Feature Grids](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_Tree-NeRV_Efficient_Non-Uniform_Sampling_for_Neural_Video_Representation_via_Tree-Structured_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/zhaojiancheng007/Tree-NeRV.git)
* [What Changed and What Could Have Changed State-Change Counterfactuals for Procedure-Aware Video Representation Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Kung_What_Changed_and_What_Could_Have_Changed_State-Change_Counterfactuals_for_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/HCIS-Lab/counterfactual-video-pretrain)
* [One Trajectory One Token Grounded Video Tokenization via Panoptic Sub-object Trajectory](http://arxiv.org/abs/2505.23617)
* [FrameFusion Combining Similarity and Importance for Video Token Reduction on Large Vision Language Models](http://arxiv.org/abs/2501.01986)<br>:star:[code](https://github.com/thu-nics/FrameFusion)
* [Progressive Growing of Video Tokenizers for Temporally Compact Latent Spaces](http://arxiv.org/abs/2501.05442)
* [StreamMind Unlocking Full Frame Rate Streaming Video Dialogue through Event-Gated Cognition](http://arxiv.org/abs/2503.06220)
* 视频理解
  * [Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs](http://arxiv.org/pdf/2506.22139v1)
  * [MDP3 A Training-free Approach for List-wise Frame Selection in Video-LLMs](http://arxiv.org/abs/2501.02885)
  * [ARGUS Hallucination and Omission Evaluation in Video-LLMs](http://arxiv.org/abs/2506.07371)
  * [Flash-VStream: Efficient Real-Time Understanding for Long Video Streams](https://arxiv.org/pdf/2506.23825v1)<br>:star:[code](https://github.com/IVGSZ/Flash-VStream)
  * [AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding](https://arxiv.org/pdf/2507.02591v1)
  * [MCAM: Multimodal Causal Analysis Model for Ego-Vehicle-Level Driving Video Understanding](https://arxiv.org/pdf/2507.06072v1)<br>:star:[code](https://github.com/SixCorePeach/MCAM)
  * [DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding](https://arxiv.org/pdf/2507.15569v1)
  * [VideoLLaMB Long Streaming Video Understanding with Recurrent Memory Bridges](http://arxiv.org/abs/2409.01071)
  * [Beyond Training Dynamic Token Merging for Zero-Shot Video Understanding](http://arxiv.org/abs/2411.14401)<br>:star:[code](https://github.com/Jam1ezhang/DYTO)
  * [LVAgent Long Video Understanding by Multi-Round Dynamical Collaboration of MLLM Agents](http://arxiv.org/abs/2503.10200)<br>:star:[code](https://github.com/64327069/LVAgent)
  * [Streaming VideoLLMs for Real-Time Procedural Video Understanding](http://arxiv.org/abs/2504.13915)
  * [From Trial to Triumph Advancing Long Video Understanding via Visual Context Sample Scaling and Self-reward Alignment](http://arxiv.org/abs/2503.20472)
  * [AdsQA Towards Advertisement Video Understanding](http://arxiv.org/abs/2509.08621)
  * [Principles of Visual Tokens for Efficient Video Understanding](http://arxiv.org/abs/2411.13626)<br>:star:[code](https://github.com/maggieHao/Efficient-LITE)
  * [Open-ended Hierarchical Streaming Video Understanding with Vision Language Models](http://arxiv.org/abs/2509.12145)
  * [VideoAds for Fast-Paced Video Understanding](http://arxiv.org/abs/2504.09282)
  * [VCA Video Curious Agent for Long Video Understanding](http://arxiv.org/abs/2412.10471)
* 视频摘要
  * [SummDiff Generative Modeling of Video Summarization with Diffusion](http://arxiv.org/abs/2510.08458)
* 视频时序定位
  * [Hierarchical Event Memory for Accurate and Low-latency Online Video Temporal Grounding](https://arxiv.org/pdf/2508.04546v1)<br>:star:[code](https://github.com/minghangz/OnVTG)
  * [KDA Knowledge Diffusion Alignment with Enhanced Context for Video Temporal Grounding](https://openaccess.thecvf.com/content/ICCV2025/papers/Ran_KDA_Knowledge_Diffusion_Alignment_with_Enhanced_Context_for_Video_Temporal_ICCV_2025_paper.pdf)
  * [Sparse-Dense Side-Tuner for efficient Video Temporal Grounding](https://openaccess.thecvf.com/content/ICCV2025/papers/Pujol-Perich_Sparse-Dense_Side-Tuner_for_efficient_Video_Temporal_Grounding_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/davidpujol/SDST)
  * [TimeExpert An Expert-Guided Video LLM for Video Temporal Grounding](http://arxiv.org/abs/2508.01699)
  * [OVG-HQ Online Video Grounding with Hybrid-modal Queries](https://openaccess.thecvf.com/content/ICCV2025/papers/Zeng_OVG-HQ_Online_Video_Grounding_with_Hybrid-modal_Queries_ICCV_2025_paper.pdf)
  * [Vid-Group Temporal Video Grounding Pretraining from Unlabeled Videos in the Wild](https://openaccess.thecvf.com/content/ICCV2025/papers/Bao_Vid-Group_Temporal_Video_Grounding_Pretraining_from_Unlabeled_Videos_in_the_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/baopj/Vid-Group)
  * [Enrich and Detect Video Temporal Grounding with Multimodal LLMs](https://openaccess.thecvf.com/content/ICCV2025/papers/Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs_ICCV_2025_paper.pdf)
  * [VTimeCoT Thinking by Drawing for Video Temporal Grounding and Reasoning](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_VTimeCoT_Thinking_by_Drawing_for_Video_Temporal_Grounding_and_Reasoning_ICCV_2025_paper.pdf)
* 视频异常检测
  * [Mixture of Experts Guided by Gaussian Splatters Matters A new Approach to Weakly-Supervised Video Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new_ICCV_2025_paper.pdf)
  * [Sequential keypoint density estimator an overlooked baseline of skeleton-based video anomaly detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video_ICCV_2025_paper.pdf)
* 视频时刻检索  
  * [The Devil is in the Spurious Correlations Boosting Moment Retrieval with Dynamic Learning](http://arxiv.org/abs/2501.07305)<br>:star:[code](https://github.com/xyangzhou/TD-DETR)
  * [Augmenting Moment Retrieval Zero-Dependency Two-Stage Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Wei_Augmenting_Moment_Retrieval_Zero-Dependency_Two-Stage_Learning_ICCV_2025_paper.pdf)
  * [Borrowing Eyes for the Blind Spot Overcoming Data Scarcity in Malicious Video Detection via Cross-Domain Retrieval Augmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Hong_Borrowing_Eyes_for_the_Blind_Spot_Overcoming_Data_Scarcity_in_ICCV_2025_paper.pdf)
* 视频帧插值
  * [TLB-VFI Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lyu_TLB-VFI_Temporal-Aware_Latent_Brownian_Bridge_Diffusion_for_Video_Frame_Interpolation_ICCV_2025_paper.pdf)
* 视频预测
  * [OCK Unsupervised Dynamic Video Prediction with Object-Centric Kinematics](http://arxiv.org/abs/2404.18423)
  * [AID Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction](http://arxiv.org/abs/2406.06465)
* 视频定制
  * [DreamRelation Relation-Centric Video Customization](http://arxiv.org/abs/2503.07602)<br>:house:[project](https://dreamrelation.github.io/)
  * [MagicID Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization](http://arxiv.org/abs/2503.12689)
  * [PersonalVideo High ID-Fidelity Video Customization without Dynamic and Semantic Degradation](http://arxiv.org/abs/2411.17048)
  * [DualReal Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization](http://arxiv.org/abs/2505.02192)<br>:house:[project](https://wenc-k.github.io/dualreal-customization)

<a name="18"/>

## 18.Person Re-Identification(行人重识别)
* [Multi-modal Multi-platform Person Re-Identification Benchmark and Method](https://openaccess.thecvf.com/content/ICCV2025/papers/Ha_Multi-modal_Multi-platform_Person_Re-Identification_Benchmark_and_Method_ICCV_2025_paper.pdf)
* [VIPerson Flexibly Generating Virtual Identity for Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_VIPerson_Flexibly_Generating_Virtual_Identity_for_Person_Re-Identification_ICCV_2025_paper.pdf)<br>:house:[project](https://isee-laboratory.github.io/VIPerson)
* [One-Shot Knowledge Transfer for Scalable Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_One-Shot_Knowledge_Transfer_for_Scalable_Person_Re-Identification_ICCV_2025_paper.pdf)
* [OpenAnimals Revisiting Person Re-Identification for Animals Towards Better Generalization](http://arxiv.org/abs/2410.00204)
* [Bridging the Sky and Ground Towards View-Invariant Feature Learning for Aerial-Ground Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2025/papers/Khalid_Bridging_the_Sky_and_Ground_Towards_View-Invariant_Feature_Learning_for_ICCV_2025_paper.pdf)
* [Prompt-driven Transferable Adversarial Attack on Person Re-Identification with Attribute-aware Textual Inversion](http://arxiv.org/abs/2502.19697)
* [Cross-Category Subjectivity Generalization for Style-Adaptive Sketch Re-ID](https://openaccess.thecvf.com/content/ICCV2025/papers/Hu_Cross-Category_Subjectivity_Generalization_for_Style-Adaptive_Sketch_Re-ID_ICCV_2025_paper.pdf)
* 基于视频的重识别
  * [HAMoBE: Hierarchical and Adaptive Mixture of Biometric Experts for Video-based Person ReID](https://arxiv.org/pdf/2508.05038v1)
* 换衣重识别
  * [Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement](https://arxiv.org/pdf/2507.07230v1)<br>:star:[code](https://github.com/ppriyank/ICCV-CSCI-Person-ReID)
* 终身重识别
  * [Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification](https://arxiv.org/pdf/2507.01884v1)<br>:star:[code](https://github.com/zhoujiahuan1991/ICCV2025-SPRED)
* 红外可见光
  * [Weakly Supervised Visible-Infrared Person Re-Identification via Heterogeneous Expert Collaborative Consistency Learning](http://arxiv.org/abs/2507.12942)<br>:star:[code](https://github.com/KongLingqi2333/WSL-VIReID)
  * [Augmented and Softened Matching for Unsupervised Visible-Infrared Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2025/papers/Pang_Augmented_and_Softened_Matching_for_Unsupervised_Visible-Infrared_Person_Re-Identification_ICCV_2025_paper.pdf)
  * [Unsupervised Visible-Infrared Person Re-identification under Unpaired Settings](https://openaccess.thecvf.com/content/ICCV2025/papers/Yao_Unsupervised_Visible-Infrared_Person_Re-identification_under_Unpaired_Settings_ICCV_2025_paper.pdf)
* 行为理解
  * [ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video](https://arxiv.org/pdf/2508.09818v1)
  * [HiERO Understanding the Hierarchy of Human Behavior Enhances Reasoning on Egocentric Videos](http://arxiv.org/abs/2505.12911)
  * [Towards Human-like Virtual Beings Simulating Human Behavior in 3D Scenes](https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_Towards_Human-like_Virtual_Beings_Simulating_Human_Behavior_in_3D_Scenes_ICCV_2025_paper.pdf)
* 行人检索
  * [ChatReID Open-ended Interactive Person Retrieval via Hierarchical Progressive Tuning for Vision Language Models](http://arxiv.org/abs/2502.19958)
  * [Towards Robustness of Person Search against Corruptions](https://openaccess.thecvf.com/content/ICCV2025/papers/Son_Towards_Robustness_of_Person_Search_against_Corruptions_ICCV_2025_paper.pdf)
  * [Leveraging Prior Knowledge of Diffusion Model for Person Search](http://arxiv.org/abs/2510.01841)
* 步态识别
  * [CarGait Cross-Attention based Re-ranking for Gait recognition](http://arxiv.org/abs/2503.03501)
  * [Learning A Unified Template for Gait Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_Learning_A_Unified_Template_for_Gait_Recognition_ICCV_2025_paper.pdf)
  * [Gait-X Exploring X modality for Generalized Gait Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Gait-X_Exploring_X_modality_for_Generalized_Gait_Recognition_ICCV_2025_paper.pdf)

<a name="17"/>

## 17.Action Recognition(动作识别)
* [ProbRes Probabilistic Jump Diffusion for Open-World Egocentric Activity Recognition](http://arxiv.org/abs/2504.03948)
* [DeSPITE Exploring Contrastive Deep Skeleton-Pointcloud-IMU-Text Embeddings for Advanced Point Cloud Human Activity Understanding](https://openaccess.thecvf.com/content/ICCV2025/papers/Kreutz_DeSPITE_Exploring_Contrastive_Deep_Skeleton-Pointcloud-IMU-Text_Embeddings_for_Advanced_Point_Cloud_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/thkreutz/despite)
* [Punching Bag vs. Punching Person: Motion Transferability in Videos](https://arxiv.org/pdf/2508.00085v1)<br>:star:[code](https://github.com/raiyaan-abdullah/Motion-Transfer)
* [Learning to Generalize without Bias for Open-Vocabulary Action Recognition](http://arxiv.org/abs/2502.20158)
* [SAMPLE Semantic Alignment through Temporal-Adaptive Multimodal Prompt Learning for Event-Based Open-Vocabulary Action Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_SAMPLE_Semantic_Alignment_through_Temporal-Adaptive_Multimodal_Prompt_Learning_for_Event-Based_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/JingWang-self/SAMPLE)
* [Exploiting Frequency Dynamics for Enhanced Multimodal Event-based Action Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Cao_Exploiting_Frequency_Dynamics_for_Enhanced_Multimodal_Event-based_Action_Recognition_ICCV_2025_paper.pdf)
* [Less Static More Private Towards Transferable Privacy-Preserving Action Recognition by Generative Decoupled Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Xia_Less_Static_More_Private_Towards_Transferable_Privacy-Preserving_Action_Recognition_by_ICCV_2025_paper.pdf)
* [Dynamic Group Detection using VLM-augmented Temporal Groupness Graph](http://arxiv.org/abs/2509.04758)<br>:star:[code](https://github.com/irajisamurai/VLM-GroupDetection.git)
* 动作预测
  * [Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions](https://arxiv.org/pdf/2507.09446v1)<br>:star:[code](https://github.com/Yuanhong-Zheng/EMPMP)
  * [PriorMotion Generative Class-Agnostic Motion Prediction with Raster-Vector Motion Field Priors](https://openaccess.thecvf.com/content/ICCV2025/papers/Qian_PriorMotion_Generative_Class-Agnostic_Motion_Prediction_with_Raster-Vector_Motion_Field_Priors_ICCV_2025_paper.pdf)
  * [Gaussian-based World Model Gaussian Priors for Voxel-Based Occupancy Prediction and Future Motion Prediction](https://openaccess.thecvf.com/content/ICCV2025/papers/Feng_Gaussian-based_World_Model_Gaussian_Priors_for_Voxel-Based_Occupancy_Prediction_and_ICCV_2025_paper.pdf)
  * [Proxy-Bridged Game Transformer for Interactive Extreme Motion Prediction](https://openaccess.thecvf.com/content/ICCV2025/papers/Fang_Proxy-Bridged_Game_Transformer_for_Interactive_Extreme_Motion_Prediction_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/joyfang1106/pgformer)
* 小样本动作识别
  * [Beyond Label Semantics: Language-Guided Action Anatomy for Few-shot Action Recognition](https://arxiv.org/pdf/2507.16287v1)
  * [Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition](https://arxiv.org/pdf/2508.03695v1)<br>:star:[code](https://trokens-iccv25.github.io)<br>:star:[code](https://github.com/pulkitkumar95/trokens)
  * [D2ST-Adapter Disentangled-and-Deformable Spatio-Temporal Adapter for Few-shot Action Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Pei_D2ST-Adapter_Disentangled-and-Deformable_Spatio-Temporal_Adapter_for_Few-shot_Action_Recognition_ICCV_2025_paper.pdf)
* 动作分割
  * [Skeleton Motion Words for Unsupervised Skeleton-Based Temporal Action Segmentation](https://arxiv.org/pdf/2508.04513v1)<br>:star:[code](https://github.com/bachlab/SMQ)
  * [Multi-Modal Few-Shot Temporal Action Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_Multi-Modal_Few-Shot_Temporal_Action_Segmentation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ZijiaLewisLu/ICCV2025-MMF-TAS)
  * [DuoCLR Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation](http://arxiv.org/abs/2509.05543)
  * [Joint Self-Supervised Video Alignment and Action Segmentation](http://arxiv.org/abs/2503.16832)
  * [CLOT: Closed Loop Optimal Transport for Unsupervised Action Segmentation](https://arxiv.org/pdf/2507.03539v1)
* 动作检测
  * [Scaling Action Detection AdaTAD with Transformer-Enhanced Temporal-Spatial Adaptation](https://openaccess.thecvf.com/content/ICCV2025/papers/Agrawal_Scaling_Action_Detection_AdaTAD_with_Transformer-Enhanced_Temporal-Spatial_Adaptation_ICCV_2025_paper.pdf)
  * [MMAD Multi-label Micro-Action Detection in Videos](http://arxiv.org/abs/2407.05311)<br>:star:[code](https://github.com/VUT-HFUT/Micro-Action)
* 基于骨架的动作识别
  * [Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition](http://arxiv.org/pdf/2506.22179v1)
  * [Bridging the Skeleton-Text Modality Gap Diffusion-Powered Modality Alignment for Zero-shot Skeleton-based Action Recognition](http://arxiv.org/abs/2411.10745)
  * [Hierarchical-aware Orthogonal Disentanglement Framework for Fine-grained Skeleton-based Action Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Chang_Hierarchical-aware_Orthogonal_Disentanglement_Framework_for_Fine-grained_Skeleton-based_Action_Recognition_ICCV_2025_paper.pdf)
  * [Bridging Class Imbalance and Partial Labeling via Spectral-Balanced Energy Propagation for Skeleton-based Action Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Bridging_Class_Imbalance_and_Partial_Labeling_via_Spectral-Balanced_Energy_Propagation_ICCV_2025_paper.pdf)
  * [Adaptive Hyper-Graph Convolution Network for Skeleton-based Human Action Recognition with Virtual Connections](http://arxiv.org/abs/2411.14796)<br>:star:[code](https://github.com/6UOOON9/Hyper-GCN)
* 动作预期(action anticipation)
  * [MixANT Observation-dependent Memory Propagation for Stochastic Dense Action Anticipation](http://arxiv.org/abs/2509.11394)<br>:house:[project](https://talalwasim.github.io/MixANT)

<a name="16"/>

## 16.Human Motion
* [Cycle Consistency as Reward Learning Image-Text Alignment without Human Preferences](http://arxiv.org/abs/2506.02095)<br>:house:[project](https://cyclereward.github.io/)
* [IMoRe Implicit Program-Guided Reasoning for Human Motion QA](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_IMoRe_Implicit_Program-Guided_Reasoning_for_Human_Motion_QA_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/LUNAProject22/IMoRe)
* [Future-Aware Interaction Network For Motion Forecasting](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Future-Aware_Interaction_Network_For_Motion_Forecasting_ICCV_2025_paper.pdf)
* [Decouple and Track Benchmarking and Improving Video Diffusion Transformers For Motion Transfer](http://arxiv.org/abs/2503.17350)
* [Privacy-centric Deep Motion Retargeting for Anonymization of Skeleton-Based Motion Visualization](https://openaccess.thecvf.com/content/ICCV2025/papers/Carr_Privacy-centric_Deep_Motion_Retargeting_for_Anonymization_of_Skeleton-Based_Motion_Visualization_ICCV_2025_paper.pdf)
* [MagShield Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances](http://arxiv.org/abs/2506.22907)<br>:star:[code](https://github.com/YZ-Shiao/MagShield)
* [GENMO A GENeralist Model for Human MOtion](http://arxiv.org/abs/2505.01425)
* [KinMo Kinematic-aware Human Motion Understanding and Generation](http://arxiv.org/abs/2411.15472)<br>:house:[project](https://andypinxinliu.github.io/KinMo)
* [Continuous-Time Human Motion Field from Event Cameras](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Continuous-Time_Human_Motion_Field_from_Event_Cameras_ICCV_2025_paper.pdf)
* [Probabilistic Inertial Poser (ProbIP) Uncertainty-aware Human Motion Modeling from Sparse Inertial Sensors](https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_Probabilistic_Inertial_Poser_ProbIP_Uncertainty-aware_Human_Motion_Modeling_from_Sparse_ICCV_2025_paper.pdf)
* [MVTrajecter Multi-View Pedestrian Tracking with Trajectory Motion Cost and Trajectory Appearance Cost](http://arxiv.org/abs/2509.01157)
* [StyleMotif Multi-Modal Motion Stylization using Style-Content Cross Fusion](http://arxiv.org/abs/2503.21775)<br>:house:[project](https://stylemotif.github.io)
* [HumanSAM Classifying Human-centric Forgery Videos in Human Spatial Appearance and Motion Anomaly](http://arxiv.org/abs/2507.19924)
* [Contact-Aware Refinement of Human Pose Pseudo-Ground Truth via Bioimpedance Sensing](https://openaccess.thecvf.com/content/ICCV2025/papers/Forte_Contact-Aware_Refinement_of_Human_Pose_Pseudo-Ground_Truth_via_Bioimpedance_Sensing_ICCV_2025_paper.pdf)
* [Punching Bag vs Punching Person Motion Transferability in Videos](http://arxiv.org/abs/2508.00085)<br>:star:[code](https://github.com/raiyaan-abdullah/Motion-Transfer)
* 人体运动分割
  * [RoMo Robust Motion Segmentation Improves Structure from Motion](http://arxiv.org/abs/2411.18650)
  * [Temporal Rate Reduction Clustering for Human Motion Segmentation](http://arxiv.org/abs/2506.21249)<br>:star:[code](https://github.com/mengxianghan123/TR2C)
* 运动生成
  * [PINO: Person-Interaction Noise Optimization for Long-Duration and Customizable Motion Generation of Arbitrary-Sized Groups](https://arxiv.org/pdf/2507.19292v1)<br>:star:[code](https://sinc865.github.io/pino/)
  * [RapVerse Coherent Vocals and Whole-Body Motion Generation from Text](http://arxiv.org/abs/2405.20336)
  * [Go to Zero Towards Zero-shot Motion Generation with Million-scale Data](http://arxiv.org/abs/2507.07095)
  * [PUMPS: Skeleton-Agnostic Point-based Universal Motion Pre-Training for Synthesis in Human Motion Tasks](https://arxiv.org/pdf/2507.20170v1)
  * [GenM3 Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Shi_GenM3_Generative_Pretrained_Multi-path_Motion_Model_for_Text_Conditional_Human_ICCV_2025_paper.pdf)
  * [SMGDiff Soccer Motion Generation using Diffusion Probabilistic Models](http://arxiv.org/abs/2411.16216)
  * [FineMotion A Dataset and Benchmark with both Spatial and Temporal Annotation for Fine-grained Motion Generation and Editing](http://arxiv.org/abs/2507.19850)
  * [VMBench A Benchmark for Perception-Aligned Video Motion Generation](http://arxiv.org/abs/2503.10076)<br>:star:[code](https://github.com/AMAP-ML/VMBench)
  * [Towards Immersive Human-X Interaction A Real-Time Framework for Physically Plausible Motion Synthesis](https://openaccess.thecvf.com/content/ICCV2025/papers/Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible_ICCV_2025_paper.pdf)
  * [I2VControl Disentangled and Unified Video Motion Synthesis Control](http://arxiv.org/abs/2411.17765)<br>:house:[project](https://wanquanf.github.io/I2VControl)
  * [MoMaps Semantics-Aware Scene Motion Generation with Motion Maps](http://arxiv.org/abs/2510.11107)
  * [Morph A Motion-free Physics Optimization Framework for Human Motion Generation](http://arxiv.org/abs/2411.14951)
  * [MaskControl Spatio-Temporal Control for Masked Motion Synthesis](http://arxiv.org/abs/2410.10780)<br>:house:[project](https://anonymous-ai-agent.github.io/CAM)
  * [InterSyn Interleaved Learning for Dynamic Motion Synthesis in the Wild](http://arxiv.org/abs/2508.10297)
  * [Motion Synthesis with Sparse and Flexible Keyjoint Control](http://arxiv.org/abs/2503.15557)
  * [MotionStreamer Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space](http://arxiv.org/abs/2503.15451)<br>:house:[project](https://zju3dv.github.io/MotionStreamer/) :house:[project](https://zju3dv.github.io/MotionStreamer)
  * [SemTalk Holistic Co-speech Motion Generation with Frame-level Semantic Emphasis](http://arxiv.org/abs/2412.16563)
  * [InfiniDreamer Arbitrarily Long Human Motion Generation via Segment Score Distillation](http://arxiv.org/abs/2411.18303)
  * [Text-to-Any-Skeleton Motion Generation Without Retargeting](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Text-to-Any-Skeleton_Motion_Generation_Without_Retargeting_ICCV_2025_paper.pdf)
  * [MotionLab Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm](http://arxiv.org/abs/2502.02358)<br>:house:[project](https://diouo.github.io/motionlab.github.io)
  * [Motion-2-to-3 Leveraging 2D Motion Data for 3D Motion Generations](https://openaccess.thecvf.com/content/ICCV2025/papers/Guo_Motion-2-to-3_Leveraging_2D_Motion_Data_for_3D_Motion_Generations_ICCV_2025_paper.pdf)
  * [DIMO Diverse 3D Motion Generation for Arbitrary Objects](https://openaccess.thecvf.com/content/ICCV2025/papers/Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects_ICCV_2025_paper.pdf)
  * [You Think You ACT The New Task of Arbitrary Text to Motion Generation](http://arxiv.org/abs/2404.14745)<br>:star:[code](https://github.com/RunqiWang77/TAAT.github.io)
  * [Dual Reciprocal Learning of Language-based Human Motion Understanding and Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_Dual_Reciprocal_Learning_of_Language-based_Human_Motion_Understanding_and_Generation_ICCV_2025_paper.pdf)
* 运动重建
  * [UniEgoMotion A Unified Model for Egocentric Motion Reconstruction Forecasting and Generation](http://arxiv.org/abs/2508.01126)
* 运动估计
  * [Learning Large Motion Estimation from Intermediate Representations with a High-Resolution Optical Flow Dataset Featuring Long-Range Dynamic Motion](https://openaccess.thecvf.com/content/ICCV2025/papers/Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Chohoonhee/RelayFlow-4K)
  * [EMoTive Event-guided Trajectory Modeling for 3D Motion Estimation](http://arxiv.org/abs/2503.11371)
  * [MBTI Masked Blending Transformers with Implicit Positional Encoding for Frame-rate Agnostic Motion Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Huh_MBTI_Masked_Blending_Transformers_with_Implicit_Positional_Encoding_for_Frame-rate_ICCV_2025_paper.pdf)
  * [EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba](http://arxiv.org/abs/2508.10522)
* 舞蹈生成
  * [DanceEditor Towards Iterative Editable Music-driven Dance Generation with Open-Vocabulary Descriptions](http://arxiv.org/abs/2508.17342)<br>:house:[project](https://lzvsdy.github.io/DanceEditor)
  * [FreeDance Towards Harmonic Free-Number Group Dance Generation via a Unified Framework](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_FreeDance_Towards_Harmonic_Free-Number_Group_Dance_Generation_via_a_Unified_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Tsukasane/FreeDance)
  * [MDD A Dataset for Text-and-Music Conditioned Duet Dance Generation](http://arxiv.org/abs/2508.16911)
  * [Music-Aligned Holistic 3D Dance Generation via Hierarchical Motion Modeling](http://arxiv.org/abs/2507.14915)<br>:house:[project](https://xjli360.github.io/SoulDance)
* 运动编辑
  * [MotionDiff Training-free Zero-shot Interactive Motion Editing via Flow-assisted Multi-view Diffusion](http://arxiv.org/abs/2503.17695)
  * [MotionFollower Editing Video Motion via Score-Guided Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Tu_MotionFollower_Editing_Video_Motion_via_Score-Guided_Diffusion_ICCV_2025_paper.pdf)


<a name="15"/>

## 15.pose
* [Detection Pose Estimation and Segmentation for Multiple Bodies Closing the Virtuous Circle](http://arxiv.org/abs/2412.01562)
* [HIS-GPT Towards 3D Human-In-Scene Multimodal Understanding](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_HIS-GPT_Towards_3D_Human-In-Scene_Multimodal_Understanding_ICCV_2025_paper.pdf)
* [AdaHuman Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion](http://arxiv.org/abs/2505.24877)
* [TriDi Trilateral Diffusion of 3D Humans Objects and Interactions](http://arxiv.org/abs/2412.06334)
* [PHD Personalized 3D Human Body Fitting with Point Diffusion](http://arxiv.org/abs/2508.21257)
* [SIGMAN Scaling 3D Human Gaussian Generation with Millions of Assets](http://arxiv.org/abs/2504.06982)
* [Group Inertial Poser Multi-Person Pose and Global Translation from Sparse Inertial Sensors and Ultra-Wideband Ranging](https://openaccess.thecvf.com/content/ICCV2025/papers/Xue_Group_Inertial_Poser_Multi-Person_Pose_and_Global_Translation_from_Sparse_ICCV_2025_paper.pdf)
* 人体网格恢复
  * [AJAHR Amputated Joint Aware 3D Human Mesh Recovery](http://arxiv.org/abs/2509.19939)<br>:house:[project](https://chojinie.github.io/project_AJAHR)
  * [Humans as Checkerboards Calibrating Camera Motion Scale for World-Coordinate Human Mesh Recovery](http://arxiv.org/abs/2407.00574)<br>:house:[project](https://martayang.github.io/HAC)
  * [Fish2Mesh Transformer 3D Human Mesh Recovery from Egocentric Vision](https://openaccess.thecvf.com/content/ICCV2025/papers/Shen_Fish2Mesh_Transformer_3D_Human_Mesh_Recovery_from_Egocentric_Vision_ICCV_2025_paper.pdf)
* 人体重建
  * [PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single Image](https://arxiv.org/pdf/2507.17332v1)<br>:star:[code](https://hygenie1228.github.io/PARTE/)
  * [LHM Large Animatable Human Reconstruction Model for Single Image to 3D in Seconds](https://openaccess.thecvf.com/content/ICCV2025/papers/Qiu_LHM_Large_Animatable_Human_Reconstruction_Model_for_Single_Image_to_ICCV_2025_paper.pdf)
  * [CHROME Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image](http://arxiv.org/abs/2503.15671)
* 手势合成
  * [SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning](https://arxiv.org/pdf/2507.19359v1)<br>:star:[code](https://semgesture.github.io/)
  * [Democratizing High-Fidelity Co-Speech Gesture Video Generation](http://arxiv.org/abs/2507.06812)<br>:house:[project](https://mpi-lab.github.io/Democratizing-CSG)
  * [GestureLSM Latent Shortcut based Co-Speech Gesture Generation with Spatial-Temporal Modeling](http://arxiv.org/abs/2501.18898) 
  * [Understanding Co-speech Gestures in-the-wild](http://arxiv.org/abs/2503.22668)
  * [GestureHYDRA Semantic Co-speech Gesture Synthesis via Hybrid Modality Diffusion Transformer and Cascaded-Synchronized Retrieval-Augmented Generation](http://arxiv.org/abs/2507.22731) 
* HPE
  * [LDPose Towards Inclusive Human Pose Estimation for Limb-Deficient Individuals in the Wild](https://openaccess.thecvf.com/content/ICCV2025/papers/Ying_LDPose_Towards_Inclusive_Human_Pose_Estimation_for_Limb-Deficient_Individuals_in_ICCV_2025_paper.pdf)
  * [From Sharp to Blur Unsupervised Domain Adaptation for 2D Human Pose Estimation Under Extreme Motion Blur Using Event Cameras](http://arxiv.org/abs/2507.22438)<br>:star:[code](https://github.com/kmax2001/EvSharp2Blur)
  * [High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation](http://arxiv.org/abs/2510.11017)
  * [Generative Modeling of Shape-Dependent Self-Contact Human Poses](http://arxiv.org/abs/2509.23393)
  * 3D HPE
    * [DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior](https://arxiv.org/pdf/2508.00599v1)<br>:star:[code](https://github.com/moonbow721/DPoser)
    * [A Structure-aware and Motion-adaptive Framework for 3D Human Pose Estimation with Mamba](http://arxiv.org/abs/2507.19852)
    * [Bring Your Rear Cameras for Egocentric 3D Human Pose Estimation](http://arxiv.org/abs/2503.11652)
    * [VOccl3D A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions](http://arxiv.org/abs/2508.06757)
    * [PoseAnchor Robust Root Position Estimation for 3D Human Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_PoseAnchor_Robust_Root_Position_Estimation_for_3D_Human_Pose_Estimation_ICCV_2025_paper.pdf)
    * [PersPose 3D Human Pose Estimation with Perspective Encoding and Perspective Rotation](http://arxiv.org/abs/2508.17239)<br>:star:[code](https://github.com/KenAdamsJoseph/PersPose)
* 人体姿态生成
  * [Head2Body Body Pose Generation from Multi-sensory Head-mounted Inputs](https://openaccess.thecvf.com/content/ICCV2025/papers/Tran_Head2Body_Body_Pose_Generation_from_Multi-sensory_Head-mounted_Inputs_ICCV_2025_paper.pdf)
* 手部姿态
  * [Prior-aware Dynamic Temporal Modeling Framework for Sequential 3D Hand Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Ren_Prior-aware_Dynamic_Temporal_Modeling_Framework_for_Sequential_3D_Hand_Pose_ICCV_2025_paper.pdf)
  * [MaskHand Generative Masked Modeling for Robust Hand Mesh Reconstruction in the Wild](http://arxiv.org/abs/2412.13393)<br>:house:[project](https://m-usamasaleem.github.io/publication) :house:[project](https://m-usamasaleem.github.io/publication/MaskHand/MaskHand.html)
  * [Diffusion-based 3D Hand Motion Recovery with Intuitive Physics](http://arxiv.org/abs/2508.01835)
* 手语生成
  * [Signs as Tokens A Retrieval-Enhanced Multilingual Sign Language Generator](http://arxiv.org/abs/2411.17799)
  * [GReg Geometry-Aware Region Refinement for Sign Language Video Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Shi_GReg_Geometry-Aware_Region_Refinement_for_Sign_Language_Video_Generation_ICCV_2025_paper.pdf)
  * [Cross-View Isolated Sign Language Recognition via View Synthesis and Feature Disentanglement](https://openaccess.thecvf.com/content/ICCV2025/papers/Shen_Cross-View_Isolated_Sign_Language_Recognition_via_View_Synthesis_and_Feature_ICCV_2025_paper.pdf)
  * [Leveraging the Power of MLLMs for Gloss-Free Sign Language Translation](http://arxiv.org/abs/2411.16789)<br>:star:[code](https://github.com/helpmeIamnewbie/MMSLT)
* 关键点检测
  * [Towards Annotation-Free Evaluation KPAScore for Human Keypoint Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Towards_Annotation-Free_Evaluation_KPAScore_for_Human_Keypoint_Detection_ICCV_2025_paper.pdf)
  * [VoxelKP A Voxel-based Network Architecture for Human Keypoint Estimation in LiDAR Data](http://arxiv.org/abs/2312.08871)<br>:star:[code](https://github.com/shijianjian/VoxelKP)


<a name="14"/>


## 14.Object Track(目标跟踪)
* [Efficient Track Anything](http://arxiv.org/abs/2411.18933)<br>:star:[code](https://github.com/yformer/EfficientTAM)
* [UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions](https://arxiv.org/pdf/2507.00648v1)<br>:star:[code](https://github.com/Z-Z188/UMDATrack)
* [Is Tracking really more challenging in First Person Egocentric Vision?](https://arxiv.org/pdf/2507.16015v1)
* [What You Have is What You Track: Adaptive and Robust Multimodal Tracking](https://arxiv.org/pdf/2507.05899v1)<br>:star:[code](https://github.com/supertyd/FlexTrack/tree/main)
* [Temporal Unlearnable Examples: Preventing Personal Video Data from Unauthorized Exploitation by Object Tracking](https://arxiv.org/pdf/2507.07483v1)
* [General Compression Framework for Efficient Transformer Object Tracking](http://arxiv.org/abs/2409.17564)<br>:star:[code](https://github.com/LingyiHongfd/CompressTracker)
* [COVTrack Continuous Open-Vocabulary Tracking via Adaptive Multi-Cue Fusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Qian_COVTrack_Continuous_Open-Vocabulary_Tracking_via_Adaptive_Multi-Cue_Fusion_ICCV_2025_paper.pdf)
* [Attention to Trajectory Trajectory-Aware Open-Vocabulary Tracking](http://arxiv.org/abs/2503.08145)<br>:star:[code](https://github.com/Nathan-Li123/TRACT)
* [BlinkTrack Feature Tracking over 80 FPS via Events and Images](http://arxiv.org/abs/2409.17981)<br>:star:[code](https://github.com/ColieShen/BlinkTrack)
* [egoPPG Heart Rate Estimation from Eye-Tracking Cameras in Egocentric Systems to Benefit Downstream Vision Tasks](http://arxiv.org/abs/2502.20879)
* [M2EIT Multi-Domain Mixture of Experts for Robust Neural Inertial Tracking](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_M2EIT_Multi-Domain_Mixture_of_Experts_for_Robust_Neural_Inertial_Tracking_ICCV_2025_paper.pdf)
* [CAT A Unified Click-and-Track Framework for Realistic Tracking](https://openaccess.thecvf.com/content/ICCV2025/papers/Yuan_CAT_A_Unified_Click-and-Track_Framework_for_Realistic_Tracking_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ysyuann/CAT)
* [SMSTracker Tri-path Score Mask Sigma Fusion for Multi-Modal Tracking](https://openaccess.thecvf.com/content/ICCV2025/papers/Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Leezed525/SMSTracker)
* [How To Make Your Cell Tracker Say I dunno](https://openaccess.thecvf.com/content/ICCV2025/papers/Paul_How_To_Make_Your_Cell_Tracker_Say_I_dunno_ICCV_2025_paper.pdf)
* [What You Have is What You Track Adaptive and Robust Multimodal Tracking](http://arxiv.org/abs/2507.05899)
* [Tracking Tiny Drones against Clutter Large-Scale Infrared Benchmark with Motion-Centric Adaptive Algorithm](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Tracking_Tiny_Drones_against_Clutter_Large-Scale_Infrared_Benchmark_with_Motion-Centric_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/zhangjiahao02/MCATrack)
* 多目标跟踪
  * [Language Decoupling with Fine-grained Knowledge Guidance for Referring Multi-object Tracking](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Language_Decoupling_with_Fine-grained_Knowledge_Guidance_for_Referring_Multi-object_Tracking_ICCV_2025_paper.pdf)
  * [VOVTrack Exploring the Potentiality in Raw Videos for Open-Vocabulary Multi-Object Tracking](https://openaccess.thecvf.com/content/ICCV2025/papers/Qian_VOVTrack_Exploring_the_Potentiality_in_Raw_Videos_for_Open-Vocabulary_Multi-Object_ICCV_2025_paper.pdf)
  * [LA-MOTR End-to-End Multi-Object Tracking by Learnable Association](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_LA-MOTR_End-to-End_Multi-Object_Tracking_by_Learnable_Association_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/PenK1nG/LA-MOTR)
* 点跟踪
  * [TAPNext Tracking Any Point (TAP) as Next Token Prediction](http://arxiv.org/abs/2504.05579)
  * [AllTracker Efficient Dense Point Tracking at High Resolution](http://arxiv.org/abs/2506.07310)
  * [CoTracker3 Simpler and Better Point Tracking by Pseudo-Labelling Real Videos](http://arxiv.org/abs/2410.11831)
  * [SpatialTrackerV2: 3D Point Tracking Made Easy](https://arxiv.org/pdf/2507.12462v1)<br>:house:[project](https://huggingface.co/spaces/Yuxihenry/SpatialTrackerV2)<br>:star:[code](https://github.com/henry123-boy/SpaTrackerV2)
  * [ReTracker Exploring Image Matching for Robust Online Any Point Tracking](https://openaccess.thecvf.com/content/ICCV2025/papers/Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking_ICCV_2025_paper.pdf)
  * [Event-aided Dense and Continuous Point Tracking Everywhere and Anytime](https://openaccess.thecvf.com/content/ICCV2025/papers/Wan_Event-aided_Dense_and_Continuous_Point_Tracking_Everywhere_and_Anytime_ICCV_2025_paper.pdf)
  * [Online Dense Point Tracking with Streaming Memory](http://arxiv.org/abs/2503.06471)<br>:house:[project](https://dqiaole.github.io/SPOT)
  * [Multi-View 3D Point Tracking](https://openaccess.thecvf.com/content/ICCV2025/papers/Rajic_Multi-View_3D_Point_Tracking_ICCV_2025_paper.pdf)<br>:house:[project](https://ethz-vlg.github.io/mvtracker)
  * [SpatialTrackerV2 Advancing 3D Point Tracking with Explicit Camera Motion](https://openaccess.thecvf.com/content/ICCV2025/papers/Xiao_SpatialTrackerV2_Advancing_3D_Point_Tracking_with_Explicit_Camera_Motion_ICCV_2025_paper.pdf)
  * [MATE Motion-Augmented Temporal Consistency for Event-based Point Tracking](http://arxiv.org/abs/2412.01300)
* 3D跟踪
  * [GSOT3D Towards Generic 3D Single Object Tracking in the Wild](http://arxiv.org/abs/2412.02129)<br>:star:[code](https://github.com/ailovejinx/GSOT3D)
  * [Street Gaussians without 3D Object Tracker](http://arxiv.org/abs/2412.05548)
  * [ASCENT Annotation-free Self-supervised Contrastive Embeddings for 3D Neuron Tracking in Fluorescence Microscopy](https://openaccess.thecvf.com/content/ICCV2025/papers/Han_ASCENT_Annotation-free_Self-supervised_Contrastive_Embeddings_for_3D_Neuron_Tracking_in_ICCV_2025_paper.pdf)
* 视频目标跟踪
  * [XTrack Multimodal Training Boosts RGB-X Video Object Trackers](https://openaccess.thecvf.com/content/ICCV2025/papers/Tan_XTrack_Multimodal_Training_Boosts_RGB-X_Video_Object_Trackers_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/supertyd/XTrack)





<a name="13"/>

## 13.Object Detection(目标检测) 
* [DuET: Dual Incremental Object Detection via Exemplar-Free Task Arithmetic](http://arxiv.org/pdf/2506.21260v1)
* [Visual Modality Prompt for Adapting Vision-Language Object Detectors](http://arxiv.org/abs/2412.00622)<br>:star:[code](https://github.com/heitorrapela/ModPrompt)
* [Gradient Decomposition and Alignment for Incremental Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Luo_Gradient_Decomposition_and_Alignment_for_Incremental_Object_Detection_ICCV_2025_paper.pdf)
* [Visual Textualization for Image Prompted Object Detection](https://arxiv.org/pdf/2506.23785v1)<br>:star:[code](https://github.com/WitGotFlg/VisTex-OVLM)
* [Task-Specific Zero-shot Quantization-Aware Training for Object Detection](https://arxiv.org/pdf/2507.16782v1)<br>:star:[code](https://github.com/DFQ-Dojo/dfq-toolkit)
* [PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection](https://arxiv.org/pdf/2506.23581v1)
* [UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement](https://arxiv.org/pdf/2507.00721v1)<br>:star:[code](https://github.com/AMAP-ML/UPRE)
* [SFUOD: Source-Free Unknown Object Detection](https://arxiv.org/pdf/2507.17373v1)
* [LMM-Det: Make Large Multimodal Models Excel in Object Detection](https://arxiv.org/pdf/2507.18300v1)<br>:star:[code](https://github.com/360CVGroup/LMM-Det)
* [Adversarial Attention Perturbations for Large Object Detection Transformers](https://arxiv.org/pdf/2508.02987v1)<br>:star:[code](https://github.com/zacharyyahn/AFOG)
* [ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting](https://arxiv.org/pdf/2508.07089v1)
* [Gradient-Reweighted Adversarial Camouflage for Physical Object Detection Evasion](https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_Gradient-Reweighted_Adversarial_Camouflage_for_Physical_Object_Detection_Evasion_ICCV_2025_paper.pdf)
* [Unified Category-Level Object Detection and Pose Estimation from RGB Images using 3D Prototypes](http://arxiv.org/abs/2508.02157)
* [Cycle-Consistent Learning for Joint Layout-to-Image Generation and Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Cai_Cycle-Consistent_Learning_for_Joint_Layout-to-Image_Generation_and_Object_Detection_ICCV_2025_paper.pdf)
* [Rethinking Multi-modal Object Detection from the Perspective of Mono-Modality Feature Learning](http://arxiv.org/abs/2503.11780)<br>:star:[code](https://github.com/Zhao-Tian-yi/M2D-LIF)
* [Automated Model Evaluation for Object Detection via Prediction Consistency and Reliability](http://arxiv.org/abs/2508.12082)<br>:star:[code](https://github.com/YonseiML/autoeval-det)
* [Diffusion-based Source-biased Model for Single Domain Generalized Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_Diffusion-based_Source-biased_Model_for_Single_Domain_Generalized_Object_Detection_ICCV_2025_paper.pdf)
* [VISO Accelerating In-orbit Object Detection with Language-Guided Mask Learning and Sparse Inference](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_VISO_Accelerating_In-orbit_Object_Detection_with_Language-Guided_Mask_Learning_and_ICCV_2025_paper.pdf)
* [Beyond RGB Adaptive Parallel Processing for RAW Object Detection](http://arxiv.org/abs/2503.13163)
* [Dark-ISP Enhancing RAW Image Processing for Low-Light Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Guo_Dark-ISP_Enhancing_RAW_Image_Processing_for_Low-Light_Object_Detection_ICCV_2025_paper.pdf)
* [DoppDrive Doppler-Driven Temporal Aggregation for Improved Radar Object Detection](http://arxiv.org/abs/2508.12330)<br>:house:[project](https://yuvalhg.github.io/DoppDrive) :house:[project](https://yuvalhg.github.io/DoppDrive/)
* [Continual Adaptation Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios](http://arxiv.org/abs/2506.24063)
* [From Objects to Events Unlocking Complex Visual Understanding in Object Detectors via LLM-guided Symbolic Reasoning](http://arxiv.org/abs/2502.05843)
* [Online Generic Event Boundary Detection](http://arxiv.org/abs/2510.06855)
* [Revisiting Adversarial Patch Defenses on Object Detectors Unified Evaluation Large-Scale Dataset and New Insights](http://arxiv.org/abs/2508.00649)<br>:star:[code](https://github.com/Gandolfczjh/APDE)
* 小目标检测
  * [Event-based Tiny Object Detection A Benchmark Dataset and Baseline](http://arxiv.org/abs/2506.23575)
  * [Uncertainty-Aware Gradient Stabilization for Small Object Detection](http://arxiv.org/abs/2303.01803)
  * [DM-EFS Dynamically Multiplexed Expanded Features Set Form for Robust and Efficient Small Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Sharma_DM-EFS_Dynamically_Multiplexed_Expanded_Features_Set_Form_for_Robust_and_ICCV_2025_paper.pdf)
* 开集目标检测
  * [3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection](https://arxiv.org/pdf/2507.23567v1)
  * [ASGS Single-Domain Generalizable Open-Set Object Detection via Adaptive Subgraph Searching](https://openaccess.thecvf.com/content/ICCV2025/papers/Yuan_ASGS_Single-Domain_Generalizable_Open-Set_Object_Detection_via_Adaptive_Subgraph_Searching_ICCV_2025_paper.pdf)
* 三维目标检测
  * [Detect Anything 3D in the Wild](http://arxiv.org/abs/2504.07958)
  * [OcRFDet: Object-Centric Radiance Fields for Multi-View 3D Object Detection in Autonomous Driving](https://arxiv.org/pdf/2506.23565v1)<br>:star:[code](https://github.com/Mingqj/OcRFDet)
  * [MambaFusion: Height-Fidelity Dense Global Fusion for Multi-modal 3D Object Detection](https://arxiv.org/pdf/2507.04369v1)<br>:star:[code](https://github.com/AutoLab-SAI-SJTU/MambaFusion)
  * [Perspective-Invariant 3D Object Detection](https://arxiv.org/pdf/2507.17665v1)<br>:star:[code](https://pi3det.github.io)
  * [Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume Construction](https://arxiv.org/pdf/2507.18331v1)<br>:star:[code](https://github.com/RM-Zhang/SGCDet)
  * [Motal Unsupervised 3D Object Detection by Modality and Task-specific Knowledge Transfer](https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Motal_Unsupervised_3D_Object_Detection_by_Modality_and_Task-specific_Knowledge_ICCV_2025_paper.pdf)
  * [MemDistill Distilling LiDAR Knowledge into Memory for Camera-Only 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Kwon_MemDistill_Distilling_LiDAR_Knowledge_into_Memory_for_Camera-Only_3D_Object_ICCV_2025_paper.pdf)
  * [Accelerate 3D Object Detection Models via Zero-Shot Attention Key Pruning](http://arxiv.org/abs/2503.08101)<br>:star:[code](https://github.com/iseri27/tg_gbc)
  * [GeoFormer Geometry Point Encoder for 3D Object Detection with Graph-based Transformer](https://openaccess.thecvf.com/content/ICCV2025/papers/Jin_GeoFormer_Geometry_Point_Encoder_for_3D_Object_Detection_with_Graph-based_ICCV_2025_paper.pdf)
  * [Adaptive Dual Uncertainty Optimization Boosting Monocular 3D Object Detection under Test-Time Shifts](http://arxiv.org/abs/2508.20488)<br>:star:[code](https://github.com/hzcar/DUO)
  * [EVT Efficient View Transformation for Multi-Modal 3D Object Detection](http://arxiv.org/abs/2411.10715)
  * [CVFusion Cross-View Fusion of 4D Radar and Camera for 3D Object Detection](http://arxiv.org/abs/2507.04587)<br>:star:[code](https://github.com/zhzhzhzhzhz/CVFusion)
  * [OV-SCAN Semantically Consistent Alignment for Novel Object Discovery in Open-Vocabulary 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Chow_OV-SCAN_Semantically_Consistent_Alignment_for_Novel_Object_Discovery_in_Open-Vocabulary_ICCV_2025_paper.pdf)
  * [Unleashing the Temporal Potential of Stereo Event Cameras for Continuous-Time 3D Object Detection](http://arxiv.org/abs/2508.02288)<br>:star:[code](https://github.com/mickeykang16/Ev-Stereo3D)
  * [FreqPDE Rethinking Positional Depth Embedding for Multi-View 3D Object Detection Transformers](https://openaccess.thecvf.com/content/ICCV2025/papers/Su_FreqPDE_Rethinking_Positional_Depth_Embedding_for_Multi-View_3D_Object_Detection_ICCV_2025_paper.pdf)
  * [Harnessing Uncertainty-aware Bounding Boxes for Unsupervised 3D Object Detection](http://arxiv.org/abs/2408.00619)
  * [OpenM3D Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations](http://arxiv.org/abs/2508.20063)
  * [RCTDistill Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion](http://arxiv.org/abs/2509.17712)
  * [Towards Accurate and Efficient 3D Object Detection for Autonomous Driving A Mixture of Experts Computing System on Edge](http://arxiv.org/abs/2507.04123)
  * [MonoSOWA Scalable Monocular 3D Object Detector Without Human Annotations](http://arxiv.org/abs/2501.09481)
  * [Doppler-Aware LiDAR-RADAR Fusion for Weather-Robust 3D Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Chae_Doppler-Aware_LiDAR-RADAR_Fusion_for_Weather-Robust_3D_Detection_ICCV_2025_paper.pdf)
  * [CHARM3R Towards Unseen Camera Height Robust Monocular 3D Detector](http://arxiv.org/abs/2508.11185)
* 伪装目标检测
  * [Rethinking Detecting Salient and Camouflaged Objects in Unconstrained Scenes](http://arxiv.org/abs/2412.10943)<br>:star:[code](https://github.com/ssecv/USCNet)
  * [ESCNetEdge-Semantic Collaborative Network for Camouflaged Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Ye_ESCNetEdge-Semantic_Collaborative_Network_for_Camouflaged_Object_Detection_ICCV_2025_paper.pdf)
  * [Enhancing Prompt Generation with Adaptive Refinement for Camouflaged Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Enhancing_Prompt_Generation_with_Adaptive_Refinement_for_Camouflaged_Object_Detection_ICCV_2025_paper.pdf)
  * [Beyond Single Images Retrieval Self-Augmented Unsupervised Camouflaged Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Du_Beyond_Single_Images_Retrieval_Self-Augmented_Unsupervised_Camouflaged_Object_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/xiaohainku/RISE)
  * [Improving SAM for Camouflaged Object Detection via Dual Stream Adapters](http://arxiv.org/abs/2503.06042)
  * [Scoring Remember and Reference Catching Camouflaged Objects in Videos](http://arxiv.org/abs/2503.17050)
* 半监督目标检测
  * [STEP-DETR Advancing DETR-based Semi-Supervised Object Detection with Super Teacher and Pseudo-Label Guided Text Queries](https://openaccess.thecvf.com/content/ICCV2025/papers/Shehzadi_STEP-DETR_Advancing_DETR-based_Semi-Supervised_Object_Detection_with_Super_Teacher_and_ICCV_2025_paper.pdf)
  * [Power of Cooperative Supervision Multiple Teachers Framework for Advanced 3D Semi-Supervised Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_Power_of_Cooperative_Supervision_Multiple_Teachers_Framework_for_Advanced_3D_ICCV_2025_paper.pdf)
* 小样本目标检测
  * [When Pixel Difference Patterns Meet ViT PiDiViT for Few-Shot Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_When_Pixel_Difference_Patterns_Meet_ViT_PiDiViT_for_Few-Shot_Object_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Seaz9/PiDiViT)
* 域适应目标检测
  * [Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability](http://arxiv.org/pdf/2506.21042v1)<br>:star:[code](https://github.com/heboyong/Fitness-Generalization-Transferability)
  * [Dual-Rate Dynamic Teacher for Source-Free Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/He_Dual-Rate_Dynamic_Teacher_for_Source-Free_Domain_Adaptive_Object_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/qih96/DDT)
  * [Debiased Teacher for Day-to-Night Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Cui_Debiased_Teacher_for_Day-to-Night_Domain_Adaptive_Object_Detection_ICCV_2025_paper.pdf)
* 开放词汇目标检测
  * [Dynamic-DINO Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_Dynamic-DINO_Fine-Grained_Mixture_of_Experts_Tuning_for_Real-time_Open-Vocabulary_Object_ICCV_2025_paper.pdf)
  * [Benefit From Seen Enhancing Open-Vocabulary Object Detection by Bridging Visual and Textual Co-Occurrence Knowledge](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Benefit_From_Seen_Enhancing_Open-Vocabulary_Object_Detection_by_Bridging_Visual_ICCV_2025_paper.pdf)
  * [Superpowering Open-Vocabulary Object Detectors for X-ray Vision](https://openaccess.thecvf.com/content/ICCV2025/papers/Garcia-Fernandez_Superpowering_Open-Vocabulary_Object_Detectors_for_X-ray_Vision_ICCV_2025_paper.pdf)<br>:house:[project](https://pagf188.github.io/RAXO)
* 可见光红外目标检测
  * [WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection](https://arxiv.org/pdf/2507.18173v1)
* 红外小目标检测
  * [From Easy to Hard Progressive Active Learning Framework for Infrared Small Target Detection with Single Point Supervision](http://arxiv.org/abs/2412.11154)<br>:star:[code](https://github.com/YuChuang1205/PAL)
  * [Text-IRSTD Leveraging Semantic Text to Promote Infrared Small Target Detection in Complex Scenes](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_Text-IRSTD_Leveraging_Semantic_Text_to_Promote_Infrared_Small_Target_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Zhengsy0407/Text-IRSTD)
  * [DISTA-Net Dynamic Closely-Spaced Infrared Small Target Unmixing](https://openaccess.thecvf.com/content/ICCV2025/papers/Han_DISTA-Net_Dynamic_Closely-Spaced_Infrared_Small_Target_Unmixing_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/GrokCV/GrokCSO)




<a name="12"/>

## 12.Avatar
* [TeRA Rethinking Text-guided Realistic 3D Avatar Generation](http://arxiv.org/abs/2509.02466)
* [HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars](https://arxiv.org/pdf/2507.19481v1)<br>:star:[code](https://bjkim95.github.io/haircup/)
* [MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar Reconstruction](https://arxiv.org/pdf/2507.23597v1)<br>:star:[code](https://zj-dong.github.io/MoGA/)<br>:star:[code](https://github.com/zj-dong/moga)
* [PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image](https://arxiv.org/pdf/2508.09973v1)<br>:star:[code](https://mks0601.github.io/PERSONA/)<br>:star:[code](https://github.com/mks0601/persona_release)
* [Towards Explicit Exoskeleton for the Reconstruction of Complicated 3D Human Avatars](http://arxiv.org/abs/2410.08082)
* [Im2Haircut Single-view Strand-based Hair Reconstruction for Human Avatars](http://arxiv.org/abs/2509.01469)
* [HADES Human Avatar with Dynamic Explicit Hair Strands](https://openaccess.thecvf.com/content/ICCV2025/papers/Liao_HADES_Human_Avatar_with_Dynamic_Explicit_Hair_Strands_ICCV_2025_paper.pdf)
* [GaussianSpeech Audio-Driven Personalized 3D Gaussian Avatars](https://openaccess.thecvf.com/content/ICCV2025/papers/Aneja_GaussianSpeech_Audio-Driven_Personalized_3D_Gaussian_Avatars_ICCV_2025_paper.pdf)
* [GUAVA Generalizable Upper Body 3D Gaussian Avatar](http://arxiv.org/abs/2505.03351)
* [Disentangled Clothed Avatar Generation with Layered Representation](http://arxiv.org/abs/2501.04631)<br>:house:[project](https://olivia23333.github.io/LayerAvatar)
* [GAS Generative Avatar Synthesis from a Single Image](http://arxiv.org/abs/2502.06957)
* 虚拟头像
  * [Capturing head avatar with hand contacts from a monocular video](https://openaccess.thecvf.com/content/ICCV2025/papers/He_Capturing_head_avatar_with_hand_contacts_from_a_monocular_video_ICCV_2025_paper.pdf)
  * [OneGT One-Shot Geometry-Texture Neural Rendering for Head Avatars](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_OneGT_One-Shot_Geometry-Texture_Neural_Rendering_for_Head_Avatars_ICCV_2025_paper.pdf)
  * [Avat3r Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars](http://arxiv.org/abs/2502.20220)
  * [StrandHead Text to Hair-Disentangled 3D Head Avatars Using Human-Centric Priors](http://arxiv.org/abs/2412.11586)<br>:house:[project](https://xiaokunsun.github.io/StrandHead.github.io/) :house:[project](https://xiaokunsun.github.io/StrandHead.github.io)
  * [Fine-Grained 3D Gaussian Head Avatars Modeling from Static Captures via Joint Reconstruction and Registration](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Fine-Grained_3D_Gaussian_Head_Avatars_Modeling_from_Static_Captures_via_ICCV_2025_paper.pdf)
  * [GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar](https://arxiv.org/pdf/2507.18155v1)<br>:star:[code](https://hahminlew.github.io/geoavatar/)
  * [Identity Preserving 3D Head Stylization with Multiview Score Distillation](https://openaccess.thecvf.com/content/ICCV2025/papers/Bilecen_Identity_Preserving_3D_Head_Stylization_with_Multiview_Score_Distillation_ICCV_2025_paper.pdf)


<a name="11"/>

## 11.Face
* [IDFace: Face Template Protection for Efficient and Secure Identification](https://arxiv.org/pdf/2507.12050v1)
* [FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models](https://arxiv.org/pdf/2507.02714v1)
* [F-Bench Rethinking Human Preference Evaluation Metrics for Benchmarking Face Generation Customization and Restoration](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation_ICCV_2025_paper.pdf)
* [DH-FaceVid-1K A Large-Scale High-Quality Dataset for Face Video Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Di_DH-FaceVid-1K_A_Large-Scale_High-Quality_Dataset_for_Face_Video_Generation_ICCV_2025_paper.pdf)<br>:house:[project](https://luna-ai-lab.github.io/DH-FaceVid-1K/) :house:[project](https://luna-ai-lab.github.io/DH-FaceVid-1K)
* [DynamicID Zero-Shot Multi-ID Image Personalization with Flexible Facial Editability](http://arxiv.org/abs/2503.06505)
* [Monocular Facial Appearance Capture in the Wild](http://arxiv.org/abs/2412.12765)
* [FPEM Face Prior Enhanced Facial Attractiveness Prediction for Live Videos with Face Retouching](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Estella-LH/FPEM)
* [FaceXFormer A Unified Transformer for Facial Analysis](http://arxiv.org/abs/2403.12960)
* [FaceShield Defending Facial Image against Deepfake Threats](http://arxiv.org/abs/2412.09921)
* [TimeBooth Disentangled Facial Invariant Representation for Diverse and Personalized Face Aging](https://openaccess.thecvf.com/content/ICCV2025/papers/Su_TimeBooth_Disentangled_Facial_Invariant_Representation_for_Diverse_and_Personalized_Face_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/szp-aigc/TimeBooth)
* [InteractAvatar Modeling Hand-Face Interaction in Photorealistic Avatars with Deformable Gaussians](http://arxiv.org/abs/2504.07949)
* [MR-FIQA Face Image Quality Assessment with Multi-Reference Representations from Synthetic Data Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Ou_MR-FIQA_Face_Image_Quality_Assessment_with_Multi-Reference_Representations_from_Synthetic_ICCV_2025_paper.pdf)
* [Face Retouching with Diffusion Data Generation and Spectral Restorement](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_Face_Retouching_with_Diffusion_Data_Generation_and_Spectral_Restorement_ICCV_2025_paper.pdf)
* [FaceLift Learning Generalizable Single Image 3D Face Reconstruction from Synthetic Heads](http://arxiv.org/abs/2412.17812)
* 人脸识别
  * [LVFace Progressive Cluster Optimization for Large Vision Models in Face Recognition](http://arxiv.org/abs/2501.13420)<br>:star:[code](https://github.com/bytedance/LVFace)
  * [Bi-Level Optimization for Self-Supervised AI-Generated Face Detection](http://arxiv.org/abs/2507.22824)
  * [Stylized-Face A Million-level Stylized Face Dataset for Face Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Peng_Stylized-Face_A_Million-level_Stylized_Face_Dataset_for_Face_Recognition_ICCV_2025_paper.pdf)
  * [VIGFace Virtual Identity Generation for Privacy-Free Face Recognition Dataset](https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_VIGFace_Virtual_Identity_Generation_for_Privacy-Free_Face_Recognition_Dataset_ICCV_2025_paper.pdf)
* 人脸恢复
  * [DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance](https://arxiv.org/pdf/2507.13797v1)
  * [MoFRR Mixture of Diffusion Models for Face Retouching Restoration](http://arxiv.org/abs/2507.19770)
  * [Unlocking the Potential of Diffusion Priors in Blind Face Restoration](http://arxiv.org/abs/2508.08556)
  * [Dirichlet-Constrained Variational Codebook Learning for Temporally Coherent Video Face Restoration](http://arxiv.org/abs/2506.13355)<br>:star:[code](https://github.com/fudan-generative-vision/DicFace)
* 人脸表情识别
  * [Multimodal Prompt Alignment for Facial Expression Recognition](http://arxiv.org/pdf/2506.21017v1)
  * [AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation](https://arxiv.org/pdf/2507.12001v1)<br>:star:[code](https://github.com/wslh852/AUBlendNet.git)
  * [SynFER Towards Boosting Facial Expression Recognition with Synthetic Data](http://arxiv.org/abs/2410.09865)
  * [SEREP Semantic Facial Expression Representation for Robust In-the-Wild Capture and Retargeting](http://arxiv.org/abs/2412.14371)
  * [ContextFace Generating Facial Expressions from Emotional Contexts](https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_ContextFace_Generating_Facial_Expressions_from_Emotional_Contexts_ICCV_2025_paper.pdf)
* 说话头
  * [GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation](http://arxiv.org/pdf/2506.21513v1)<br>:star:[code](https://vincenthu19.github.io/GGTalker/)
  * [DGTalker Disentangled Generative Latent Space Learning for Audio-Driven Gaussian Talking Heads](https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_DGTalker_Disentangled_Generative_Latent_Space_Learning_for_Audio-Driven_Gaussian_Talking_ICCV_2025_paper.pdf)
  * [FLOAT Generative Motion Latent Flow Matching for Audio-driven Talking Portrait](http://arxiv.org/abs/2412.01064)
  * [Who is a Better Talker: Subjective and Objective Quality Assessment for AI-Generated Talking Heads](http://arxiv.org/abs/2507.23343)<br>:star:[code](https://github.com/zyj-2000/Talker)
  * [ARIG: Autoregressive Interactive Head Generation for Real-time Conversations](https://arxiv.org/pdf/2507.00472v1)<br>:star:[code](https://jinyugy21.github.io/ARIG/)
  * [FixTalk Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases](http://arxiv.org/abs/2507.01390)
  * [Audio-visual Controlled Video Diffusion with Masked Selective State Spaces Modeling for Natural Talking Head Generation](http://arxiv.org/abs/2504.02542)
* 人脸交换
  * [CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation](https://arxiv.org/abs/2507.02691)<br>:house:[project](https://luoxyhappy.github.io/CanonSwap/)
  * [Controllable and Expressive One-Shot Video Head Swapping](http://arxiv.org/abs/2506.16852)
  * [NullSwap Proactive Identity Cloaking Against Deepfake Face Swapping](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping_ICCV_2025_paper.pdf)
  * [DynamicFace High-Quality and Consistent Face Swapping for Image and Video using Composable 3D Facial Priors](http://arxiv.org/abs/2501.08553)<br>:house:[project](https://dynamic-face.github.io/)
* 活体检测
  * [Group-wise Scaling and Orthogonal Decomposition for Domain-Invariant Feature Extraction in Face Anti-Spoofing](https://arxiv.org/pdf/2507.04006v1)<br>:star:[code](https://github.com/SeungjinJung/GD-FAS)
  * [Multi-View Slot Attention Using Paraphrased Texts for Face Anti-Spoofing](http://arxiv.org/abs/2509.06336)
  * [DADM Dual Alignment of Domain and Modality for Face Anti-spoofing](http://arxiv.org/abs/2503.00429)<br>:star:[code](https://github.com/yjyddq/DADM)
* 三维人脸动画
  * [MemoryTalker: Personalized Speech-Driven 3D Facial Animation via Audio-Guided Stylization](https://arxiv.org/pdf/2507.20562v1)<br>:star:[code](https://cau-irislab.github.io/ICCV25-MemoryTalker/)
  * [FaceCraft4D Animated 3D Facial Avatar Generation from a Single Image](http://arxiv.org/abs/2504.15179)
* 微表情识别
  * [FED-PsyAU: Privacy-Preserving Micro-Expression Recognition via Psychological AU Coordination and Dynamic Facial Motion Modeling](https://arxiv.org/pdf/2507.20557v1)<br>:star:[code](https://github.com/MELABIPCAS/FED-PsyAU.git)
  * [Rethinking Key-frame-based Micro-expression Recognition A Robust and Accurate Framework Against Key-frame Errors](http://arxiv.org/abs/2508.06640)<br>:star:[code](https://github.com/tony19980810/CausalNet)
* 人脸关键点检测
  * [PossLoss A Reliable and Sensitive Facial Landmark Detection Loss Function](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_PossLoss_A_Reliable_and_Sensitive_Facial_Landmark_Detection_Loss_Function_ICCV_2025_paper.pdf)
  * [Heatmap Regression without Soft-Argmax for Facial Landmark Detection](http://arxiv.org/abs/2508.14929)
* 头部重建
  * [SVG-Head Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_SVG-Head_Hybrid_Surface-Volumetric_Gaussians_for_High-Fidelity_Head_Reconstruction_and_Real-Time_ICCV_2025_paper.pdf)
  * [WarpHE4D Dense 4D Head Map toward Full Head Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Yun_WarpHE4D_Dense_4D_Head_Map_toward_Full_Head_Reconstruction_ICCV_2025_paper.pdf)

<a name="10"/>

## 10.Medical Image Progress(医学图像处理)
* [Medical World Model](http://arxiv.org/abs/2506.02327)
* [MedSegFactory Text-Guided Generation of Medical Image-Mask Pairs](http://arxiv.org/abs/2504.06897)
* [Towards a Universal 3D Medical Multi-modality Generalization via Learning Personalized Invariant Representation](http://arxiv.org/abs/2411.06106)
* [Is Visual in-Context Learning for Compositional Medical Tasks within Reach?](https://arxiv.org/pdf/2507.00868v1)
* [FB-Diff: Fourier Basis-guided Diffusion for Temporal Interpolation of 4D Medical Imaging](https://arxiv.org/pdf/2507.04547v1)
* [Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines](https://arxiv.org/pdf/2507.10737v1)
* [Tiling artifacts and trade-offs of feature normalization in the segmentation of large biological images](http://arxiv.org/abs/2503.19545)<br>:star:[code](https://github.com/kreshuklab/no_tiling_artifacts)
* [ProbMED A Probabilistic Framework for Medical Multimodal Binding](http://arxiv.org/abs/2509.25711)
* [Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation](https://arxiv.org/pdf/2507.11055v1)
* [M-Net: MRI Brain Tumor Sequential Segmentation Network via Mesh-Cast](https://arxiv.org/pdf/2507.20582v1)
* [Beyond Brain Decoding Visual-Semantic Reconstructions to Mental Creation Extension Based on fMRI](https://openaccess.thecvf.com/content/ICCV2025/papers/Jing_Beyond_Brain_Decoding_Visual-Semantic_Reconstructions_to_Mental_Creation_Extension_Based_ICCV_2025_paper.pdf)
* [MRGen Segmentation Data Engine For Underrepresented MRI Modalities](http://arxiv.org/abs/2412.04106)<br>:house:[project](https://haoningwu3639.github.io/MRGen)
* [Learn2Synth Learning Optimal Data Synthesis Using Hypergradients for Brain Image Segmentation](http://arxiv.org/abs/2411.16719)<br>:star:[code](https://github.com/HuXiaoling/Learn2Synth)
* [MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy](https://arxiv.org/pdf/2508.03596v1)<br>:star:[code](https://cuhk-aim-group.github.io/MetaScope/)<br>:star:[code](https://github.com/cuhk-aim-group/metascope)
* [COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets](https://arxiv.org/pdf/2508.09886v1)
* [CoStoDet-DDPM Collaborative Training of Stochastic and Deterministic Models Improves Surgical Workflow Anticipation and Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_CoStoDet-DDPM_Collaborative_Training_of_Stochastic_and_Deterministic_Models_Improves_Surgical_ICCV_2025_paper.pdf)
* [Optimal Transport for Brain-Image Alignment Unveiling Redundancy and Synergy in Neural Information Processing](http://arxiv.org/abs/2503.10663)<br>:star:[code](https://github.com/NKUShaw/OT-Alignment4brain-to-image)
* [SAMora Enhancing SAM through Hierarchical Self-Supervised Pre-Training for Medical Images](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_SAMora_Enhancing_SAM_through_Hierarchical_Self-Supervised_Pre-Training_for_Medical_Images_ICCV_2025_paper.pdf)
* [CoSMIC Continual Self-supervised Learning for Multi-Domain Medical Imaging via Conditional Mutual Information Maximization](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_CoSMIC_Continual_Self-supervised_Learning_for_Multi-Domain_Medical_Imaging_via_Conditional_ICCV_2025_paper.pdf)
* [Test-time Adaptation for Foundation Medical Segmentation Model Without Parametric Updates](http://arxiv.org/abs/2504.02008)
* [AcZeroTS Active Learning for Zero-shot Tissue Segmentation in Pathology Images](https://openaccess.thecvf.com/content/ICCV2025/papers/Tang_AcZeroTS_Active_Learning_for_Zero-shot_Tissue_Segmentation_in_Pathology_Images_ICCV_2025_paper.pdf)
* [TokenUnify Scaling Up Autoregressive Pretraining for Neuron Segmentation](http://arxiv.org/abs/2405.16847)<br>:star:[code](https://github.com/ydchen0806/TokenUnify)
* [Keep Your Friends Close and Your Enemies Farther Distance-aware Voxel-wise Contrastive Learning for Semi-supervised Multi-organ Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_Keep_Your_Friends_Close_and_Your_Enemies_Farther_Distance-aware_Voxel-wise_ICCV_2025_paper.pdf)
* [Breaking Grid Constraints Dynamic Graph Reconstruction Network for Multi-organ Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Xiao_Breaking_Grid_Constraints_Dynamic_Graph_Reconstruction_Network_for_Multi-organ_Segmentation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/robert1818118/DGRNet)
* [Seeing the Trees for the Forest Rethinking Weakly-Supervised Medical Visual Grounding](http://arxiv.org/abs/2505.15123)
* [GEMeX A Large-Scale Groundable and Explainable Medical VQA Benchmark for Chest X-ray Diagnosis](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_GEMeX_A_Large-Scale_Groundable_and_Explainable_Medical_VQA_Benchmark_for_ICCV_2025_paper.pdf)
* [Debiased Curriculum Adaptation for Safe Transfer Learning in Chest X-ray Classification](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Debiased_Curriculum_Adaptation_for_Safe_Transfer_Learning_in_Chest_X-ray_ICCV_2025_paper.pdf)
* [Scaling Tumor Segmentation Best Lessons from Real and Synthetic Data](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Scaling_Tumor_Segmentation_Best_Lessons_from_Real_and_Synthetic_Data_ICCV_2025_paper.pdf)
* [PathFinder A Multi-Modal Multi-Agent System for Medical Diagnostic Decision-Making Applied to Histopathology](http://arxiv.org/abs/2502.08916)
* [GECKO Gigapixel Vision-Concept Contrastive Pretraining in Histopathology](http://arxiv.org/abs/2504.01009)<br>:star:[code](https://github.com/bmi-imaginelab/GECKO)
* [Boosting Vision Semantic Density with Anatomy Normality Modeling for Medical Vision-language Pre-training](http://arxiv.org/abs/2508.03742)
* [TPG-INR Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging](https://openaccess.thecvf.com/content/ICCV2025/papers/Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view_ICCV_2025_paper.pdf)
* 医学图像分割
  * [Adaptive Learning of High-Value Regions for Semi-Supervised Medical Image Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lei_Adaptive_Learning_of_High-Value_Regions_for_Semi-Supervised_Medical_Image_Segmentation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ziziyao/ALHVR)
  * [Teaching AI the Anatomy Behind the Scan Addressing Anatomical Flaws in Medical Image Segmentation with Learnable Prior](http://arxiv.org/abs/2403.18878)
  * [UKBOB One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation](http://arxiv.org/abs/2504.06908)
  * [MaskSAM Auto-prompt SAM with Mask Classification for Volumetric Medical Image Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_MaskSAM_Auto-prompt_SAM_with_Mask_Classification_for_Volumetric_Medical_Image_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/bxie9/MaskSAM)
  * [Progressive Test Time Energy Adaptation for Medical Image Segmentation](http://arxiv.org/abs/2503.16616)<br>:house:[project](https://voldemort108x.github.io/pttea_seg)
  * [Toward Fair and Accurate Cross-Domain Medical Image Segmentation A VLM-Driven Active Domain Adaptation Paradigm](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Toward_Fair_and_Accurate_Cross-Domain_Medical_Image_Segmentation_A_VLM-Driven_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/whq-xxh/Fair-AP)
  * [SPA Efficient User-Preference Alignment against Uncertainty in Medical Image Segmentation](http://arxiv.org/abs/2411.15513)<br>:star:[code](https://github.com/SuperMedIntel/SPA)
  * [Similarity Memory Prior is All You Need for Medical Image Segmentation](http://arxiv.org/abs/2507.00585)<br>:star:[code](https://github.com/vpsg-research/Sim-MPNet)
* 医学图像融合
  * [UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments](https://arxiv.org/pdf/2506.22736v1)<br>:star:[code](https://github.com/slrl123/UniFuse)
* 报告生成
  * [MedRegion-CT: Region-Focused Multimodal LLM for Comprehensive 3D CT Report Generation](https://arxiv.org/pdf/2506.23102v1)
  * [Learnable Retrieval Enhanced Visual-Text Alignment and Fusion for Radiology Report Generation](http://arxiv.org/abs/2507.07568)
* 切片分析
  * [Continual Multiple Instance Learning with Enhanced Localization for Histopathological Whole Slide Image Analysis](https://arxiv.org/pdf/2507.02395v1)
  * [Cracking Instance Jigsaw Puzzles: An Alternative to Multiple Instance Learning for Whole Slide Image Analysis](https://arxiv.org/pdf/2507.08178v1)<br>:star:[code](https://github.com/xiwenc1/MIL-JigsawPuzzles)
  * [Bridging Local Inductive Bias and Long-Range Dependencies with Pixel-Mamba for End-to-end Whole Slide Image Analysis](https://openaccess.thecvf.com/content/ICCV2025/papers/Qiu_Bridging_Local_Inductive_Bias_and_Long-Range_Dependencies_with_Pixel-Mamba_for_ICCV_2025_paper.pdf)
  * [WSI-LLaVA A Multimodal Large Language Model for Whole Slide Image](https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_WSI-LLaVA_A_Multimodal_Large_Language_Model_for_Whole_Slide_Image_ICCV_2025_paper.pdf)
* 切片分割
  * [Flow-MIL Constructing Highly-expressive Latent Feature Space For Whole Slide Image Classification Using Normalizing Flow](https://openaccess.thecvf.com/content/ICCV2025/papers/Ma_Flow-MIL_Constructing_Highly-expressive_Latent_Feature_Space_For_Whole_Slide_Image_ICCV_2025_paper.pdf)
  * [GMMamba Group Masking Mamba for Whole Slide Image Classification](https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification_ICCV_2025_paper.pdf)
* 3D医学
  * [Structure-aware Semantic Discrepancy and Consistency for 3D Medical Image Self-supervised Learning](https://arxiv.org/pdf/2507.02581v1)<br>:star:[code](https://github.com/Ashespt/S2DC)
  * [An OpenMind for 3D Medical Vision Self-supervised Learning](http://arxiv.org/abs/2412.17041)
* 息肉分割
  * [One Polyp Identifies All: One-Shot Polyp Segmentation with SAM via Cascaded Priors and Iterative Prompt Evolution](https://arxiv.org/pdf/2507.16337v1)
  * [STDDNet Harnessing Mamba for Video Polyp Segmentation via Spatial-aligned Temporal Modeling and Discriminative Dynamic Representation Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_STDDNet_Harnessing_Mamba_for_Video_Polyp_Segmentation_via_Spatial-aligned_Temporal_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/C-GLGLGL/STDDNet)
* 医学影像隐私保护
  * [Semantics versus Identity: A Divide-and-Conquer Approach towards Adjustable Medical Image De-Identification](https://arxiv.org/pdf/2507.21703v1)
* 细胞分割
  * [COIN Confidence Score-Guided Distillation for Annotation-Free Cell Segmentation](http://arxiv.org/abs/2503.11439)<br>:house:[project](https://shjo-april.github.io/COIN)
* 关键点检测
  * [CABLD Contrast-Agnostic Brain Landmark Detection with Consistency-Based Regularization](http://arxiv.org/abs/2411.17845)<br>:star:[code](https://github.com/HealthX-Lab/CABLD)

<a name="9"/>

## 9.Image/Video Compression(图像/视频压缩)
* [Compression-Aware One-Step Diffusion Model for JPEG Artifact Removal](http://arxiv.org/abs/2502.09873)<br>:star:[code](https://github.com/jp-guo/CODiff)
* [Compression of 3D Gaussian Splatting with Optimized Feature Planes and Standard Video Codecs](http://arxiv.org/abs/2501.03399)
* 图像压缩
  * [Learned Image Compression with Hierarchical Progressive Context Modeling](https://arxiv.org/pdf/2507.19125v1)<br>:star:[code](https://github.com/lyq133/LIC-HPCM)
  * [Cassic Towards Content-Adaptive State-Space Models for Learned Image Compression](https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Cassic_Towards_Content-Adaptive_State-Space_Models_for_Learned_Image_Compression_ICCV_2025_paper.pdf)
  * [An Information-Theoretic Regularizer for Lossy Neural Image Compression](http://arxiv.org/abs/2411.16727)
  * [Cross-Granularity Online Optimization with Masked Compensated Information for Learned Image Compression](https://openaccess.thecvf.com/content/ICCV2025/papers/Kuang_Cross-Granularity_Online_Optimization_with_Masked_Compensated_Information_for_Learned_Image_ICCV_2025_paper.pdf)<br>:house:[project](https://ellisonkuang.github.io/CGOO.github.io)
  * [Knowledge Distillation for Learned Image Compression](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Knowledge_Distillation_for_Learned_Image_Compression_ICCV_2025_paper.pdf)
  * [DLF Extreme Image Compression with Dual-generative Latent Fusion](http://arxiv.org/abs/2503.01428)<br>:house:[project](https://dlfcodec.github.io/)
  * [StableCodec Taming One-Step Diffusion for Extreme Image Compression](http://arxiv.org/abs/2506.21977)
* VC
  * [Context Guided Transformer Entropy Modeling for Video Compression](http://arxiv.org/abs/2508.01852)
  * [Beyond Perspective Neural 360-Degree Video Compression](https://openaccess.thecvf.com/content/ICCV2025/papers/Regensky_Beyond_Perspective_Neural_360-Degree_Video_Compression_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/FAU-LMS/NVC360)
  * [CRAM Large Scale Video Continual Learning with Bootstrapped Compression](http://arxiv.org/abs/2508.05001)
  * [GIViC Generative Implicit Video Compression](http://arxiv.org/abs/2503.19604)
* 视频编解码
  * [HyTIP Hybrid Temporal Information Propagation for Masked Conditional Residual Video Coding](http://arxiv.org/abs/2508.02072)<br>:star:[code](https://github.com/NYCU-MAPL/HyTIP)
  * [MH-LVC Multi-Hypothesis Temporal Prediction for Learned Conditional Residual Video Coding](https://openaccess.thecvf.com/content/ICCV2025/papers/Phung_MH-LVC_Multi-Hypothesis_Temporal_Prediction_for_Learned_Conditional_Residual_Video_Coding_ICCV_2025_paper.pdf)
  * [ResidualViT for Efficient Temporally Dense Video Encoding](http://arxiv.org/abs/2509.13255)
  * [EEGMirror Leveraging EEG Data in the Wild via Montage-Agnostic Self-Supervision for EEG to Video Decoding](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_EEGMirror_Leveraging_EEG_Data_in_the_Wild_via_Montage-Agnostic_Self-Supervision_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/XuanhaoLiu/EEGMirror)
* 视频自动编码
  * [VideoVAE Large Motion Video Autoencoding with Cross-modal Video VAE](https://openaccess.thecvf.com/content/ICCV2025/papers/Xing_VideoVAE_Large_Motion_Video_Autoencoding_with_Cross-modal_Video_VAE_ICCV_2025_paper.pdf)视频自动编码


<a name="8"/>

## 8.Image/Video Retrieval(图像/视频检索)
* [Adversarial Reconstruction Feedback for Robust Fine-grained Generalization](https://arxiv.org/pdf/2507.21742v1)
* [MobileViCLIP: An Efficient Video-Text Model for Mobile Devices](https://arxiv.org/pdf/2508.07312v1)<br>:star:[code](https://github.com/MCG-NJU/MobileViCLIP)
* [Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object Retrieval](https://arxiv.org/pdf/2507.21489v1)<br>:star:[code](https://github.com/wangzhichuan123/DAC)
* [Taming the Untamed Graph-Based Knowledge Retrieval and Reasoning for MLLMs to Conquer the Unknown](http://arxiv.org/abs/2506.17589)
* 图像检索
  * [Learning Visual Hierarchies in Hyperbolic Space for Image Retrieval](http://arxiv.org/abs/2411.17490)
* 视频检索
  * [HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning](https://arxiv.org/pdf/2507.17402v1)<br>:star:[code](https://github.com/lijun2005/ICCV25-HLFormer)
  * [Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning](http://arxiv.org/abs/2507.17402)<br>:star:[code](https://github.com/lijun2005/ICCV25-HLFormer)
  * [Beyond Simple Edits Composed Video Retrieval with Dense Modifications](http://arxiv.org/abs/2508.14039)
  * [Prototypes are Balanced Units for Efficient and Effective Partially Relevant Video Retrieval](http://arxiv.org/abs/2504.13035)
* 文本-视频检索
  * [Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization](https://arxiv.org/pdf/2507.15504v1)
  * [Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval](https://arxiv.org/pdf/2507.23284v1)<br>:star:[code](https://github.com/mlvlab/BLiM)
  * [Hybrid-Tower Fine-grained Pseudo-query Interaction and Generation for Text-to-Video Retrieval](https://openaccess.thecvf.com/content/ICCV2025/papers/Lan_Hybrid-Tower_Fine-grained_Pseudo-query_Interaction_and_Generation_for_Text-to-Video_Retrieval_ICCV_2025_paper.pdf)
* 组合图像检索
  * [Zero-Shot Composed Image Retrieval via Dual-Stream Instruction-Aware Distillation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhong_Zero-Shot_Composed_Image_Retrieval_via_Dual-Stream_Instruction-Aware_Distillation_ICCV_2025_paper.pdf)
  * [Hierarchy-Aware Pseudo Word Learning with Text Adaptation for Zero-Shot Composed Image Retrieval](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Hierarchy-Aware_Pseudo_Word_Learning_with_Text_Adaptation_for_Zero-Shot_Composed_ICCV_2025_paper.pdf)
  * [An Efficient Post-hoc Framework for Reducing Task Discrepancy of Text Encoders for Composed Image Retrieval](http://arxiv.org/abs/2406.09188)<br>:star:[code](https://github.com/jaeseokbyun/RTD)
  * [Multi-Schema Proximity Network for Composed Image Retrieval](https://openaccess.thecvf.com/content/ICCV2025/papers/Shi_Multi-Schema_Proximity_Network_for_Composed_Image_Retrieval_ICCV_2025_paper.pdf)
  * [CoTMR Chain-of-Thought Multi-Scale Reasoning for Training-Free Zero-Shot Composed Image Retrieval](http://arxiv.org/abs/2502.20826)
  * [MA-CIR A Multimodal Arithmetic Benchmark for Composed Image Retrieval](https://openaccess.thecvf.com/content/ICCV2025/papers/Byun_MA-CIR_A_Multimodal_Arithmetic_Benchmark_for_Composed_Image_Retrieval_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/jaeseokbyun/MACIR)

<a name="7"/>

## 7.Image Classification(图像分类)
* [Generate, Refine, and Encode: Leveraging Synthesized Novel Samples for On-the-Fly Fine-Grained Category Discovery](https://arxiv.org/pdf/2507.04051v1)
* [Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification](https://arxiv.org/pdf/2506.23285v1)
* [CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous Nuisance Shifts](https://arxiv.org/pdf/2507.17651v1)<br>:star:[code](https://genintel.github.io/CNS)
* [I Am Big, You Are Little; I Am Right, You Are Wrong](https://arxiv.org/pdf/2507.23509v1)
* [Is Meta-Learning Out Rethinking Unsupervised Few-Shot Classification with Limited Entropy](http://arxiv.org/abs/2509.13185)
* [Synergistic Prompting for Robust Visual Recognition with Missing Modalities](http://arxiv.org/abs/2507.07802)
* [MolParser End-to-end Visual Recognition of Molecule Structures in the Wild](http://arxiv.org/abs/2411.11098)
* [Think Twice Test-Time Reasoning for Robust CLIP Zero-Shot Classification](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_Think_Twice_Test-Time_Reasoning_for_Robust_CLIP_Zero-Shot_Classification_ICCV_2025_paper.pdf)
* [Supervised Exploratory Learning for Long-Tailed Visual Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Jian_Supervised_Exploratory_Learning_for_Long-Tailed_Visual_Recognition_ICCV_2025_paper.pdf)
* [Hierarchical Divide-and-Conquer Grouping for Classification Adaptation of Pre-Trained Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_Hierarchical_Divide-and-Conquer_Grouping_for_Classification_Adaptation_of_Pre-Trained_Models_ICCV_2025_paper.pdf)
* [Long-Tailed Classification with Multi-Granularity Semantics](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Long-Tailed_Classification_with_Multi-Granularity_Semantics_ICCV_2025_paper.pdf)
* [On Large Multimodal Models as Open-World Image Classifiers](http://arxiv.org/abs/2503.21851)
* [NAPPure Adversarial Purification for Robust Image Classification under Non-Additive Perturbations](https://openaccess.thecvf.com/content/ICCV2025/papers/Nan_NAPPure_Adversarial_Purification_for_Robust_Image_Classification_under_Non-Additive_Perturbations_ICCV_2025_paper.pdf)
* [MPBR Multimodal Progressive Bidirectional Reasoning for Open-Set Fine-Grained Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Tan_MPBR_Multimodal_Progressive_Bidirectional_Reasoning_for_Open-Set_Fine-Grained_Recognition_ICCV_2025_paper.pdf)
* 细粒度分类
  * [Vocabulary-free Fine-grained Visual Recognition via Enriched Contextually Grounded Vision-Language Model](https://arxiv.org/pdf/2507.23070v1)<br>:star:[code](https://github.com/demidovd98/e-finer)
  * [LLM-assisted Entropy-based Adaptive Distillation for Unsupervised Fine-grained Visual Representation Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Dong_LLM-assisted_Entropy-based_Adaptive_Distillation_for_Unsupervised_Fine-grained_Visual_Representation_Learning_ICCV_2025_paper.pdf)
  * [Learning Separable Fine-Grained Representation via Dendrogram Construction from Coarse Labels for Fine-grained Visual Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Shi_Learning_Separable_Fine-Grained_Representation_via_Dendrogram_Construction_from_Coarse_Labels_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/BeCarefulOfYournaoke/BuCSFR)
* 图像分类
  * [Looking in the Mirror A Faithful Counterfactual Explanation Method for Interpreting Deep Image Classification Models](http://arxiv.org/abs/2509.16822)
  * [SIC Similarity-Based Interpretable Image Classification with Neural Networks](http://arxiv.org/abs/2501.17328)
  * [Learning Interpretable Queries for Explainable Image Classification with Information Pursuit](http://arxiv.org/abs/2312.11548)
  * [Category-Specific Selective Feature Enhancement for Long-Tailed Multi-Label Image Classification](https://openaccess.thecvf.com/content/ICCV2025/papers/Du_Category-Specific_Selective_Feature_Enhancement_for_Long-Tailed_Multi-Label_Image_Classification_ICCV_2025_paper.pdf)
  * [MambaML Exploring State Space Models for Multi-Label Image Classification](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_MambaML_Exploring_State_Space_Models_for_Multi-Label_Image_Classification_ICCV_2025_paper.pdf)
* 广义类别发现
  * [Dissecting Generalized Category Discovery Multiplex Consensus under Self-Deconstruction](http://arxiv.org/abs/2508.10731)<br>:star:[code](https://github.com/lytang63/ConGCD)
  * [A Hidden Stumbling Block in Generalized Category Discovery Distracted Attention](http://arxiv.org/abs/2507.14315)<br>:star:[code](https://github.com/Afleve/AFGCD)
  * [AllGCD Leveraging All Unlabeled Data for Generalized Category Discovery](https://openaccess.thecvf.com/content/ICCV2025/papers/Cao_AllGCD_Leveraging_All_Unlabeled_Data_for_Generalized_Category_Discovery_ICCV_2025_paper.pdf)


<a name="6"/>

## 6.Image Segmentation(图像分割)
* [SAM4D: Segment Anything in Camera and LiDAR Streams](http://arxiv.org/pdf/2506.21547v1)<br>:star:[code](https://SAM4D-Project.github.io)
* [Flow Stochastic Segmentation Networks](https://arxiv.org/pdf/2507.18838v1)<br>:star:[code](https://github.com/biomedia-mira/flow-ssn)
* [Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive](https://arxiv.org/pdf/2507.09612v1)
* [SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures](https://arxiv.org/pdf/2508.06127v1)
* [Correspondence as Video Test-Time Adaption on SAM2 for Reference Segmentation in the Wild](http://arxiv.org/abs/2508.07759)<br>:star:[code](https://github.com/wanghr64/cav-sam)
* [ProSAM Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts](http://arxiv.org/abs/2506.21835)
* [RA-BUSSeg Relation-aware Semi-supervised Breast Ultrasound Image Segmentation via Adjacent Propagation and Cross-layer Alignment](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_RA-BUSSeg_Relation-aware_Semi-supervised_Breast_Ultrasound_Image_Segmentation_via_Adjacent_Propagation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/dodooo1/RA-BUSSeg)
* [DictAS A Framework for Class-Generalizable Few-Shot Anomaly Segmentation via Dictionary Lookup](http://arxiv.org/abs/2508.13560)
* [Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation](http://arxiv.org/abs/2506.23120)
* [Intermediate Connectors and Geometric Priors for Language-Guided Affordance Segmentation on Unseen Object Categories](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Intermediate_Connectors_and_Geometric_Priors_for_Language-Guided_Affordance_Segmentation_on_ICCV_2025_paper.pdf)
* [Unified Open-World Segmentation with Multi-Modal Prompts](http://arxiv.org/abs/2510.10524)
* [LawDIS Language-Window-based Controllable Dichotomous Image Segmentation](http://arxiv.org/abs/2508.01152)<br>:star:[code](https://github.com/XinyuYanTJU/LawDIS)
* [HiMTok Learning Hierarchical Mask Tokens for Image Segmentation with Large Multimodal Model](http://arxiv.org/abs/2503.13026)
* [InstructSeg Unifying Instructed Visual Segmentation with Multi-modal Large Language Models](http://arxiv.org/abs/2412.14006)
* [ViLLa Video Reasoning Segmentation with Large Language Model](http://arxiv.org/abs/2407.14500)
* [LIRA Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance](http://arxiv.org/abs/2507.06272)<br>:star:[code](https://github.com/echo840/LIRA)
* [Text-guided Visual Prompt DINO for Generic Segmentation](http://arxiv.org/abs/2508.06146)<br>:star:[code](https://github.com/WeChatCV/WeVisionOne)
* [HyPiDecoder Hybrid Pixel Decoder for Efficient Segmentation and Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_HyPiDecoder_Hybrid_Pixel_Decoder_for_Efficient_Segmentation_and_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/HyPiDecoder/HybridPixelDecoder)
* [Refer to Any Segmentation Mask Group With Vision-Language Prompts](https://openaccess.thecvf.com/content/ICCV2025/papers/Cao_Refer_to_Any_Segmentation_Mask_Group_With_Vision-Language_Prompts_ICCV_2025_paper.pdf)<br>:house:[project](https://Ref2Any.github.io)
* [Adapt Foundational Segmentation Models with Heterogeneous Searching Space](https://openaccess.thecvf.com/content/ICCV2025/papers/Yi_Adapt_Foundational_Segmentation_Models_with_Heterogeneous_Searching_Space_ICCV_2025_paper.pdf)
* [SegAnyPET Universal Promptable Segmentation from Positron Emission Tomography Images](http://arxiv.org/abs/2502.14351)
* [Multi-scenario Overlapping Text Segmentation with Depth Awareness](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Multi-scenario_Overlapping_Text_Segmentation_with_Depth_Awareness_ICCV_2025_paper.pdf)
* [Trace3D Consistent Segmentation Lifting via Gaussian Instance Tracing](http://arxiv.org/abs/2508.03227)
* [TopoTTA Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation](http://arxiv.org/abs/2508.00442)
* [FE-CLIP Frequency Enhanced CLIP Model for Zero-Shot Anomaly Detection and Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Gong_FE-CLIP_Frequency_Enhanced_CLIP_Model_for_Zero-Shot_Anomaly_Detection_and_ICCV_2025_paper.pdf)
* [ReferEverything Towards Segmenting Everything We Can Speak of in Videos](http://arxiv.org/abs/2410.23287)
* 部分分割
  * [PartField Learning 3D Feature Fields for Part Segmentation and Beyond](http://arxiv.org/abs/2504.11451)
  * [Knowledge-Guided Part Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Gou_Knowledge-Guided_Part_Segmentation_ICCV_2025_paper.pdf)
* 场景分割
  * [Multi-modal Segment Anything Model for Camouflaged Scene Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Ren_Multi-modal_Segment_Anything_Model_for_Camouflaged_Scene_Segmentation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ic-qialanqian/Vision-Language-SAM)
* 目标分割
  * [Seeing the Unseen A Semantic Alignment and Context-Aware Prompt Framework for Open-Vocabulary Camouflaged Object Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Ren_Seeing_the_Unseen_A_Semantic_Alignment_and_Context-Aware_Prompt_Framework_ICCV_2025_paper.pdf)
  * [Controllable-LPMoE Adapting to Challenging Object Segmentation via Dynamic Local Priors from Mixture-of-Experts](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Controllable-LPMoE_Adapting_to_Challenging_Object_Segmentation_via_Dynamic_Local_Priors_ICCV_2025_paper.pdf)
  * [Breaking Rectangular Shackles Cross-View Object Segmentation for Fine-Grained Object Geo-Localization](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Breaking_Rectangular_Shackles_Cross-View_Object_Segmentation_for_Fine-Grained_Object_Geo-Localization_ICCV_2025_paper.pdf)<br>:house:[project](https://zqwlearning.github.io/CVOS)
  * [Temporal Overlapping Prediction A Self-supervised Pre-training Method for LiDAR Moving Object Segmentation](http://arxiv.org/abs/2503.07167)<br>:star:[code](https://github.com/ZiliangMiao/TOP)
* 抠图
  * [SDMatte: Grafting Diffusion Models for Interactive Matting](https://arxiv.org/pdf/2508.00443v1)<br>:star:[code](https://github.com/vivoCameraResearch/SDMatte)
  * [ZIM Zero-Shot Image Matting for Anything](http://arxiv.org/abs/2411.00626)<br>:house:[project](https://naver-ai.github.io/ZIM)
* 小样本分割
  * [Balancing Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot Segmentation](https://arxiv.org/pdf/2507.19140v1)<br>:star:[code](https://github.com/tianyu-zou/PAHNet)
  * [DeFSS Image-to-Mask Denoising Learning for Few-shot Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_DeFSS_Image-to-Mask_Denoising_Learning_for_Few-shot_Segmentation_ICCV_2025_paper.pdf)
  * [Adapting In-Domain Few-Shot Segmentation to New Domains without Source Domain Retraining](https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_Adapting_In-Domain_Few-Shot_Segmentation_to_New_Domains_without_Source_Domain_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/fanq15/ISA)
  * [Balancing Conservatism and Aggressiveness Prototype-Affinity Hybrid Network for Few-Shot Segmentation](http://arxiv.org/abs/2507.19140)<br>:star:[code](https://github.com/tianyu-zou/PAHNet)
  * [Object-level Correlation for Few-Shot Segmentation](http://arxiv.org/abs/2509.07917)
* 开放词汇分割
  * [ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation](http://arxiv.org/pdf/2506.21233v1)<br>:star:[code](https://github.com/xiweix/ReME)
  * [Stepping Out of Similar Semantic Space for Open-Vocabulary Segmentation](http://arxiv.org/abs/2506.16058)
  * [Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation](http://arxiv.org/abs/2508.20265)
  * [Talking to DINO Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation](http://arxiv.org/abs/2411.19331)
  * [Harnessing Vision Foundation Models for High-Performance Training-Free Open Vocabulary Segmentation](http://arxiv.org/abs/2411.09219)<br>:star:[code](https://github.com/YuHengsss/Trident)
* 实例分割
  * [Details Matter for Indoor Open-vocabulary 3D Instance Segmentation](https://arxiv.org/pdf/2507.23134v1)
  * [OV3D-CG Open-vocabulary 3D Instance Segmentation with Contextual Guidance](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_OV3D-CG_Open-vocabulary_3D_Instance_Segmentation_with_Contextual_Guidance_ICCV_2025_paper.pdf)<br>:house:[project](https://vipl-vsu.github.io/OV3D-CG)
  * [MOBIUS Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning](https://openaccess.thecvf.com/content/ICCV2025/papers/Segu_MOBIUS_Big-to-Mobile_Universal_Instance_Segmentation_via_Multi-modal_Bottleneck_Fusion_and_ICCV_2025_paper.pdf)
  * [WeaveSeg Iterative Contrast-weaving and Spectral Feature-refining for Nuclei Instance Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation_ICCV_2025_paper.pdf)
  * [CutS3D Cutting Semantics in 3D for 2D Unsupervised Instance Segmentation](http://arxiv.org/abs/2411.16319)
  * [S4M Boosting Semi-Supervised Instance Segmentation with SAM](https://openaccess.thecvf.com/content/ICCV2025/papers/Yoon_S4M_Boosting_Semi-Supervised_Instance_Segmentation_with_SAM_ICCV_2025_paper.pdf)
  * 零样本实例分割
    * [Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation](https://arxiv.org/pdf/2508.04122v1)
* 全景分割
  * [PanSt3R: Multi-view Consistent Panoptic Segmentation](http://arxiv.org/pdf/2506.21348v1)
  * [Prior2Former - Evidential Modeling of Mask Transformers for Assumption-Free Open-World Panoptic Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World_ICCV_2025_paper.pdf)
  * [4DSegStreamer Streaming 4D Panoptic Segmentation via Dual Threads](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_4DSegStreamer_Streaming_4D_Panoptic_Segmentation_via_Dual_Threads_ICCV_2025_paper.pdf)
* 语义分割
  * [Exploring Probabilistic Modeling Beyond Domain Generalization for Semantic Segmentation](https://arxiv.org/pdf/2507.21367v1)
  * [Revisiting Efficient Semantic Segmentation: Learning Offsets for Better Spatial and Class Feature Alignment](https://arxiv.org/pdf/2508.08811v1)<br>:star:[code](https://github.com/HVision-NKU/OffSeg)
  * [Revisiting Efficient Semantic Segmentation Learning Offsets for Better Spatial and Class Feature Alignment](http://arxiv.org/abs/2508.08811)
  * [Identity-aware Language Gaussian Splatting for Open-vocabulary 3D Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Jang_Identity-aware_Language_Gaussian_Splatting_for_Open-vocabulary_3D_Semantic_Segmentation_ICCV_2025_paper.pdf)
  * [Incremental Few-Shot Semantic Segmentation via Multi-Level Switchable Visual Prompts](https://openaccess.thecvf.com/content/ICCV2025/papers/Wan_Incremental_Few-Shot_Semantic_Segmentation_via_Multi-Level_Switchable_Visual_Prompts_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/WanMotion/MSVP)
  * [Reducing Unimodal Bias in Multi-Modal Semantic Segmentation with Multi-Scale Functional Entropy Regularization](http://arxiv.org/abs/2505.06635)
  * [UniDxMD Towards Unified Representation for Cross-Modal Unsupervised Domain Adaptation in 3D Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in_ICCV_2025_paper.pdf)
  * [Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Jeon_Exploiting_Domain_Properties_in_Language-Driven_Domain_Generalization_for_Semantic_Segmentation_ICCV_2025_paper.pdf)
  * [Auto-Vocabulary Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Ulger_Auto-Vocabulary_Semantic_Segmentation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ozzyou/AutoSeg)
  * [Stronger Steadier  Superior Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation](http://arxiv.org/abs/2504.12753)<br>:star:[code](https://github.com/SY-Ch/DepthForge)
  * [Unsupervised Histopathological Image Semantic Segmentation with Overlapping Patches Consistency Constraint](https://openaccess.thecvf.com/content/ICCV2025/papers/Cai_Unsupervised_Histopathological_Image_Semantic_Segmentation_with_Overlapping_Patches_Consistency_Constraint_ICCV_2025_paper.pdf)
  * [Pseudo-SD Pseudo Controlled Stable Diffusion for Semi-Supervised and Cross-Domain Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_Pseudo-SD_Pseudo_Controlled_Stable_Diffusion_for_Semi-Supervised_and_Cross-Domain_Semantic_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/DZhaoXd/Pseudo-SD)
  * [Learning Yourself Class-Incremental Semantic Segmentation with Language-Inspired Bootstrapped Disentanglement](http://arxiv.org/abs/2509.00527)
  * [Exploring Weather-aware Aggregation and Adaptation for Semantic Segmentation under Adverse Conditions](https://openaccess.thecvf.com/content/ICCV2025/papers/Pan_Exploring_Weather-aware_Aggregation_and_Adaptation_for_Semantic_Segmentation_under_Adverse_ICCV_2025_paper.pdf)
  * [OmniSAM Omnidirectional Segment Anything Model for UDA in Panoramic Semantic Segmentation](http://arxiv.org/abs/2503.07098)
  * [CoralSRT Revisiting Coral Reef Semantic Segmentation by Feature Rectification via Self-supervised Guidance](https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_CoralSRT_Revisiting_Coral_Reef_Semantic_Segmentation_by_Feature_Rectification_via_ICCV_2025_paper.pdf)
  * [Communication-Efficient Multi-Vehicle Collaborative Semantic Segmentation via Sparse 3D Gaussian Sharing](https://openaccess.thecvf.com/content/ICCV2025/papers/Hong_Communication-Efficient_Multi-Vehicle_Collaborative_Semantic_Segmentation_via_Sparse_3D_Gaussian_Sharing_ICCV_2025_paper.pdf)
  * 半监督语义分割
    * [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/pdf/2507.15803v1)
    * [When Confidence Fails Revisiting Pseudo-Label Selection in Semi-supervised Semantic Segmentation](http://arxiv.org/abs/2509.16704)<br>:star:[code](https://github.com/PanLiuCSU/CSL)
    * [Two Losses One Goal Balancing Conflict Gradients for Semi-supervised Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic_ICCV_2025_paper.pdf)
  * 弱监督语义分割
    * [Class Token as Proxy Optimal Transport-assisted Proxy Learning for Weakly Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Class_Token_as_Proxy_Optimal_Transport-assisted_Proxy_Learning_for_Weakly_ICCV_2025_paper.pdf)
  * [Bias-Resilient Weakly Supervised Semantic Segmentation Using Normalizing Flows](https://openaccess.thecvf.com/content/ICCV2025/papers/Qiu_Bias-Resilient_Weakly_Supervised_Semantic_Segmentation_Using_Normalizing_Flows_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/DpDark/BRNF)
  * [Know Your Attention Maps Class-specific Token Masking for Weakly Supervised Semantic Segmentation](http://arxiv.org/abs/2507.06848)
  * 小样本语义分割
    * [Probabilistic Prototype Calibration of Vision-Language Models for Generalized Few-shot Semantic Segmentation](https://arxiv.org/pdf/2506.22979v1)<br>:star:[code](https://github.com/jliu4ai/FewCLIP)
  * 开放词汇语义分割
    * [Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](https://arxiv.org/pdf/2507.11030v1)
    * [Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](http://arxiv.org/abs/2507.11030)
    * [Training-Free Class Purification for Open-Vocabulary Semantic Segmentation](https://arxiv.org/pdf/2508.00557v1)
    * [Images as Noisy Labels Unleashing the Potential of the Diffusion Model for Open-Vocabulary Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion_ICCV_2025_paper.pdf)
    * [FLOSS Free Lunch in Open-vocabulary Semantic Segmentation](http://arxiv.org/abs/2504.10487)<br>:star:[code](https://github.com/yasserben/FLOSS)
    * [DIH-CLIP Unleashing the Diversity of Multi-Head Self-Attention for Training-Free Open-Vocabulary Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Duan_DIH-CLIP_Unleashing_the_Diversity_of_Multi-Head_Self-Attention_for_Training-Free_Open-Vocabulary_ICCV_2025_paper.pdf)
    * [CLIPer Hierarchically Improving Spatial Representation of CLIP for Open-Vocabulary Semantic Segmentation](http://arxiv.org/abs/2411.13836)<br>:star:[code](https://github.com/linsun449/cliper.code)
    * [CorrCLIP Reconstructing Patch Correlations in CLIP for Open-Vocabulary Semantic Segmentation](http://arxiv.org/abs/2411.10086)<br>:star:[code](https://github.com/zdk258/CorrCLIP)
    * [CLIP-Adapted Region-to-Text Learning for Generative Open-Vocabulary Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Ge_CLIP-Adapted_Region-to-Text_Learning_for_Generative_Open-Vocabulary_Semantic_Segmentation_ICCV_2025_paper.pdf)
* 指代图像分割
  * [DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy](https://arxiv.org/pdf/2507.01738v1)<br>:star:[code](https://github.com/Dmmm1997/DeRIS)
  * [Latent Expression Generation for Referring Image Segmentation and Grounding](https://arxiv.org/pdf/2508.05123v1)
* 视频分割
  * [Online Reasoning Video Segmentation with Just-in-Time Digital Twins](http://arxiv.org/abs/2503.21056)<br>:star:[code](https://github.com/yiqings/jitbench)
  * [SAM2Long Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree](http://arxiv.org/abs/2410.16268)<br>:star:[code](https://github.com/Mark12Ding/SAM2Long)
  * [GDKVM Echocardiography Video Segmentation via Spatiotemporal Key-Value Memory with Gated Delta Rule](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_GDKVM_Echocardiography_Video_Segmentation_via_Spatiotemporal_Key-Value_Memory_with_Gated_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/wangrui2025/GDKVM)
* 交互分割  
  * [Towards Fine-grained Interactive Segmentation in Images and Videos](http://arxiv.org/abs/2502.09660)
  * [DC-TTA Divide-and-Conquer Framework for Test-Time Adaptation of Interactive Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_DC-TTA_Divide-and-Conquer_Framework_for_Test-Time_Adaptation_of_Interactive_Segmentation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/jihun1998/DCTTA)
  * [Inter2Former Dynamic Hybrid Attention for Efficient High-Precision Interactive Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_Inter2Former_Dynamic_Hybrid_Attention_for_Efficient_High-Precision_Interactive_Segmentation_ICCV_2025_paper.pdf)
  * [Easy3D A Simple Yet Effective Method for 3D Interactive Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation_ICCV_2025_paper.pdf)
  * [MultiverSeg Scalable Interactive Segmentation of Biomedical Imaging Datasets with In-Context Guidance](http://arxiv.org/abs/2412.15058)
* VIS
  * [Latest Object Memory Management for Temporally Consistent Video Instance Segmentation](https://arxiv.org/pdf/2507.19754v1)<br>:star:[code](https://seung-hun-lee.github.io/projects/LOMM/)<br>:star:[code](https://github.com/Seung-Hun-Lee/LOMM)
  * [LOMM Latest Object Memory Management for Temporally Consistent Video Instance Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_LOMM_Latest_Object_Memory_Management_for_Temporally_Consistent_Video_Instance_ICCV_2025_paper.pdf)
  * [Hierarchical Visual Prompt Learning for Continual Video Instance Segmentation](https://arxiv.org/pdf/2508.08612v1)<br>:star:[code](https://github.com/JiahuaDong/HVPL)
  * [CAVIS Context-Aware Video Instance Segmentation](http://arxiv.org/abs/2407.03010)
  * [Temporal-aware Query Routing for Real-time Video Instance Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Cheng_Temporal-aware_Query_Routing_for_Real-time_Video_Instance_Segmentation_ICCV_2025_paper.pdf)
  * [Sliced Wasserstein Bridge for Open-Vocabulary Video Instance Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation_ICCV_2025_paper.pdf)
* VOS
  * [MOVE: Motion-Guided Few-Shot Video Object Segmentation](https://arxiv.org/pdf/2507.22061v1)<br>:house:[project](https://henghuiding.com/MOVE/)
  * [Structure Matters Revisiting Boundary Refinement in Video Object Segmentation](http://arxiv.org/abs/2507.18944)
  * [EVOLVE Event-Guided Deformable Feature Transfer and Dual-Memory Refinement for Low-Light Video Object Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Baek_EVOLVE_Event-Guided_Deformable_Feature_Transfer_and_Dual-Memory_Refinement_for_Low-Light_ICCV_2025_paper.pdf)
  * [ReferDINO Referring Video Object Segmentation with Visual Grounding Foundations](http://arxiv.org/abs/2501.14607)
  * [MPG-SAM 2 Adapting SAM 2 with Mask Priors and Global Context for Referring Video Object Segmentation](http://arxiv.org/abs/2501.13667)<br>:star:[code](https://github.com/rongfu-dsb/MPG-SAM2)
* VSS
  * [Dual-Temporal Exemplar Representation Network for Video Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_Dual-Temporal_Exemplar_Representation_Network_for_Video_Semantic_Segmentation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/zlxilo/DTERN)
* GRES
  * [CoHD A Counting-Aware Hierarchical Decoding Framework for Generalized Referring Expression Segmentation](http://arxiv.org/abs/2405.15658)




<a name="5"/>

## 5.Image Generation(图像生成)
* [Rethinking Layered Graphic Design Generation with a Top-Down Approach](https://arxiv.org/pdf/2507.05601v1)
* [QR-LoRA: Efficient and Disentangled Fine-tuning via QR Decomposition for Customized Generation](https://arxiv.org/pdf/2507.04599v1)
* [Text Embedding Knows How to Quantize Text-Guided Diffusion Models](https://arxiv.org/pdf/2507.10340v1)
* [Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models](https://arxiv.org/pdf/2507.14797v1)<br>:star:[code](https://github.com/BeierZhu/EPD)
* [Stable Diffusion Models are Secretly Good at Visual In-Context Learning](https://arxiv.org/pdf/2508.09949v1)
* [FlexGen Flexible Multi-View Generation from Text and Image Inputs](http://arxiv.org/abs/2410.10745)
* [Model Reveals What to Cache Profiling-Based Feature Reuse for Video Diffusion Models](http://arxiv.org/abs/2504.03140)
* [DAViD Modeling Dynamic Affordance of 3D Objects Using Pre-trained Video Diffusion Models](http://arxiv.org/abs/2501.08333)
* [From Prompt to Progression Taming Video Diffusion Models for Seamless Attribute Transition](http://arxiv.org/abs/2509.19690)
* [Dynamic Typography Bringing Text to Life via Video Diffusion Prior](http://arxiv.org/abs/2404.11614)
* [Mobile Video Diffusion](http://arxiv.org/abs/2412.07583)<br>:house:[project](https://qualcomm-ai-research.github.io/mobile-video-diffusion)
* [Latent-Reframe Enabling Camera Control for Video Diffusion Models without Training](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_Latent-Reframe_Enabling_Camera_Control_for_Video_Diffusion_Models_without_Training_ICCV_2025_paper.pdf)
* [LangScene-X Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_LangScene-X_Reconstruct_Generalizable_3D_Language-Embedded_Scenes_with_TriMap_Video_Diffusion_ICCV_2025_paper.pdf)
* [NormalCrafter Learning Temporally Consistent Normals from Video Diffusion Priors](http://arxiv.org/abs/2504.11427)<br>:house:[project](https://normalcrafter.github.io/)
* [Prompt-A-Video Prompt Your Video Diffusion Model via Preference-Aligned LLM](https://openaccess.thecvf.com/content/ICCV2025/papers/Ji_Prompt-A-Video_Prompt_Your_Video_Diffusion_Model_via_Preference-Aligned_LLM_ICCV_2025_paper.pdf)
* [DimensionX Create Any 3D and 4D Scenes from a Single Image with Decoupled Video Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_DimensionX_Create_Any_3D_and_4D_Scenes_from_a_Single_ICCV_2025_paper.pdf)
* [CameraCtrl II Dynamic Scene Exploration via Camera-controlled Video Diffusion Models](http://arxiv.org/abs/2503.10592)
* [Beyond Next-Token Next-X Prediction for Autoregressive Visual Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Ren_Beyond_Next-Token_Next-X_Prediction_for_Autoregressive_Visual_Generation_ICCV_2025_paper.pdf)<br>:house:[project](https://oliverrensu.github.io/project)
* [SpectralAR Spectral Autoregressive Visual Generation](http://arxiv.org/abs/2506.10962)<br>:house:[project](https://huang-yh.github.io/spectralar) :house:[project](https://huang-yh.github.io/spectralar/)
* [Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation](http://arxiv.org/abs/2503.16430)<br>:house:[project](https://yuqingwang1029.github.io/TokenBridge)
* [Randomized Autoregressive Visual Generation](http://arxiv.org/abs/2411.00776)<br>:star:[code](https://github.com/bytedance/1d-tokenizer)
* [PUMA Empowering Unified MLLM with Multi-granular Visual Generation](http://arxiv.org/abs/2410.13861)
* [Neighboring Autoregressive Modeling for Efficient Visual Generation](http://arxiv.org/abs/2503.10696)
* [RealGeneral Unifying Visual Generation via Temporal In-Context Learning with Video Models](http://arxiv.org/abs/2503.10406)<br>:star:[code](https://github.com/Lyne1/RealGeneral) :house:[project](https://lyne1.github.io/realgeneral_web/) :house:[project](https://lyne1.github.io/realgeneral_web)
* [3D Mesh Editing using Masked LRMs](http://arxiv.org/abs/2412.08641)
* [PixTalk Controlling Photorealistic Image Processing and Editing with Language](https://openaccess.thecvf.com/content/ICCV2025/papers/Conde_PixTalk_Controlling_Photorealistic_Image_Processing_and_Editing_with_Language_ICCV_2025_paper.pdf)
* [TextMaster A Unified Framework for Realistic Text Editing via Glyph-Style Dual-Control](http://arxiv.org/abs/2410.09879)
* [FlowEdit Inversion-Free Text-Based Editing Using Pre-Trained Flow Models](http://arxiv.org/abs/2412.08629)
* [Efficient Autoregressive Shape Generation via Octree-Based Adaptive Tokenization](http://arxiv.org/abs/2504.02817)
* [ObjectMate A Recurrence Prior for Object Insertion and Subject-Driven Generation](http://arxiv.org/abs/2412.08645)
* [WikiAutoGen Towards Multi-Modal Wikipedia-Style Article Generation](http://arxiv.org/abs/2503.19065)
* [CMT A Cascade MAR with Topology Predictor for Multimodal Conditional CAD Generation](http://arxiv.org/abs/2504.20830)
* [MaterialMVP Illumination-Invariant Material Generation via Multi-view PBR Diffusion](http://arxiv.org/abs/2503.10289)
* [Unleashing Vecset Diffusion Model for Fast Shape Generation](http://arxiv.org/abs/2503.16302)<br>:star:[code](https://github.com/Tencent-Hunyuan/FlashVDM)
* [LaneDiffusion Improving Centerline Graph Learning via Prior Injected BEV Feature Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_LaneDiffusion_Improving_Centerline_Graph_Learning_via_Prior_Injected_BEV_Feature_ICCV_2025_paper.pdf)
* [ScanEdit Hierarchically-Guided Functional 3D Scan Editing](http://arxiv.org/abs/2504.15049)
* [NeuralSVG An Implicit Representation for Text-to-Vector Generation](http://arxiv.org/abs/2501.03992)
* [Fine-Tuning Visual Autogressive Models for Subject-Driven Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Chung_Fine-Tuning_Visual_Autogressive_Models_for_Subject-Driven_Generation_ICCV_2025_paper.pdf)
* 扩散模型
  * [Penalizing Boundary Activation for Object Completeness in Diffusion Models](http://arxiv.org/abs/2509.16968)
  * [Golden Noise for Diffusion Models A Learning Framework](http://arxiv.org/abs/2411.09502)
  * [Learning 3D Object Spatial Relationships from Pre-trained 2D Diffusion Models](http://arxiv.org/abs/2503.19914)
  * [DiffDoctor Diagnosing Image Diffusion Models Before Treating](http://arxiv.org/abs/2501.12382)
  * [LoRAverse A Submodular Framework to Retrieve Diverse Adapters for Diffusion Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Sonmezer_LoRAverse_A_Submodular_Framework_to_Retrieve_Diverse_Adapters_for_Diffusion_ICCV_2025_paper.pdf)
  * [Revelio Interpreting and leveraging semantic information in diffusion models](https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_Revelio_Interpreting_and_leveraging_semantic_information_in_diffusion_models_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/revelio-diffusion/revelio)
  * [Latent Diffusion Models with Masked AutoEncoders](http://arxiv.org/abs/2507.09984)
  * [Timestep-Aware Diffusion Model for Extreme Image Rescaling](http://arxiv.org/abs/2408.09151)<br>:star:[code](https://github.com/wwangcece/TADM)
  * [Bootstrap3D Improving Multi-view Diffusion Model with Synthetic Data](http://arxiv.org/abs/2406.00093)
  * [DiffSim Taming Diffusion Models for Evaluating Visual Similarity](http://arxiv.org/abs/2412.14580)
* 布局生成
  * [IGD: Instructional Graphic Design with Multimodal Layer Generation](https://arxiv.org/pdf/2507.09910v1)
* 图像合成
  * [Preserve Anything: Controllable Image Synthesis with Object Preservation](https://arxiv.org/pdf/2506.22531v1)
  * [PathDiff Histopathology Image Synthesis with Unpaired Text and Mask Conditions](http://arxiv.org/abs/2506.23440)<br>:star:[code](https://github.com/bhosalems/PathDiff)
  * [Rethinking Discrete Tokens Treating Them as Conditions for Continuous Autoregressive Image Synthesis](http://arxiv.org/abs/2507.01756)<br>:house:[project](https://pengzheng0707.github.io/DisCon)
  * [AIComposer: Any Style and Content Image Composition via Feature Integration](https://arxiv.org/pdf/2507.20721v1)<br>:star:[code](https://github.com/sherlhw/AIComposer)
  * [Toward Better Out-painting Improving the Image Composition with Initialization Policy Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Han_Toward_Better_Out-painting_Improving_the_Image_Composition_with_Initialization_Policy_ICCV_2025_paper.pdf)
  * [ViCTr Vital Consistency Transfer for Pathology Aware Image Synthesis](http://arxiv.org/abs/2505.04963)
  * [InfGen A Resolution-Agnostic Paradigm for Scalable Image Synthesis](http://arxiv.org/abs/2509.10441)
  * [Frequency-Aware Autoregressive Modeling for Efficient High-Resolution Image Synthesis](http://arxiv.org/abs/2507.20454)
  * [AM-Adapter Appearance Matching Adapter for Exemplar-based Semantic Image Synthesis in-the-Wild](https://openaccess.thecvf.com/content/ICCV2025/papers/Jin_AM-Adapter_Appearance_Matching_Adapter_for_Exemplar-based_Semantic_Image_Synthesis_in-the-Wild_ICCV_2025_paper.pdf)
  * [Leveraging BEV Paradigm for Ground-to-Aerial Image Synthesis](http://arxiv.org/abs/2408.01812)<br>:house:[project](https://opendatalab.github.io/skydiffusion)
  * [PolarAnything Diffusion-based Polarimetric Image Synthesis](http://arxiv.org/abs/2507.17268)
* 图像生成
  * [DC-AR: Efficient Masked Autoregressive Image Generation with Deep Compression Hybrid Tokenizer](https://arxiv.org/pdf/2507.04947v1)
  * [Scale Your Instructions: Enhance the Instruction-Following Fidelity of Unified Image Generation Model by Self-Adaptive Attention Scaling](https://arxiv.org/pdf/2507.16240v1)<br>:star:[code](https://github.com/zhouchao-ops/SaaS)
  * [HPSv3: Towards Wide-Spectrum Human Preference Score](https://arxiv.org/pdf/2508.03789v1)
  * [Anti-Tamper Protection for Unauthorized Individual Image Generation](https://arxiv.org/pdf/2508.06325v1)<br>:star:[code](https://github.com/Seeyn/Anti-Tamper-Perturbation)
  * [Lumina-Image 20 A Unified and Efficient Image Generative Framework](https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Lumina-Image_2.0_A_Unified_and_Efficient_Image_Generative_Framework_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Alpha-VLLM/Lumina-Image-2.0)
  * [Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment](https://arxiv.org/pdf/2507.19002v1)<br>:star:[code](https://github.com/BarretBa/ICTHP)
  * [LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing](https://arxiv.org/pdf/2507.22627v1)<br>:star:[code](https://intelligolabs.github.io/lots/)<br>:star:[code](https://github.com/intelligolabs/lots)
  * [Trade-offs in Image Generation: How Do Different Dimensions Interact?](https://arxiv.org/pdf/2507.22100v1)<br>:star:[code](https://github.com/fesvhtr/TRIG)
  * [Grouped Speculative Decoding for Autoregressive Image Generation](https://arxiv.org/pdf/2508.07747v1)<br>:star:[code](https://github.com/junhyukso/GSD)
  * [LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering](https://arxiv.org/pdf/2508.07647v1)<br>:star:[code](https://xiaohangzhan.github.io/projects/larender/)
  * [HypDAE Hyperbolic Diffusion Autoencoders for Hierarchical Few-shot Image Generation](http://arxiv.org/abs/2411.17784)
  * [USP Unified Self-Supervised Pretraining for Image Generation and Understanding](http://arxiv.org/abs/2503.06132)
  * [Holistic Tokenizer for Autoregressive Image Generation](http://arxiv.org/abs/2507.02358)<br>:star:[code](https://github.com/CVMI-Lab/Hita)
  * [HDR Image Generation via Gain Map Decomposed Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Guan_HDR_Image_Generation_via_Gain_Map_Decomposed_Diffusion_ICCV_2025_paper.pdf)
  * [EmotiCrafter Text-to-Emotional-Image Generation based on Valence-Arousal Model](http://arxiv.org/abs/2501.05710)
  * [MV-Adapter Multi-View Consistent Image Generation Made Easy](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_MV-Adapter_Multi-View_Consistent_Image_Generation_Made_Easy_ICCV_2025_paper.pdf)
  * [Enhancing Reward Models for High-quality Image Generation Beyond Text-Image Alignment](http://arxiv.org/abs/2507.19002)
  * [CAP Evaluation of Persuasive and Creative Image Generation](http://arxiv.org/abs/2412.10426)
  * [Trade-offs in Image Generation How Do Different Dimensions Interact](http://arxiv.org/abs/2507.22100)<br>:star:[code](https://github.com/fesvhtr/TRIG)
  * [VisualCloze A Universal Image Generation Framework via Visual In-Context Learning](http://arxiv.org/abs/2504.07960)<br>:house:[project](https://visualcloze.github.io/)
  * [LiT Delving into a Simple Linear Diffusion Transformer for Image Generation](http://arxiv.org/abs/2501.12976)
  * [UniVG A Generalist Diffusion Model for Unified Image Generation and Editing](http://arxiv.org/abs/2503.12652)
  * [CompSlider Compositional Slider for Disentangled Multiple-Attribute Image Generation](http://arxiv.org/abs/2509.01028)
  * [DC-ControlNet Decoupling Inter- and Intra-Element Conditions in Image Generation with Diffusion Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_DC-ControlNet_Decoupling_Inter-_and_Intra-Element_Conditions_in_Image_Generation_with_ICCV_2025_paper.pdf)
  * [Wasserstein Style Distribution Analysis and Transform for Stylized Image Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation_ICCV_2025_paper.pdf)
  * [LLM Thought Divergence and Convergence for Dialogue-Based Image Generation Control](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_LLM_Thought_Divergence_and_Convergence_for_Dialogue-Based_Image_Generation_Control_ICCV_2025_paper.pdf)
  * [Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin](http://arxiv.org/abs/2505.24222)
  * [The Silent Assistant NoiseQuery as Implicit Guidance for Goal-Driven Image Generation](http://arxiv.org/abs/2412.05101)
  * [FICGen Frequency-Inspired Contextual Disentanglement for Layout-driven Degraded Image Generation](http://arxiv.org/abs/2509.01107)
  * [PlanGen Towards Unified Layout Planning and Image Generation in Auto-Regressive Vision Language Models](http://arxiv.org/abs/2503.10127)
  * [GigaTok Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation](http://arxiv.org/abs/2504.08736)
  * [Contrastive Test-Time Composition of Multiple LoRA Models for Image Generation](http://arxiv.org/abs/2403.19776)
  * [GeoDiffusion A Training-Free Framework for Accurate 3D Geometric Conditioning in Image Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Mueller_GeoDiffusion_A_Training-Free_Framework_for_Accurate_3D_Geometric_Conditioning_in_ICCV_2025_paper.pdf)
  * [LoRArar Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Shenaj_LoRA.rar_Learning_to_Merge_LoRAs_via_Hypernetworks_for_Subject-Style_Conditioned_ICCV_2025_paper.pdf)
  * [IntrinsicControlNet Cross-distribution Image Generation with Real and Unreal](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_IntrinsicControlNet_Cross-distribution_Image_Generation_with_Real_and_Unreal_ICCV_2025_paper.pdf)
  * [Dual-Process Image Generation](http://arxiv.org/abs/2506.01955)
  * [LMM4LMM Benchmarking and Evaluating Large-multimodal Image Generation with LMMs](http://arxiv.org/abs/2504.08358)<br>:star:[code](https://github.com/IntMeGroup/LMM4LMM)
* 文本-图像
  * [Rethink Sparse Signals for Pose-guided Text-to-image Generation](http://arxiv.org/pdf/2506.20983v1)<br>:star:[code](https://github.com/DREAMXFAR/SP-Ctrl)
  * [CharaConsist: Fine-Grained Consistent Character Generation](https://arxiv.org/pdf/2507.11533v1)<br>:star:[code](https://murray-wang.github.io/CharaConsist/)<br>:star:[code](https://github.com/Murray-Wang/CharaConsist)
  * [Multimodal LLMs as Customized Reward Models for Text-to-Image Generation](https://arxiv.org/pdf/2507.21391v1)<br>:star:[code](https://github.com/sjz5202/LLaVA-Reward)
  * [TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance](https://arxiv.org/pdf/2507.18192v1)<br>:star:[code](https://github.com/AIDC-AI/TeEFusion)
  * [T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation](https://arxiv.org/pdf/2507.20536v1)<br>:star:[code](https://github.com/SHI-Labs/T2I-Copilot)
  * [YOLO-Count: Differentiable Object Counting for Text-to-Image Generation](https://arxiv.org/pdf/2508.00728v1)
  * [Steering Guidance for Personalized Text-to-Image Diffusion Models](https://arxiv.org/pdf/2508.00319v1)
  * [PLA: Prompt Learning Attack against Text-to-Image Generative Models](https://arxiv.org/pdf/2508.03696v1)
  * [ROVI A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation](http://arxiv.org/abs/2508.01008)<br>:star:[code](https://github.com/CihangPeng/ROVI)
  * [FedDifRC Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning](http://arxiv.org/abs/2507.06482)
  * [Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models](https://arxiv.org/pdf/2508.03481v1)
  * [Automated Red Teaming for Text-to-Image Models through Feedback-Guided Prompt Iteration with Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_Automated_Red_Teaming_for_Text-to-Image_Models_through_Feedback-Guided_Prompt_Iteration_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Weiww-Xu/FGPI)
  * [Scene Graph Guided Generation Enable Accurate Relations Generation in Text-to-Image Models via Textural Rectification](https://openaccess.thecvf.com/content/ICCV2025/papers/Shen_Scene_Graph_Guided_Generation_Enable_Accurate_Relations_Generation_in_Text-to-Image_ICCV_2025_paper.pdf)
  * [ImageGen-CoT Enhancing Text-to-Image In-context Learning with Chain-of-Thought Reasoning](https://openaccess.thecvf.com/content/ICCV2025/papers/Liao_ImageGen-CoT_Enhancing_Text-to-Image_In-context_Learning_with_Chain-of-Thought_Reasoning_ICCV_2025_paper.pdf)<br>:house:[project](https://ImageGen-CoT.github.io/)
  * [Holistic Unlearning Benchmark A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning](http://arxiv.org/abs/2410.05664)
  * [Efficient Input-level Backdoor Defense on Text-to-Image Synthesis via Neuron Activation Variation](http://arxiv.org/abs/2503.06453)
  * [Leveraging Panoptic Scene Graph for Evaluating Fine-Grained Text-to-Image Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Deng_Leveraging_Panoptic_Scene_Graph_for_Evaluating_Fine-Grained_Text-to-Image_Generation_ICCV_2025_paper.pdf)
  * [Decoding Correlation-Induced Misalignment in the Stable Diffusion Workflow for Text-to-Image Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Tong_Decoding_Correlation-Induced_Misalignment_in_the_Stable_Diffusion_Workflow_for_Text-to-Image_ICCV_2025_paper.pdf)
  * [Dense2MoE Restructuring Diffusion Transformer to MoE for Efficient Text-to-Image Generation](http://arxiv.org/abs/2510.09094)
  * [Fair Generation without Unfair Distortions Debiasing Text-to-Image Generation with Entanglement-Free Attention](http://arxiv.org/abs/2506.13298)
  * [Adaptive Routing of Text-to-Image Generation Requests Between Large Cloud Model and Light-Weight Edge Model](http://arxiv.org/abs/2411.13787)
  * [IFAdapter Instance Feature Control for Grounded Text-to-Image Generation](http://arxiv.org/abs/2409.08240)
  * [AutoPrompt Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_AutoPrompt_Automated_Red-Teaming_of_Text-to-Image_Models_via_LLM-Driven_Adversarial_Prompts_ICCV_2025_paper.pdf)
  * [AlignGuard Scalable Safety Alignment for Text-to-Image Generation](http://arxiv.org/abs/2412.10493)
  * [UniversalBooth Model-Agnostic Personalized Text-to-Image Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_UniversalBooth_Model-Agnostic_Personalized_Text-to-Image_Generation_ICCV_2025_paper.pdf)
  * [TF-TI2I Training-Free Text-and-Image-to-Image Generation via Multi-Modal Implicit-Context Learning In Text-to-Image Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Hsiao_TF-TI2I_Training-Free_Text-and-Image-to-Image_Generation_via_Multi-Modal_Implicit-Context_Learning_In_Text-to-Image_ICCV_2025_paper.pdf)
  * [LBM Latent Bridge Matching for Fast Image-to-Image Translation](http://arxiv.org/abs/2503.07535)
  * [Scalable Ranked Preference Optimization for Text-to-Image Generation](http://arxiv.org/abs/2410.18013)
  * [RAGD Regional-Aware Diffusion Model for Text-to-Image Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_RAGD_Regional-Aware_Diffusion_Model_for_Text-to-Image_Generation_ICCV_2025_paper.pdf)
  * [TRCE Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models](http://arxiv.org/abs/2503.07389)
  * [Region-Level Data Attribution for Text-to-Image Generative Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Nguyen_Region-Level_Data_Attribution_for_Text-to-Image_Generative_Models_ICCV_2025_paper.pdf)
  * [FairGen Enhancing Fairness in Text-to-Image Diffusion Models via Self-Discovering Latent Directions](https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_FairGen_Enhancing_Fairness_in_Text-to-Image_Diffusion_Models_via_Self-Discovering_Latent_ICCV_2025_paper.pdf)
  * [DIMCIM A Quantitative Evaluation Framework for Default-mode Diversity and Generalization in Text-to-Image Generative Models](http://arxiv.org/abs/2506.05108)
  * [From Reflection to Perfection Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning](http://arxiv.org/abs/2504.16080)<br>:house:[project](https://diffusion-cot.github.io/reflection2perfection)
  * [CoMPaSS Enhancing Spatial Understanding in Text-to-Image Diffusion Models](http://arxiv.org/abs/2412.13195)
  * [SuMa A Subspace Mapping Approach for Robust and Effective Concept Erasure in Text-to-Image Diffusion Models](http://arxiv.org/abs/2509.05625)
  * [VSC Visual Search Compositional Text-to-Image Diffusion Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Dat_VSC_Visual_Search_Compositional_Text-to-Image_Diffusion_Model_ICCV_2025_paper.pdf)
  * [Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens](http://arxiv.org/abs/2501.07730)
  * [Reflect-DiT Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Reflect-DiT_Inference-Time_Scaling_for_Text-to-Image_Diffusion_Transformers_via_In-Context_Reflection_ICCV_2025_paper.pdf)
  * [Supercharged One-step Text-to-Image Diffusion Models with Negative Prompts](http://arxiv.org/abs/2412.02687)
  * [TCFG Truncated Classifier-Free Guidance for Efficient and Scalable Text-to-Image Acceleration](https://openaccess.thecvf.com/content/ICCV2025/papers/Fu_TCFG_Truncated_Classifier-Free_Guidance_for_Efficient_and_Scalable_Text-to-Image_Acceleration_ICCV_2025_paper.pdf)
  * [Discovering Divergent Representations between Text-to-Image Models](http://arxiv.org/abs/2509.08940)
  * [CuRe Cultural Gaps in the Long Tail of Text-to-Image Systems](http://arxiv.org/abs/2506.08071)<br>:house:[project](https://aniketrege.github.io/cure)
  * [Transformed Low-rank Adaptation via Tensor Decomposition and Its Applications to Text-to-image Models](http://arxiv.org/abs/2501.08727)
  * [Generating Multi-Image Synthetic Data for Text-to-Image Customization](http://arxiv.org/abs/2502.01720)
  * [Parametric Shadow Control for Portrait Generation in Text-to-Image Diffusion Models](http://arxiv.org/abs/2503.21943)
  * [Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets](http://arxiv.org/abs/2508.21032)
  * [DreamRenderer Taming Multi-Instance Attribute Control in Large-Scale Text-to-Image Models](http://arxiv.org/abs/2503.12885)<br>:house:[project](https://limuloo.github.io/DreamRenderer/) :house:[project](https://limuloo.github.io/DreamRenderer)
  * [Scalable Dual Fingerprinting for Hierarchical Attribution of Text-to-Image Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models_ICCV_2025_paper.pdf)
  * [Who Controls the Authorization Invertible Networks for Copyright Protection in Text-to-Image Synthesis](https://openaccess.thecvf.com/content/ICCV2025/papers/Hu_Who_Controls_the_Authorization_Invertible_Networks_for_Copyright_Protection_in_ICCV_2025_paper.pdf)
  * [Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion](http://arxiv.org/abs/2508.09575)<br>:star:[code](https://github.com/jwonkm/DRF)
* 文本-视频
  * [TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models](https://arxiv.org/pdf/2508.00289v1)<br>:star:[code](https://titanguide.github.io)
  * [BadVideo Stealthy Backdoor Attack against Text-to-Video Generation](http://arxiv.org/abs/2504.16907)<br>:house:[project](https://wrt2000.github.io/BadVideo2025)
  * [VPO Aligning Text-to-Video Generation Models with Prompt Optimization](http://arxiv.org/abs/2503.20491)<br>:star:[code](https://github.com/thu-coai/VPO)
  * [MotionShot Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation](http://arxiv.org/abs/2507.16310)<br>:house:[project](https://motionshot.github.io/)
  * [Free2Guide Training-Free Text-to-Video Alignment using Image LVLM](https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_Free2Guide_Training-Free_Text-to-Video_Alignment_using_Image_LVLM_ICCV_2025_paper.pdf)<br>:house:[project](https://free2guide.github.io/)
  * [ETVA Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering](http://arxiv.org/abs/2503.16867)
  * [EfficientMT Efficient Temporal Adaptation for Motion Transfer in Text-to-Video Diffusion Models](http://arxiv.org/abs/2503.19369)<br>:star:[code](https://github.com/PrototypeNx/EfficientMT)
* 图像-视频
  * [RealCam-I2V Real-World Image-to-Video Generation with Interactive Complex Camera Control](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_RealCam-I2V_Real-World_Image-to-Video_Generation_with_Interactive_Complex_Camera_Control_ICCV_2025_paper.pdf)
  * [TIP-I2V A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_TIP-I2V_A_Million-Scale_Real_Text_and_Image_Prompt_Dataset_for_ICCV_2025_paper.pdf)<br>:house:[project](https://tip-i2v.github.io/)
  * [GeoMan Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion](http://arxiv.org/abs/2505.23085)
  * [I2V3D Controllable Image-to-video Generation with 3D Guidance](http://arxiv.org/abs/2503.09733)
  * [Versatile Transition Generation with Image-to-Video Diffusion](http://arxiv.org/abs/2508.01698)
* 视频合成
  * [Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis](https://arxiv.org/pdf/2506.23263v1)
  * [Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis](https://arxiv.org/pdf/2507.18569v1)
  * [Turbo2K Towards Ultra-Efficient and High-Quality 2K Video Synthesis](http://arxiv.org/abs/2504.14470)
  * [Synthetic Video Enhances Physical Fidelity in Video Synthesis](http://arxiv.org/abs/2503.20822)
  * [VACE All-in-One Video Creation and Editing](http://arxiv.org/abs/2503.07598)
  * [V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models](https://arxiv.org/pdf/2508.03254v1)<br>:star:[code](https://jiiiisoo.github.io/VIP.github.io/)<br>:star:[code](https://github.com/jiiiisoo/VIP.github.io)
  * [Precise Action-to-Video Generation Through Visual Action Prompts](http://arxiv.org/abs/2508.13104)
  * [AnimateAnyMesh A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation](http://arxiv.org/abs/2506.09982)
  * [MagicMotion Controllable Video Generation with Dense-to-Sparse Trajectory Guidance](http://arxiv.org/abs/2503.16421)
  * [DOLLAR Few-Step Video Generation via Distillation and Latent Reward Optimization](http://arxiv.org/abs/2412.15689)
  * [DLFR-Gen Diffusion-based Video Generation with Dynamic Latent Frame Rate](https://openaccess.thecvf.com/content/ICCV2025/papers/Yuan_DLFR-Gen_Diffusion-based_Video_Generation_with_Dynamic_Latent_Frame_Rate_ICCV_2025_paper.pdf)
  * [MagicMirror ID-Preserved Video Generation in Video Diffusion Transformers](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_MagicMirror_ID-Preserved_Video_Generation_in_Video_Diffusion_Transformers_ICCV_2025_paper.pdf)
  * [Authentic 4D Driving Simulation with a Video Generation Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Authentic_4D_Driving_Simulation_with_a_Video_Generation_Model_ICCV_2025_paper.pdf)
  * [NoiseController Towards Consistent Multi-view Video Generation via Noise Decomposition and Collaboration](http://arxiv.org/abs/2504.18448)
  * [DiTaiListener Controllable High Fidelity Listener Video Generation with Diffusion](http://arxiv.org/abs/2504.04010)<br>:house:[project](https://ihp-lab.github.io/DiTaiListener)
  * [VLIPP Towards Physically Plausible Video Generation with Vision and Language Informed Physical Prior](http://arxiv.org/abs/2503.23368)<br>:house:[project](https://madaoer.github.io/projects/physically_plausible_video_generation/) :house:[project](https://madaoer.github.io/projects)
  * [DropletVideo A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation](http://arxiv.org/abs/2503.06053)<br>:house:[project](https://dropletx.github.io/)
  * [A Unified Framework for Industrial Cel-Animation Colorization with Temporal-Structural Awareness](https://openaccess.thecvf.com/content/ICCV2025/papers/Feng_A_Unified_Framework_for_Industrial_Cel-Animation_Colorization_with_Temporal-Structural_Awareness_ICCV_2025_paper.pdf)
  * [X-Dancer Expressive Music to Human Dance Video Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation_ICCV_2025_paper.pdf)
  * [The Best of Both Worlds Integrating Language Models and Diffusion Models for Video Generation](http://arxiv.org/abs/2503.04606)<br>:house:[project](https://landiff.github.io/)
  * [Long Context Tuning for Video Generation](http://arxiv.org/abs/2503.10589)
  * [Dual-Expert Consistency Model for Efficient and High-Quality Video Generation](http://arxiv.org/abs/2506.03123)
  * [Reangle-A-Video 4D Video Generation as Video-to-Video Translation](https://openaccess.thecvf.com/content/ICCV2025/papers/Jeong_Reangle-A-Video_4D_Video_Generation_as_Video-to-Video_Translation_ICCV_2025_paper.pdf)<br>:house:[project](https://anony1anony2.github.io/)
  * [InstaDrive Instance-Aware Driving World Models for Realistic and Consistent Video Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_InstaDrive_Instance-Aware_Driving_World_Models_for_Realistic_and_Consistent_Video_ICCV_2025_paper.pdf)
  * [Importance-Based Token Merging for Efficient Image and Video Generation](http://arxiv.org/abs/2411.16720)
  * [MotionAgent Fine-grained Controllable Video Generation via Motion Field Agent](http://arxiv.org/abs/2502.03207)
  * [Video-T1 Test-time Scaling for Video Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Video-T1_Test-time_Scaling_for_Video_Generation_ICCV_2025_paper.pdf)
  * [Unified Video Generation via Next-Set Prediction in Continuous Domain](https://openaccess.thecvf.com/content/ICCV2025/papers/Feng_Unified_Video_Generation_via_Next-Set_Prediction_in_Continuous_Domain_ICCV_2025_paper.pdf)
  * [Phantom Subject-Consistent Video Generation via Cross-Modal Alignment](http://arxiv.org/abs/2502.11079)
  * [VideoAuteur Towards Long Narrative Video Generation](http://arxiv.org/abs/2501.06173)
  * [STIV Scalable Text and Image Conditioned Video Generation](http://arxiv.org/abs/2412.07730)
  * [Generating Fast and Slow Scalable Parallel Video Generation with Video Interface Networks](http://arxiv.org/abs/2503.17539)
  * [Puppet-Master Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Puppet-Master_Scaling_Interactive_Video_Generation_as_a_Motion_Prior_for_ICCV_2025_paper.pdf)
  * [Adaptive Caching for Faster Video Generation with Diffusion Transformers](http://arxiv.org/abs/2411.02397)
  * [T2Bs Text-to-Character Blendshapes via Video Generation](http://arxiv.org/abs/2509.10678)
  * [QuantCache Adaptive Importance-Guided Quantization with Hierarchical Latent and Layer Caching for Video Generation](http://arxiv.org/abs/2503.06545)
  * [Free-Form Motion Control Controlling the 6D Poses of Camera and Objects in Video Generation](http://arxiv.org/abs/2501.01425)
  * [FullDiT Video Generative Foundation Models with Multimodal Control via Full Attention](https://openaccess.thecvf.com/content/ICCV2025/papers/Ju_FullDiT_Video_Generative_Foundation_Models_with_Multimodal_Control_via_Full_ICCV_2025_paper.pdf)
  * 长视频合成
    * [Training-free and Adaptive Sparse Attention for Efficient Long Video Generation](http://arxiv.org/abs/2502.21079)
    * [TokensGen Harnessing Condensed Tokens for Long Video Generation](http://arxiv.org/abs/2507.15728)
* 视频编辑
  * [DIVE Taming DINO for Subject-Driven Video Editing](http://arxiv.org/abs/2412.03347)
  * [AnyPortal Zero-Shot Consistent Video Background Replacement](http://arxiv.org/abs/2509.07472)
  * [QK-Edit Revisiting Attention-based Injection in MM-DiT for Image and Video Editing](https://openaccess.thecvf.com/content/ICCV2025/papers/Shen_QK-Edit_Revisiting_Attention-based_Injection_in_MM-DiT_for_Image_and_Video_ICCV_2025_paper.pdf)
  * [FiVE-Bench A Fine-grained Video Editing Benchmark for Evaluating Emerging Diffusion and Rectified Flow Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_FiVE-Bench_A_Fine-grained_Video_Editing_Benchmark_for_Evaluating_Emerging_Diffusion_ICCV_2025_paper.pdf)<br>:house:[project](https://sites.google.com/view/five-benchmark)
  * [InsViE-1M Effective Instruction-based Video Editing with Elaborate Dataset Construction](https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_InsViE-1M_Effective_Instruction-based_Video_Editing_with_Elaborate_Dataset_Construction_ICCV_2025_paper.pdf)
* 图像编辑
  * [ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation](https://arxiv.org/pdf/2507.01496v1)<br>:star:[code](https://wlaud1001.github.io/ReFlex/)
  * [ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided Image Editing Evaluation](https://arxiv.org/pdf/2507.07317v1)<br>:star:[code](https://github.com/SherryXTChen/ADIEE.git)
  * [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](https://arxiv.org/pdf/2508.06033v1)
  * [Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing](https://arxiv.org/pdf/2508.07519v1)<br>:house:[project](https://joonghyuk.com/exploring-mmdit-web/)
  * [ArtEditor Learning Customized Instructional Image Editor from Few-Shot Examples](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_ArtEditor_Learning_Customized_Instructional_Image_Editor_from_Few-Shot_Examples_ICCV_2025_paper.pdf)
  * [Zero-Shot Depth Aware Image Editing with Diffusion Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Parihar_Zero-Shot_Depth_Aware_Image_Editing_with_Diffusion_Models_ICCV_2025_paper.pdf)
  * [Early Timestep Zero-Shot Candidate Selection for Instruction-Guided Image Editing](http://arxiv.org/abs/2504.13490)
  * [LUSD Localized Update Score Distillation for Text-Guided Image Editing](http://arxiv.org/abs/2503.11054)
  * [Training-free Geometric Image Editing on Diffusion Models](http://arxiv.org/abs/2507.23300)<br>:star:[code](https://github.com/CIawevy/FreeFine)
  * [KV-Edit Training-Free Image Editing for Precise Background Preservation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_KV-Edit_Training-Free_Image_Editing_for_Precise_Background_Preservation_ICCV_2025_paper.pdf)
  * [Streamlining Image Editing with Layered Diffusion Brushes](http://arxiv.org/abs/2405.00313)
  * [EditCLIP Representation Learning for Image Editing](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_EditCLIP_Representation_Learning_for_Image_Editing_ICCV_2025_paper.pdf)
  * [Training-Free Text-Guided Image Editing with Visual Autoregressive Model](http://arxiv.org/abs/2503.23897)
  * [UIP2P Unsupervised Instruction-based Image Editing via Edit Reversibility Constraint](http://arxiv.org/abs/2412.15216)
  * [FramePainter Endowing Interactive Image Editing with Video Diffusion Priors](http://arxiv.org/abs/2501.08225)<br>:star:[code](https://github.com/YBYBZhang/FramePainter)
  * [Edicho Consistent Image Editing in the Wild](http://arxiv.org/abs/2412.21079)
  * [Instruction-based Image Editing with Planning Reasoning and Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Ji_Instruction-based_Image_Editing_with_Planning_Reasoning_and_Generation_ICCV_2025_paper.pdf)
  * [LOCATEdit Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing](http://arxiv.org/abs/2503.21541)<br>:star:[code](https://github.com/criticalml-uw/LOCATEdit)
  * [Multi-turn Consistent Image Editing](http://arxiv.org/abs/2505.04320)<br>:house:[project](https://zhouzj-dl.github.io/Multi-turn_Consistent_Image_Editing)
  * [DCT-Shield A Robust Frequency Domain Defense against Malicious Image Editing](https://openaccess.thecvf.com/content/ICCV2025/papers/Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing_ICCV_2025_paper.pdf)
  * [RefEdit A Benchmark and Method for Improving Instruction-based Image Editing Model on Referring Expressions](http://arxiv.org/abs/2506.03448)
  * [Describe Dont Dictate Semantic Image Editing with Natural Language Intent](https://openaccess.thecvf.com/content/ICCV2025/papers/Ci_Describe_Dont_Dictate_Semantic_Image_Editing_with_Natural_Language_Intent_ICCV_2025_paper.pdf)
  * [FreeFlux Understanding and Exploiting Layer-Specific Roles in RoPE-Based MMDiT for Versatile Image Editing](http://arxiv.org/abs/2503.16153)
  * [Anchor Token Matching Implicit Structure Locking for Training-free AR Image Editing](http://arxiv.org/abs/2504.10434)
  * [Addressing Text Embedding Leakage in Diffusion-based Image Editing](http://arxiv.org/abs/2412.04715)
  * [EEdit  Rethinking the Spatial and Temporal Redundancy for Efficient Image Editing](http://arxiv.org/abs/2503.10270)<br>:star:[code](https://github.com/yuriYanZeXuan/EEdit)
  * [SuperEdit Rectifying and Facilitating Supervision for Instruction-Based Image Editing](http://arxiv.org/abs/2505.02370)<br>:star:[code](https://github.com/bytedance/SuperEdit)
* 图像渐变
  * [FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model](https://arxiv.org/pdf/2507.01953v1)<br>:star:[code](https://yukangcao.github.io/FreeMorph/)
* 视频生成
  * [AnyI2V: Animating Any Conditional Image with Motion Control](https://arxiv.org/pdf/2507.02857v1)<br>:house:[project](https://henghuiding.com/AnyI2V/)
* 3D布局
  * [LACONIC: A 3D Layout Adapter for Controllable Image Creation](https://arxiv.org/pdf/2507.03257v1)
* 文本-3D
  * [SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with Segmented Consistency Trajectory Distillation](https://arxiv.org/pdf/2507.05256v1)<br>:star:[code](https://zjhjojo.github.io/)
  * [Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation](https://arxiv.org/pdf/2507.09748v1)
  * [LEGO-Maker A Semantic-Driven Algorithm for Text-to-3D Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_LEGO-Maker_A_Semantic-Driven_Algorithm_for_Text-to-3D_Generation_ICCV_2025_paper.pdf)
  * [Benchmarking and Learning Multi-Dimensional Quality Evaluator for Text-to-3D Generation](http://arxiv.org/abs/2412.11170)
  * [VideoRFSplat Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling](http://arxiv.org/abs/2503.15855)<br>:house:[project](https://gohyojun15.github.io/VideoRFSplat/) :house:[project](https://gohyojun15.github.io/VideoRFSplat)
* 视频-4D
  * [Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis](https://arxiv.org/pdf/2507.23785v1)<br>:star:[code](https://gvfdiffusion.github.io/)<br>:star:[code](https://github.com/foreverfancy/gvfdiffusion)
  * [Not All Frame Features Are Equal Video-to-4D Generation via Decoupling Dynamic-Static Features](http://arxiv.org/abs/2502.08377)<br>:star:[code](https://github.com/LiyingCV/DS4D) :house:[project](https://github.com/LiyingCV/DS4D)
  * [SV4D 20 Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Yao_SV4D_2.0_Enhancing_Spatio-Temporal_Consistency_in_Multi-View_Video_Diffusion_for_ICCV_2025_paper.pdf)
* 故事生成
  * [Lay2Story: Extending Diffusion Transformers for Layout-Togglable Story Generation](https://arxiv.org/pdf/2508.08949v1)
* 图像拼接
  * [PixelStitch Structure-Preserving Pixel-Wise Bidirectional Warps for Unsupervised Image Stitching](https://openaccess.thecvf.com/content/ICCV2025/papers/Jin_PixelStitch_Structure-Preserving_Pixel-Wise_Bidirectional_Warps_for_Unsupervised_Image_Stitching_ICCV_2025_paper.pdf)
  * [Leveraging Local Patch Alignment to Seam-cutting for Large Parallax Image Stitching](http://arxiv.org/abs/2311.18564)<br>:star:[code](https://github.com/tlliao/LPAM_seam-cutting)
* 3D生成
  * [MS3D High-Quality 3D Generation via Multi-Scale Representation Modeling](https://openaccess.thecvf.com/content/ICCV2025/papers/Luo_MS3D_High-Quality_3D_Generation_via_Multi-Scale_Representation_Modeling_ICCV_2025_paper.pdf)
  * [DSO Aligning 3D Generators with Simulation Feedback for Physical Soundness](http://arxiv.org/abs/2503.22677)
  * [From One to More Contextual Part Latents for 3D Generation](http://arxiv.org/abs/2507.08772)<br>:house:[project](https://hkdsc.github.io/project) :house:[project](https://hkdsc.github.io/project/copart)
  * [Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation](http://arxiv.org/abs/2503.15877)
  * [SViM3D Stable Video Material Diffusion for Single Image 3D Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Engelhardt_SViM3D_Stable_Video_Material_Diffusion_for_Single_Image_3D_Generation_ICCV_2025_paper.pdf)
* 布局生成图像
  * [CreatiLayout Siamese Multimodal Diffusion Transformer for Creative Layout-to-Image Generation](http://arxiv.org/abs/2412.03859)
  * [MUSE Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion](http://arxiv.org/abs/2508.14440)<br>:star:[code](https://github.com/pf0607/MUSE)
  * [SEGA A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with_ICCV_2025_paper.pdf)<br>:house:[project](https://brucew91.github.io/SEGA.github.io)
  * [Lay-Your-Scene Natural Scene Layout Generation with Diffusion Transformers](https://openaccess.thecvf.com/content/ICCV2025/papers/Srivastava_Lay-Your-Scene_Natural_Scene_Layout_Generation_with_Diffusion_Transformers_ICCV_2025_paper.pdf)
  * [REPARO Compositional 3D Assets Generation with Differentiable 3D Layout Alignment](http://arxiv.org/abs/2405.18525)
* 网格生成
  * [Nautilus Locality-aware Autoencoder for Scalable Mesh Generation](http://arxiv.org/abs/2501.14317)
  * [MeshAnything V2 Artist-Created Mesh Generation with Adjacent Mesh Tokenization](http://arxiv.org/abs/2408.02555)
  * [MeshPad Interactive Sketch-Conditioned Artist-Reminiscent Mesh Generation and Editing](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_MeshPad_Interactive_Sketch-Conditioned_Artist-Reminiscent_Mesh_Generation_and_Editing_ICCV_2025_paper.pdf)
* 草图生成
  * [VQ-SGen A Vector Quantized Stroke Representation for Creative Sketch Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_VQ-SGen_A_Vector_Quantized_Stroke_Representation_for_Creative_Sketch_Generation_ICCV_2025_paper.pdf)
  * [Stroke2Sketch Harnessing Stroke Attributes for Training-Free Sketch Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_Stroke2Sketch_Harnessing_Stroke_Attributes_for_Training-Free_Sketch_Generation_ICCV_2025_paper.pdf)
  * [PASTA Part-Aware Sketch-to-3D Shape Generation with Text-Aligned Prior](http://arxiv.org/abs/2503.12834)
* 图像翻译
  * [CycleVAR Repurposing Autoregressive Model for Unsupervised One-Step Image Translation](http://arxiv.org/abs/2506.23347)
  * [Frequency-Guided Diffusion for Training-Free Text-Driven Image Translation](https://openaccess.thecvf.com/content/ICCV2025/papers/Gao_Frequency-Guided_Diffusion_for_Training-Free_Text-Driven_Image_Translation_ICCV_2025_paper.pdf)

<a name="4"/>

## 4.Image Captioning(图像字幕)
* [CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning](https://arxiv.org/pdf/2507.01409v1)<br>:star:[code](https://github.com/omron-sinicx/captionsmiths)
* [SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning](https://arxiv.org/pdf/2508.06125v1)
* [OmniDiff A Comprehensive Benchmark for Fine-grained Image Difference Captioning](http://arxiv.org/abs/2503.11093)
* [Embodied Image Captioning Self-supervised Learning Agents for Spatially Coherent Image Descriptions](http://arxiv.org/abs/2504.08531)<br>:house:[project](https://hsp-iit.github.io/embodied-captioning)
* [Engage for All Making Ordinary Image Descriptions Appealing Again](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Engage_for_All_Making_Ordinary_Image_Descriptions_Appealing_Again_ICCV_2025_paper.pdf)
* 图表字幕
  * [ChartCap: Mitigating Hallucination of Dense Chart Captioning](https://arxiv.org/pdf/2508.03164v1)
* 视频字幕
  * [Player-Centric Multimodal Prompt Generation for Large Language Model Based Identity-Aware Basketball Video Captioning](https://arxiv.org/pdf/2507.20163v1)<br>:star:[code](https://github.com/Zeyu1226-mt/LLM-IAVC)
  * [Describe Anything Detailed Localized Image and Video Captioning](http://arxiv.org/abs/2504.16072)
  * [SweetTok Semantic-Aware Spatial-Temporal Tokenizer for Compact Video Discretization](http://arxiv.org/abs/2412.10443)
  * [Large-scale Pre-training for Grounded Video Caption Generation](http://arxiv.org/abs/2503.10781)<br>:house:[project](https://ekazakos.github.io/grounded_video_caption_generation)


<a name="3"/>

## 3.Super-Resolution(超分辨率)
* 图像超分辨率
  * [LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning](https://arxiv.org/pdf/2506.22710v1)<br>:star:[code](https://github.com/MJ-NCEPU/LightBSR)
  * [Bridging Diffusion Models and 3D Representations: A 3D Consistent Super-Resolution Framework](https://arxiv.org/pdf/2508.04090v1)
  * [IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution](https://arxiv.org/pdf/2507.09923v1)
  * [Fine-structure Preserved Real-world Image Super-resolution via Transfer VAE Training](https://arxiv.org/pdf/2507.20291v1)<br>:star:[code](https://github.com/Joyies/TVT)
  * [ZFusion Efficient Deep Compositional Zero-shot Learning for Blind Image Super-Resolution with Generative Diffusion Prior](https://openaccess.thecvf.com/content/ICCV2025/papers/Esmaeilzehi_ZFusion_Efficient_Deep_Compositional_Zero-shot_Learning_for_Blind_Image_Super-Resolution_ICCV_2025_paper.pdf)
  * [StyleSRN Scene Text Image Super-Resolution with Text Style Embedding](https://openaccess.thecvf.com/content/ICCV2025/papers/Yuan_StyleSRN_Scene_Text_Image_Super-Resolution_with_Text_Style_Embedding_ICCV_2025_paper.pdf)
  * [Fast Image Super-Resolution via Consistency Rectified Flow](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_Fast_Image_Super-Resolution_via_Consistency_Rectified_Flow_ICCV_2025_paper.pdf)
  * [Outlier-Aware Post-Training Quantization for Image Super-Resolution](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution_ICCV_2025_paper.pdf)
  * [Benchmarking Burst Super-Resolution for Polarization Images Noise Dataset and Analysis](http://arxiv.org/abs/2503.18705)<br>:star:[code](https://github.com/KAIST-VCLAB/polarns)
  * [Hipandas Hyperspectral Image Joint Denoising and Super-Resolution by Image Fusion with the Panchromatic Image](http://arxiv.org/abs/2412.04201)<br>:star:[code](https://github.com/shuangxu96/Hipandas)
  * [Not All Degradations Are Equal A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution](http://arxiv.org/abs/2509.14841)
  * [Emulating Self-attention with Convolution for Efficient Image Super-Resolution](http://arxiv.org/abs/2503.06671)
  * [Rethinking the Upsampling Process in Light Field Super-Resolution with Spatial-Epipolar Implicit Image Function](https://openaccess.thecvf.com/content/ICCV2025/papers/Cong_Rethinking_the_Upsampling_Process_in_Light_Field_Super-Resolution_with_Spatial-Epipolar_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Congrx/SEIIF)
  * [DuCos Duality Constrained Depth Super-Resolution via Foundation Model](http://arxiv.org/abs/2503.04171)<br>:star:[code](https://github.com/yanzq95/DuCos)
  * [Generalized and Efficient 2D Gaussian Splatting for Arbitrary-scale Super-Resolution](http://arxiv.org/abs/2501.06838)<br>:star:[code](https://github.com/ChrisDud0257/GSASR)
  * [Reference-based Super-Resolution via Image-based Retrieval-Augmented Generation Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_Reference-based_Super-Resolution_via_Image-based_Retrieval-Augmented_Generation_Diffusion_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ByeonghunLee12/iRAG)
  * [Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution](https://openaccess.thecvf.com/content/ICCV2025/papers/Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution_ICCV_2025_paper.pdf)
  * [PatchScaler An Efficient Patch-Independent Diffusion Model for Image Super-Resolution](http://arxiv.org/abs/2405.17158)<br>:star:[code](https://github.com/yongliuy/PatchScaler) :house:[project](https://github.com/yongliuy/PatchScaler)
  * [DiT4SR Taming Diffusion Transformer for Real-World Image Super-Resolution](http://arxiv.org/abs/2503.23580)
  * [NeurOp-Diff Continuous Remote Sensing Image Super-Resolution via Neural Operator Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_NeurOp-Diff_Continuous_Remote_Sensing_Image_Super-Resolution_via_Neural_Operator_Diffusion_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/zerono000/NeurOp-Diff)
  * [Consistency Trajectory Matching for One-Step Generative Super-Resolution](http://arxiv.org/abs/2503.20349)
  * [Adversarial Purification via Super-Resolution and Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Park_Adversarial_Purification_via_Super-Resolution_and_Diffusion_ICCV_2025_paper.pdf)
  * [Perceive Understand and Restore Real-World Image Super-Resolution with Autoregressive Multimodal Generative Models](http://arxiv.org/abs/2503.11073)<br>:star:[code](https://github.com/nonwhy/PURE)
* 视频超分辨率
  * [VSRM: A Robust Mamba-Based Framework for Video Super-Resolution](https://arxiv.org/pdf/2506.22762v1)
  * [TurboVSR: Fantastic Video Upscalers and Where to Find Them](https://arxiv.org/pdf/2506.23618v1)
  * [MedVSR Medical Video Super-Resolution with Cross State-Space Propagation](http://arxiv.org/abs/2509.21265)<br>:star:[code](https://github.com/CUHK-AIM-Group/MedVSR)
  * [Blind Video Super-Resolution based on Implicit Kernels](http://arxiv.org/abs/2503.07856)<br>:star:[code](https://github.com/QZ1-boy/BVSR-IK)
  * [DiffVSR Revealing an Effective Recipe for Taming Robust Video Super-Resolution Against Complex Degradations](http://arxiv.org/abs/2501.10110)
  * [LDIP Long Distance Information Propagation for Video Super-Resolution](https://openaccess.thecvf.com/content/ICCV2025/papers/Bernasconi_LDIP_Long_Distance_Information_Propagation_for_Video_Super-Resolution_ICCV_2025_paper.pdf)
  * [STAR Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution](http://arxiv.org/abs/2501.02976)

<a name="2"/>

## 2.Image Progress(图像/视频处理)
* [LightsOut Diffusion-based Outpainting for Enhanced Lens Flare Removal](https://openaccess.thecvf.com/content/ICCV2025/papers/Tsai_LightsOut_Diffusion-based_Outpainting_for_Enhanced_Lens_Flare_Removal_ICCV_2025_paper.pdf)
* [Structure-Guided Diffusion Models for High-Fidelity Portrait Shadow Removal](http://arxiv.org/abs/2507.04692)<br>:star:[code](https://github.com/wanchang-yu/Structure-Guided-Diffusion-for-Portrait-Shadow-Removal)
* 去雨
  * [PRE-Mamba A 4D State Space Model for Ultra-High-Frequent Event Camera Deraining](https://openaccess.thecvf.com/content/ICCV2025/papers/Ruan_PRE-Mamba_A_4D_State_Space_Model_for_Ultra-High-Frequent_Event_Camera_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/softword-tt/PRE-Mamba)
* 去噪
  * [Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention](https://arxiv.org/pdf/2506.23542v1)<br>:star:[code](https://github.com/davidweidawang/GIGA-ToF)
  * [Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment](https://arxiv.org/pdf/2508.06160v1)<br>:star:[code](https://github.com/GATECH-EIC/PostDiff)
  * [Robust Test-Time Adaptation for Single Image Denoising Using Deep Gaussian Prior](https://openaccess.thecvf.com/content/ICCV2025/papers/Ma_Robust_Test-Time_Adaptation_for_Single_Image_Denoising_Using_Deep_Gaussian_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/qingma2016/TTAD)
  * [Autoregressive Denoising Score Matching is a Good Video Anomaly Detector](http://arxiv.org/abs/2506.23282)
  * [Blind2Sound Self-Supervised Image Denoising without Residual Noise](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Blind2Sound_Self-Supervised_Image_Denoising_without_Residual_Noise_ICCV_2025_paper.pdf)
  * [Self-Calibrated Variance-Stabilizing Transformations for Real-World Image Denoising](http://arxiv.org/abs/2407.17399)
  * [Denoising Token Prediction in Masked Autoregressive Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Yao_Denoising_Token_Prediction_in_Masked_Autoregressive_Models_ICCV_2025_paper.pdf)
  * [IDF Iterative Dynamic Filtering Networks for Generalizable Image Denoising](http://arxiv.org/abs/2508.19649)
  * [Generic Event Boundary Detection via Denoising Diffusion](http://arxiv.org/abs/2508.12084)
  * [Fewer Denoising Steps or Cheaper Per-Step Inference Towards Compute-Optimal Diffusion Model Deployment](http://arxiv.org/abs/2508.06160)<br>:star:[code](https://github.com/GATECH-EIC/PostDiff)
* 去模糊
  * [Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model](https://arxiv.org/pdf/2507.13599v1)
  * [Efficient Concertormer for Image Deblurring and Beyond](http://arxiv.org/abs/2404.06135)
  * [Performing Defocus Deblurring by Modeling its Formation Process](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Performing_Defocus_Deblurring_by_Modeling_its_Formation_Process_ICCV_2025_paper.pdf)
  * [Blind Noisy Image Deblurring Using Residual Guidance Strategy](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Blind_Noisy_Image_Deblurring_Using_Residual_Guidance_Strategy_ICCV_2025_paper.pdf)
* 图像去雾
  * [Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing](https://arxiv.org/pdf/2507.01275v1)
  * [When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training](https://arxiv.org/pdf/2507.09524v1)<br>:star:[code](https://github.com/ywxjm/DehazeSB)
  * [PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing](https://arxiv.org/pdf/2507.14826v1)
  * [GenHaze Pioneering Controllable One-Step Realistic Haze Generation for Real-World Dehazing](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_GenHaze_Pioneering_Controllable_One-Step_Realistic_Haze_Generation_for_Real-World_Dehazing_ICCV_2025_paper.pdf)
  * [HazeFlow Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](http://arxiv.org/abs/2509.18190)
* 修补
  * [DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting](https://arxiv.org/pdf/2507.00429v1)<br>:star:[code](https://rorisis.github.io/DiGA3D/)
  * [RI3D Few-Shot Gaussian Splatting With Repair and Inpainting Diffusion Priors](http://arxiv.org/abs/2503.10860)
  * [SAGI Semantically Aligned and Uncertainty Guided AI Image Inpainting](http://arxiv.org/abs/2502.06593)<br>:house:[project](https://mever-team.github.io/SAGI)
  * [OmniPaint Mastering Object-Oriented Editing via Disentangled Insertion-Removal Inpainting](http://arxiv.org/abs/2503.08677)
  * [Inpaint4Drag Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping](http://arxiv.org/abs/2509.04582)<br>:house:[project](https://visual-ai.github.io/inpaint4drag)
  * [Trans-Adapter A Plug-and-Play Framework for Transparent Image Inpainting](https://openaccess.thecvf.com/content/ICCV2025/papers/Dai_Trans-Adapter_A_Plug-and-Play_Framework_for_Transparent_Image_Inpainting_ICCV_2025_paper.pdf)
  * [Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency](http://arxiv.org/abs/2510.10993)<br>:house:[project](https://pa-inpainter.github.io/)
  * [Ultra High-Resolution Image Inpainting with Patch-Based Content Consistency Adapter](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Ultra_High-Resolution_Image_Inpainting_with_Patch-Based_Content_Consistency_Adapter_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Roveer/Patch-Based-Adapter)
* 图像恢复
  * [EAMamba: Efficient All-Around Vision State Space Model for Image Restoration](http://arxiv.org/pdf/2506.22246v1)
  * [Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints](https://arxiv.org/pdf/2507.11985v1)<br>:star:[code](https://github.com/Jiahao-UTS/MPAE)
  * [Robust Adverse Weather Removal via Spectral-based Spatial Grouping](https://arxiv.org/pdf/2507.22498v1)
  * [Exploiting Diffusion Prior for Task-driven Image Restoration](https://arxiv.org/pdf/2507.22459v1)
  * [Reverse Convolution and Its Applications to Image Restoration](https://arxiv.org/pdf/2508.09824v1)<br>:star:[code](https://github.com/cszn/ConverseNet)
  * [Enhancing Image Restoration Transformer via Adaptive Translation Equivariance](http://arxiv.org/abs/2506.18520)
  * [MP-HSIR A Multi-Prompt Framework for Universal Hyperspectral Image Restoration](https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_MP-HSIR_A_Multi-Prompt_Framework_for_Universal_Hyperspectral_Image_Restoration_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ZhehuiWu/MP-HSIR)
  * [FoundIR Unleashing Million-scale Training Data to Advance Foundation Models for Image Restoration](http://arxiv.org/abs/2412.01427)
  * [MOERL When Mixture-of-Experts Meet Reinforcement Learning for Adverse Weather Image Restoration](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_MOERL_When_Mixture-of-Experts_Meet_Reinforcement_Learning_for_Adverse_Weather_Image_ICCV_2025_paper.pdf)
  * [Conditional Visual Autoregressive Modeling for Pathological Image Restoration](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Conditional_Visual_Autoregressive_Modeling_for_Pathological_Image_Restoration_ICCV_2025_paper.pdf)
  * [Dual-level Prototype Learning for Composite Degraded Image Restoration](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Dual-level_Prototype_Learning_for_Composite_Degraded_Image_Restoration_ICCV_2025_paper.pdf)
  * [UniRes Universal Image Restoration for Complex Degradations](http://arxiv.org/abs/2506.05599)
  * [Devil is in the Uniformity Exploring Diverse Learners within Transformer for Image Restoration](http://arxiv.org/abs/2503.20174)
  * [A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions](http://arxiv.org/abs/2412.17377)<br>:house:[project](https://physicalmotionrestoration.github.io/)
  * [MIORe  VAR-MIORe Benchmarks to Push the Boundaries of Restoration](https://openaccess.thecvf.com/content/ICCV2025/papers/Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration_ICCV_2025_paper.pdf)
  * [Decouple to Reconstruct High Quality UHD Restoration via Active Feature Disentanglement and Reversible Fusion](http://arxiv.org/abs/2503.12764)
  * [Robust Low-light Scene Restoration via Illumination Transition](http://arxiv.org/abs/2507.03976)<br>:house:[project](https://pegasus2004.github.io/RoSe)
  * [Noise-Modeled Diffusion Models for Low-Light Spike Image Restoration](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/BIT-Vision/SpikeDiffusion)
  * [LD-RPS Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_LD-RPS_Zero-Shot_Unified_Image_Restoration_via_Latent_Diffusion_Recurrent_Posterior_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/AMAP-ML/LD-RPS)
  * [Frequency-Guided Posterior Sampling for Diffusion-Based Image Restoration](http://arxiv.org/abs/2411.15295)
* 图像/视频增强
  * [MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices](https://arxiv.org/pdf/2507.01838v1)<br>:star:[code](https://github.com/AVC2-UESTC/MobileIE.git)
  * [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/pdf/2507.10689v1)<br>:star:[code](https://github.com/bywlzts/CWNet-Causal-Wavelet-Network)
  * [Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement](https://arxiv.org/pdf/2507.12135v1)
  * [GT-Mean Loss: A Simple Yet Effective Solution for Brightness Mismatch in Low-Light Image Enhancement](https://arxiv.org/pdf/2507.20148v1)<br>:star:[code](https://github.com/jingxiLiao/GT-mean-loss)
  * [Learnable Feature Patches and Vectors for Boosting Low-light Image Enhancement without External Knowledge](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_Learnable_Feature_Patches_and_Vectors_for_Boosting_Low-light_Image_Enhancement_ICCV_2025_paper.pdf)
  * [Uncover Treasures in DCT Advancing JPEG Quality Enhancement by Exploiting Latent Correlations](http://arxiv.org/abs/2506.21171)
  * [Lightweight and Fast Real-time Image Enhancement via Decomposition of the Spatial-aware Lookup Tables](http://arxiv.org/abs/2508.16121)<br>:star:[code](https://github.com/WontaeaeKim/SVDLUT)
  * [GM-MoE Low-Light Enhancement with Gated-Mechanism Mixture-of-Experts](https://openaccess.thecvf.com/content/ICCV2025/papers/Liao_GM-MoE_Low-Light_Enhancement_with_Gated-Mechanism_Mixture-of-Experts_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Sameenok/gm-moe-lowlight-enhancement.git)
  * [Exploring View Consistency for Scene-Adaptive Low-Light Light Field Image Enhancement](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement_ICCV_2025_paper.pdf)
  * [Low-Light Image Enhancement Using Event-Based Illumination Estimation](http://arxiv.org/abs/2504.09379)<br>:star:[code](https://github.com/AHupuJR/RetinEV)
  * [Task-Decoupled Bezier Surface Constraint for Uneven Low-Light Image Enhancement](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_Task-Decoupled_Bezier_Surface_Constraint_for_Uneven_Low-Light_Image_Enhancement_ICCV_2025_paper.pdf)
  * [PASD A Pixel-Adaptive Swarm Dynamics Approach for Unsupervised Low-Light Image Enhancement](https://openaccess.thecvf.com/content/ICCV2025/papers/Jin_PASD_A_Pixel-Adaptive_Swarm_Dynamics_Approach_for_Unsupervised_Low-Light_Image_ICCV_2025_paper.pdf)
  * [From Enhancement to Understanding Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning](http://arxiv.org/abs/2507.08380)
  * [RetinexMCNet A Memory Controller Dominated Network for Low-Light Video Enhancement Based on Retinex](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_RetinexMCNet_A_Memory_Controller_Dominated_Network_for_Low-Light_Video_Enhancement_ICCV_2025_paper.pdf)
  * [Aligning Global Semantics and Local Textures in Generative Video Enhancement](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Aligning_Global_Semantics_and_Local_Textures_in_Generative_Video_Enhancement_ICCV_2025_paper.pdf)
* 视频调色
  * [Video Color Grading via Look-Up Table Generation](https://arxiv.org/pdf/2508.00548v1)<br>:star:[code](https://github.com/seunghyuns98/VideoColorGrading)
* 视频去模糊
  * [Event-guided Unified Framework for Low-light Video Enhancement Frame Interpolation and Deblurring](https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_Event-guided_Unified_Framework_for_Low-light_Video_Enhancement_Frame_Interpolation_and_ICCV_2025_paper.pdf)
  * [Separation for Better Integration Disentangling Edge and Motion in Event-based Deblurring](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_Separation_for_Better_Integration_Disentangling_Edge_and_Motion_in_Event-based_ICCV_2025_paper.pdf)
  * [EVDM Event-based Real-world Video Deblurring with Mamba](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_EVDM_Event-based_Real-world_Video_Deblurring_with_Mamba_ICCV_2025_paper.pdf)
  * [ClearSight Human Vision-Inspired Solutions for Event-Based Motion Deblurring](http://arxiv.org/abs/2501.15808)
* 质量评估
  * [MVQA Mamba with Unified Sampling for Efficient Video Quality Assessment](http://arxiv.org/abs/2504.16003)<br>:star:[code](https://github.com/xiao-mi-d/MVQA)
  * [Few-Shot Image Quality Assessment via Adaptation of Vision-Language Models](http://arxiv.org/abs/2409.05381)<br>:star:[code](https://github.com/LXDxmu/GRMP-IQA)
  * [IQA-Adapter Exploring Knowledge Transfer from Image Quality Assessment to Diffusion-based Generative Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based_ICCV_2025_paper.pdf)
* 视频修补
  * [BVINet Unlocking Blind Video Inpainting with Zero Annotations](http://arxiv.org/abs/2502.01181)
  * [Vivid4D Improving 4D Reconstruction from Monocular Video by Video Inpainting](http://arxiv.org/abs/2504.11092)
* 着色
  * [DACoN DINO for Anime Paint Bucket Colorization with Any Number of Reference Images](http://arxiv.org/abs/2509.14685)<br>:star:[code](https://github.com/kzmngt/DACoN)
  * [LGA-Net Learning Local and Global Affinities for Sparse Scribble based Image Colorization](https://openaccess.thecvf.com/content/ICCV2025/papers/Lyu_LGA-Net_Learning_Local_and_Global_Affinities_for_Sparse_Scribble_based_ICCV_2025_paper.pdf)
  * [MagicColor Multi-Instance Sketch Colorization](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_MagicColor_Multi-Instance_Sketch_Colorization_ICCV_2025_paper.pdf)
* 外展
  * [Progressive Artwork Outpainting via Latent Diffusion Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Song_Progressive_Artwork_Outpainting_via_Latent_Diffusion_Models_ICCV_2025_paper.pdf)
* 图像矫正
  * [Lifting the Structural Morphing for Wide-Angle Images Rectification Unified Content and Boundary Modeling](https://openaccess.thecvf.com/content/ICCV2025/papers/Luan_Lifting_the_Structural_Morphing_for_Wide-Angle_Images_Rectification_Unified_Content_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/lwttttt/ConBo-Net)

<a name="1"/>

## 1.Other
* [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/pdf/2507.11129v1)<br>:star:[code](https://github.com/Neal2020GitHub/MMOne)
* [PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling](http://arxiv.org/pdf/2506.20936v1)
* [EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception](http://arxiv.org/pdf/2506.21080v1)
* [Learning to See in the Extremely Dark](http://arxiv.org/pdf/2506.21132v1)<br>:star:[code](https://github.com/JianghaiSCU/SIED)
* [Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation](http://arxiv.org/pdf/2506.21198v1)<br>:star:[code](https://github.com/yihong-97/UNLOCK)
* [CA-I2P: Channel-Adaptive Registration Network with Global Optimal Selection](http://arxiv.org/pdf/2506.21364v1)
* [Global and Local Entailment Learning for Natural World Imagery](http://arxiv.org/pdf/2506.21476v1)<br>:star:[code](https://vishu26.github.io/RCME/index.html)
* [Attention to Burstiness: Low-Rank Bilinear Prompt Tuning](https://arxiv.org/pdf/2506.22908v1)
* [Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding](https://arxiv.org/pdf/2506.22803v1)<br>:star:[code](https://github.com/XiGuaBo/CBM-HNMU)
* [CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models](https://arxiv.org/pdf/2507.13984v1)
* [Learning Counterfactually Decoupled Attention for Open-World Model Attribution](https://arxiv.org/pdf/2506.23074v1)<br>:star:[code](https://github.com/yzheng97/CDAL)
* [Where, What, Why: Towards Explainable Driver Attention Prediction](https://arxiv.org/pdf/2506.23088v1)
* [VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions](https://arxiv.org/pdf/2506.23236v1)<br>:star:[code](https://markomih.github.io/VolumetricSMPL)
* [AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm](https://arxiv.org/pdf/2506.23537v1)<br>:star:[code](https://github.com/eezkni/AFUNet)
* [HiNeuS High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity](http://arxiv.org/abs/2506.23854)<br>:house:[project](https://wangyida.github.io/posts)
* [Rectifying Magnitude Neglect in Linear Attention](https://arxiv.org/pdf/2507.00698v1)<br>:star:[code](https://github.com/qhfan/MALA)
* [Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes](https://arxiv.org/pdf/2507.00327v1)<br>:star:[code](https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA)
* [Zero-shot Inexact CAD Model Alignment from a Single Image](https://arxiv.org/pdf/2507.03292v1)<br>:star:[code](https://zerocad9d.github.io/)
* [MGSfM: Multi-Camera Geometry Driven Global Structure-from-Motion](https://arxiv.org/pdf/2507.03306v1)<br>:star:[code](https://github.com/3dv-casia/MGSfM/)
* [Learning Normals of Noisy Points by Local Gradient-Aware Surface Filtering](https://arxiv.org/pdf/2507.03394v1)<br>:star:[code](https://github.com/LeoQLi/LGSF)
* [Pose-Star: Anatomy-Aware Editing for Open-World Fashion Images](https://arxiv.org/pdf/2507.03402v1)
* [Unlearning the Noisy Correspondence Makes CLIP More Robust](http://arxiv.org/abs/2507.03434)<br>:star:[code](https://github.com/hhc1997/NCU)
* [Less is More: Empowering GUI Agent with Context-Aware Simplification](https://arxiv.org/pdf/2507.03730v1)
* [Voyaging into Unbounded Dynamic Scenes from a Single View](https://arxiv.org/pdf/2507.04183v1)<br>:star:[code](https://tianfr.github.io/DynamicVoyager)
* [TeethGenerator: A two-stage framework for paired pre- and post-orthodontic 3D dental data generation](https://arxiv.org/pdf/2507.04685v1)<br>:star:[code](https://github.com/lcshhh/teeth_generator)
* [Beyond One Shot, Beyond One Perspective: Cross-View and Long-Horizon Distillation for Better LiDAR Representations](https://arxiv.org/pdf/2507.05260v1)<br>:star:[code](http://github.com/Xiangxu-0103/LiMA)
* [IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization](https://arxiv.org/pdf/2507.06856v1)
* [Synchronizing Task Behavior: Aligning Multiple Tasks during Test-Time Training](https://arxiv.org/pdf/2507.07778v1)
* [From Enhancement to Understanding: Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning](https://arxiv.org/pdf/2507.08380v1)
* [ConsNoTrainLoRA: Data-driven Weight Initialization of Low-rank Adapters using Constraints](https://arxiv.org/pdf/2507.08044v1)
* [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/pdf/2507.09305v1)
* [Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves](https://arxiv.org/pdf/2507.09207v1)
* [GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space](https://arxiv.org/pdf/2507.10473v1)
* [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/pdf/2507.09291v1)
* [Imbalance in Balance: Online Concept Balancing in Generation Models](https://arxiv.org/pdf/2507.13345v1)
* [MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP](https://arxiv.org/pdf/2507.15257v1)<br>:star:[code](https://github.com/anpei96/mincd-pnp-demo)
* [DAViD: Data-efficient and Accurate Vision Models from Synthetic Data](https://arxiv.org/pdf/2507.15365v1)<br>:house:[project](https://aka.ms/DAViD)
* [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/pdf/2507.14505v1)<br>:star:[code](https://jiahao-ma.github.io/DCHM/)
* [Open-set Cross Modal Generalization via Multimodal Unified Representation](https://arxiv.org/pdf/2507.14935v1)<br>:star:[code](https://github.com/haihuangcode/CMG)
* [FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers](https://arxiv.org/pdf/2507.15249v1)<br>:star:[code](https://github.com/Monalissaa/FreeCus)
* [M-SpecGene: Generalized Foundation Model for RGBT Multispectral Vision](https://arxiv.org/pdf/2507.16318v1)<br>:star:[code](https://github.com/CalayZhou/M-SpecGene)
* [Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility](https://arxiv.org/pdf/2507.17748v1)
* [Joint Asymmetric Loss for Learning with Noisy Labels](https://arxiv.org/pdf/2507.17692v1)<br>:star:[code](https://github.com/cswjl/joint-asymmetric-loss)
* [AnimalClue: Recognizing Animals by their Traces](https://arxiv.org/pdf/2507.20240v1)<br>:star:[code](https://dahlian00.github.io/AnimalCluePage/)
* [Rethinking Few Shot CLIP Benchmarks: A Critical Analysis in the Inductive Setting](https://arxiv.org/pdf/2507.20834v1)
* [Controllable Feature Whitening for Hyperparameter-Free Bias Mitigation](https://arxiv.org/pdf/2507.20284v1)
* [Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry](https://arxiv.org/pdf/2507.20757v1)
* [Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting](https://arxiv.org/pdf/2507.18678v1)
* [CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception](https://arxiv.org/pdf/2507.19239v1)<br>:star:[code](https://github.com/zhongjiaru/CoopTrack)
* [TR-PTS: Task-Relevant Parameter and Token Selection for Efficient Tuning](https://arxiv.org/pdf/2507.22872v1)<br>:star:[code](https://github.com/synbol/TR-PTS)
* [ShortFT: Diffusion Model Alignment via Shortcut-based Fine-Tuning](https://arxiv.org/pdf/2507.22604v1)
* [DiffuMatch: Category-Agnostic Spectral Diffusion Priors for Robust Non-rigid Shape Matching](https://arxiv.org/pdf/2507.23715v1)<br>:star:[code](https://github.com/daidedou/diffumatch/)
* [SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions](https://arxiv.org/pdf/2507.23784v1)<br>:star:[code](https://github.com/ExplainableML/sub)<br>:house:[project](http://huggingface.co/datasets/Jessica-bader/SUB)
* [Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product](https://arxiv.org/pdf/2508.00230v1)
* [DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space](https://arxiv.org/pdf/2508.00413v1)<br>:star:[code](https://github.com/dc-ai-projects/DC-Gen)
* [CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective](https://arxiv.org/pdf/2508.00359v1)
* [GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration](https://arxiv.org/pdf/2508.00152v1)<br>:star:[code](https://limirs.github.io/GeoExplorer/)<br>:star:[code](https://github.com/limirs/GeoExplorer)
* [Where am I Cross-View Geo-localization with Natural Language Descriptions](http://arxiv.org/abs/2412.17007)<br>:star:[code](https://github.com/yejy53/CVG-Text)
* [SCFlow: Implicitly Learning Style and Content Disentanglement with Flow Models](https://arxiv.org/pdf/2508.03402v1)<br>:star:[code](https://compvis.github.io/SCFlow/)<br>:star:[code](https://github.com/compvis/scflow)
* [How Would It Sound? Material-Controlled Multimodal Acoustic Profile Generation for Indoor Scenes](https://arxiv.org/pdf/2508.02905v1)<br>:star:[code](https://mahnoor-fatima-saad.github.io/m-capa.html)<br>:star:[code](https://github.com/mahnoor-fatima-saad/m-capa)
* [Symmetry Understanding of 3D Shapes via Chirality Disentanglement](https://arxiv.org/pdf/2508.05505v1)<br>:star:[code](https://wei-kang-wang.github.io/chirality/)<br>:star:[code](https://github.com/wei-kang-wang/chirality)
* [kh Symmetry Understanding of 3D Shapes via Chirality Disentanglement](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_kh_Symmetry_Understanding_of_3D_Shapes_via_Chirality_Disentanglement_ICCV_2025_paper.pdf)<br>:house:[project](https://wei-kang-wang.github.io/chirality) :house:[project](https://wei-kang-wang.github.io/chirality/)
* [WIR3D Visually-Informed and Geometry-Aware 3D Shape Abstraction](http://arxiv.org/abs/2505.04813)
* [Learning an Implicit Physics Model for Image-based Fluid Simulation](https://arxiv.org/pdf/2508.08254v1)<br>:star:[code](https://physfluid.github.io/)
* [The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility](https://arxiv.org/pdf/2508.07989v1)
* [Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images](https://arxiv.org/pdf/2508.07847v1)<br>:star:[code](https://keio-smilab25.github.io/DeepSWM)<br>:star:[code](https://github.com/keio-smilab25/deepswm)
* [Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)](https://arxiv.org/pdf/2508.07775v1)<br>:star:[code](https://github.com/bastianlb/forecasting-rotational-dynamics)
* [CObL: Toward Zero-Shot Ordinal Layering without User Prompting](https://arxiv.org/pdf/2508.08498v1)<br>:house:[project](https://vision.seas.harvard.edu/cobl/)
* [UniConvNet: Expanding Effective Receptive Field while Maintaining Asymptotically Gaussian Distribution for ConvNets of Any Scale](https://arxiv.org/pdf/2508.09000v1)<br>:star:[code](https://github.com/ai-paperwithcode/UniConvNet)
* [TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos](https://arxiv.org/pdf/2508.09811v1)<br>:star:[code](https://github.com/vLAR-group/TRACE)
* [Combinative Matching for Geometric Shape Assembly](https://arxiv.org/pdf/2508.09780v1)<br>:star:[code](https://nahyuklee.github.io/cmnet)
* [Harnessing Input-Adaptive Inference for Efficient VLN](https://arxiv.org/pdf/2508.09262v1)<br>:star:[code](https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation)
* [Less-to-More Generalization: Unlocking More Controllability by In-Context Generation](http://arxiv.org/abs/2504.02160)<br>:star:[code](https://github.com/bytedance/UNO.)
* [Towards a Unified Copernicus Foundation Model for Earth Vision](http://arxiv.org/abs/2503.11849)<br>:star:[code](https://github.com/zhu-xlab/Copernicus-FM)
* [UnZipLoRA Separating Content and Style from a Single Image](http://arxiv.org/abs/2412.04465)
* [FlowDPS  Flow-Driven Posterior Sampling for Inverse Problems](http://arxiv.org/abs/2503.08136)
* [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://openaccess.thecvf.com/content/ICCV2025/papers/Tang_Closed-Loop_Transfer_for_Weakly-supervised_Affordance_Grounding_ICCV_2025_paper.pdf)
* [ReconDreamer Harmonizing Generative and Reconstructive Models for Driving Scene Representation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_ReconDreamer_Harmonizing_Generative_and_Reconstructive_Models_for_Driving_Scene_Representation_ICCV_2025_paper.pdf)
* [Generative Zoo](http://arxiv.org/abs/2412.08101)
* [PAN-Crafter Learning Modality-Consistent Alignment for PAN-Sharpening](https://openaccess.thecvf.com/content/ICCV2025/papers/Do_PAN-Crafter_Learning_Modality-Consistent_Alignment_for_PAN-Sharpening_ICCV_2025_paper.pdf)
* [SANA-Sprint One-Step Diffusion with Continuous-Time Consistency Distillation](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation_ICCV_2025_paper.pdf)
* [Erasing More Than Intended How Concept Erasure Degrades the Generation of Non-Target Concepts](http://arxiv.org/abs/2501.09833)
* [Demeter A Parametric Model of Crop Plant Morphology from the Real World](https://openaccess.thecvf.com/content/ICCV2025/papers/Cheng_Demeter_A_Parametric_Model_of_Crop_Plant_Morphology_from_the_ICCV_2025_paper.pdf)
* [S3E Self-Supervised State Estimation for Radar-Inertial System](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_S3E_Self-Supervised_State_Estimation_for_Radar-Inertial_System_ICCV_2025_paper.pdf)
* [PROGRESSOR A Perceptually Guided Reward Estimator with Self-Supervised Online Refinement](http://arxiv.org/abs/2411.17764)
* [SHeaP Self-Supervised Head Geometry Predictor Learned via 2D Gaussians](http://arxiv.org/abs/2504.12292)
* [Cooperative Pseudo Labeling for Unsupervised Federated Classification](http://arxiv.org/abs/2510.10100)<br>:star:[code](https://github.com/krumpguo/FedCoPL)
* [SignRep Enhancing Self-Supervised Sign Representations](http://arxiv.org/abs/2503.08529)
* [Self-Supervised Sparse Sensor Fusion for Long Range Perception](http://arxiv.org/abs/2508.13995)
* [AIM Amending Inherent Interpretability via Self-Supervised Masking](http://arxiv.org/abs/2508.11502)
* [Unsupervised Identification of Protein Compositions and Conformations via Implicit Content-Transformation Disentanglement](https://openaccess.thecvf.com/content/ICCV2025/papers/Uddin_Unsupervised_Identification_of_Protein_Compositions_and_Conformations_via_Implicit_Content-Transformation_ICCV_2025_paper.pdf)
* [Efficient Unsupervised Shortcut Learning Detection and Mitigation in Transformers](https://openaccess.thecvf.com/content/ICCV2025/papers/Kuhn_Efficient_Unsupervised_Shortcut_Learning_Detection_and_Mitigation_in_Transformers_ICCV_2025_paper.pdf)
* [DASH 4D Hash Encoding with Self-Supervised Decomposition for Real-Time Dynamic Scene Rendering](http://arxiv.org/abs/2507.19141)<br>:star:[code](https://github.com/chenj02/DASH)
* [DIP Unsupervised Dense In-Context Post-training of Visual Representations](http://arxiv.org/abs/2506.18463)
* [Progressive Distribution Bridging Unsupervised Adaptation for Large-scale Pre-trained Models via Adaptive Auxiliary Data](https://openaccess.thecvf.com/content/ICCV2025/papers/He_Progressive_Distribution_Bridging_Unsupervised_Adaptation_for_Large-scale_Pre-trained_Models_via_ICCV_2025_paper.pdf)
* [GloPER Unsupervised Animal Pattern Extraction from Local Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_GloPER_Unsupervised_Animal_Pattern_Extraction_from_Local_Reconstruction_ICCV_2025_paper.pdf)
* [Gain-MLP Improving HDR Gain Map Encoding via a Lightweight MLP](https://openaccess.thecvf.com/content/ICCV2025/papers/Canham_Gain-MLP_Improving_HDR_Gain_Map_Encoding_via_a_Lightweight_MLP_ICCV_2025_paper.pdf)
* [Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding](https://openaccess.thecvf.com/content/ICCV2025/papers/He_Joint_Semantic_and_Rendering_Enhancements_in_3D_Gaussian_Modeling_with_ICCV_2025_paper.pdf)
* [AdaptiveAE An Adaptive Exposure Strategy for HDR Capturing in Dynamic Scenes](http://arxiv.org/abs/2508.13503)
* [Robust Unfolding Network for HDR Imaging with Modulo Cameras](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Robust_Unfolding_Network_for_HDR_Imaging_with_Modulo_Cameras_ICCV_2025_paper.pdf)
* [DEPTHOR Depth Enhancement from a Practical Light-Weight dToF Sensor and RGB Image](http://arxiv.org/abs/2504.01596)<br>:star:[code](https://github.com/ShadowBbBb/Depthor)
* [GaSLight Gaussian Splats for Spatially-Varying Lighting in HDR](http://arxiv.org/abs/2504.10809)<br>:house:[project](https://lvsn.github.io/gaslight)
* [Neural Compression for 3D Geometry Sets](http://arxiv.org/abs/2405.15034)<br>:star:[code](https://github.com/rsy6318/NeCGS)
* [Wide2Long Learning Lens Compression and Perspective Adjustment for Wide-Angle to Telephoto Translation](https://openaccess.thecvf.com/content/ICCV2025/papers/Banerjee_Wide2Long_Learning_Lens_Compression_and_Perspective_Adjustment_for_Wide-Angle_to_ICCV_2025_paper.pdf)
* [Dataset Distillation as Data Compression A Rate-Utility Perspective](http://arxiv.org/abs/2507.17221)
* [SIMS Simulating Stylized Human-Scene Interactions with Retrieval-Augmented Script Generation](http://arxiv.org/abs/2411.19921)
* [Integrating Visual Interpretation and Linguistic Reasoning for Geometric Problem Solving](https://openaccess.thecvf.com/content/ICCV2025/papers/Guo_Integrating_Visual_Interpretation_and_Linguistic_Reasoning_for_Geometric_Problem_Solving_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/guozix/DVLR)
* [VisRL Intention-Driven Visual Perception via Reinforced Reasoning](http://arxiv.org/abs/2503.07523)
* [MINERVA Evaluating Complex Video Reasoning](http://arxiv.org/abs/2505.00681)<br>:star:[code](https://github.com/google-deepmind/neptune)
* [From Easy to Hard The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning](http://arxiv.org/abs/2509.17040)
* [VideoSetDiff Identifying and Reasoning Similarities and Differences in Similar Videos](https://openaccess.thecvf.com/content/ICCV2025/papers/Qiu_VideoSetDiff_Identifying_and_Reasoning_Similarities_and_Differences_in_Similar_Videos_ICCV_2025_paper.pdf)
* [Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization](http://arxiv.org/abs/2504.18397)
* [DWIM Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow Generation  Instruct-Masking Tuning](http://arxiv.org/abs/2503.19263)
* [A Unified Framework for Motion Reasoning and Generation in Human Interaction](http://arxiv.org/abs/2410.05628)
* [VEGGIE Instructional Editing and Reasoning Video Concepts with Grounded Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_VEGGIE_Instructional_Editing_and_Reasoning_Video_Concepts_with_Grounded_Generation_ICCV_2025_paper.pdf)
* [3DSRBench A Comprehensive 3D Spatial Reasoning Benchmark](http://arxiv.org/abs/2412.07825)
* [MMCR Benchmarking Cross-Source Reasoning in Scientific Papers](http://arxiv.org/abs/2503.16856)
* [Unveiling the Invisible Reasoning Complex Occlusions Amodally with AURA](http://arxiv.org/abs/2503.10225)<br>:house:[project](https://zhixuanli.github.io/projects)
* [VRBench A Benchmark for Multi-Step Reasoning in Long Narrative Videos](http://arxiv.org/abs/2506.10857)
* [A Visual Leap in CLIP Compositionality Reasoning through Generation of Counterfactual Sets](http://arxiv.org/abs/2507.04699)
* [From Linearity to Non-Linearity How Masked Autoencoders Capture Spatial Correlations](https://openaccess.thecvf.com/content/ICCV2025/papers/Bisulco_From_Linearity_to_Non-Linearity_How_Masked_Autoencoders_Capture_Spatial_Correlations_ICCV_2025_paper.pdf)
* [X-Capture An Open-Source Portable Device for Multi-Sensory Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Clarke_X-Capture_An_Open-Source_Portable_Device_for_Multi-Sensory_Learning_ICCV_2025_paper.pdf)
* [MonoMobility Zero-Shot 3D Mobility Analysis from Monocular Videos](http://arxiv.org/abs/2505.11868)
* [Zero-Shot Vision Encoder Grafting via LLM Surrogates](http://arxiv.org/abs/2505.22664)
* [Few-Shot Pattern Detection via Template Matching and Regression](http://arxiv.org/abs/2508.17636)
* [Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation](http://arxiv.org/abs/2504.12436)
* [FIND Few-Shot Anomaly Inspection with Normal-Only Multi-Modal Data](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_FIND_Few-Shot_Anomaly_Inspection_with_Normal-Only_Multi-Modal_Data_ICCV_2025_paper.pdf)
* [SpikeDiff Zero-shot High-Quality Video Reconstruction from Chromatic Spike Camera and Sub-millisecond Spike Streams](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_SpikeDiff_Zero-shot_High-Quality_Video_Reconstruction_from_Chromatic_Spike_Camera_and_ICCV_2025_paper.pdf)
* [TikZero Zero-Shot Text-Guided Graphics Program Synthesis](http://arxiv.org/abs/2503.11509)
* [FontAnimate High Quality Few-shot Font Generation via Animating Font Transfer Process](https://openaccess.thecvf.com/content/ICCV2025/papers/Fu_FontAnimate_High_Quality_Few-shot_Font_Generation_via_Animating_Font_Transfer_ICCV_2025_paper.pdf)
* [MAVFlow Preserving Paralinguistic Elements with Conditional Flow Matching for Zero-Shot AV2AV Multilingual Translation](http://arxiv.org/abs/2503.11026)<br>:star:[code](https://github.com/Peter-SungwooCho/MAVFlow)
* [Unknown Text Learning for CLIP-based Few-Shot Open-set Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Ma_Unknown_Text_Learning_for_CLIP-based_Few-Shot_Open-set_Recognition_ICCV_2025_paper.pdf)
* [Zero-Shot Compositional Video Learning with Coding Rate Reduction](https://openaccess.thecvf.com/content/ICCV2025/papers/Jung_Zero-Shot_Compositional_Video_Learning_with_Coding_Rate_Reduction_ICCV_2025_paper.pdf)
* [Deeply Supervised Flow-Based Generative Models](http://arxiv.org/abs/2503.14494)
* [X2I Seamless Integration of Multimodal Understanding into Diffusion Transformer via Attention Distillation](http://arxiv.org/abs/2503.06134)
* [Continual Personalization for Diffusion Models](http://arxiv.org/abs/2510.02296)
* [Federated Continual Instruction Tuning](http://arxiv.org/abs/2503.12897)<br>:star:[code](https://github.com/Ghy0501/FCIT)
* [Hybrid-TTA Continual Test-time Adaptation via Dynamic Domain Shift Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Park_Hybrid-TTA_Continual_Test-time_Adaptation_via_Dynamic_Domain_Shift_Detection_ICCV_2025_paper.pdf)
* [SMoLoRA Exploring and Defying Dual Catastrophic Forgetting in Continual Visual Instruction Tuning](http://arxiv.org/abs/2411.13949)
* [A Framework for Double-Blind Federated Adaptation of Foundation Models](http://arxiv.org/abs/2502.01289)<br>:star:[code](https://github.com/tnurbek/blindfed)
* [Stable Score Distillation](http://arxiv.org/abs/2507.09168)<br>:star:[code](https://github.com/Alex-Zhu1/SSD)
* [LoRA-FAIR Federated LoRA Fine-Tuning with Aggregation and Initialization Refinement](https://openaccess.thecvf.com/content/ICCV2025/papers/Bian_LoRA-FAIR_Federated_LoRA_Fine-Tuning_with_Aggregation_and_Initialization_Refinement_ICCV_2025_paper.pdf)
* [Class-Wise Federated Averaging for Efficient Personalization](http://arxiv.org/abs/2406.07800)
* [EFTViT Efficient Federated Training of Vision Transformers with Masked Images on Resource-Constrained Clients](http://arxiv.org/abs/2412.00334)
* [Tensor-aggregated LoRA in Federated Fine-tuning](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Tensor-aggregated_LoRA_in_Federated_Fine-tuning_ICCV_2025_paper.pdf)
* [CMAD Correlation-Aware and Modalities-Aware Distillation for Multimodal Sentiment Analysis with Missing Modalities](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhuang_CMAD_Correlation-Aware_and_Modalities-Aware_Distillation_for_Multimodal_Sentiment_Analysis_with_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/YetZzzzzz/CMAD)
* [Federated Prompt-Tuning with Heterogeneous and Incomplete Multimodal Client Data](https://openaccess.thecvf.com/content/ICCV2025/papers/Phung_Federated_Prompt-Tuning_with_Heterogeneous_and_Incomplete_Multimodal_Client_Data_ICCV_2025_paper.pdf)
* [GlassWizard Harvesting Diffusion Priors for Glass Surface Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_GlassWizard_Harvesting_Diffusion_Priors_for_Glass_Surface_Detection_ICCV_2025_paper.pdf)
* [CLIPSym Delving into Symmetry Detection with CLIP](http://arxiv.org/abs/2508.14197)
* [Axis-level Symmetry Detection with Group-Equivariant Representation](http://arxiv.org/abs/2508.10740)
* [MistSense Versatile Online Detection of Procedural and Execution Mistakes](https://openaccess.thecvf.com/content/ICCV2025/papers/Patsch_MistSense_Versatile_Online_Detection_of_Procedural_and_Execution_Mistakes_ICCV_2025_paper.pdf)
* [Moderating the Generalization of Score-based Generative Model](http://arxiv.org/abs/2412.07229)<br>:star:[code](https://github.com/yunfengdiao/Moderated-Score-based-Generative-Model)
* [On the Generalization of Representation Uncertainty in Earth Observation](http://arxiv.org/abs/2503.07082)<br>:star:[code](https://github.com/Orion-AI-Lab/EOUncertaintyGeneralization)
* [Learning Few-Step Diffusion Models by Trajectory Distribution Matching](http://arxiv.org/abs/2503.06674)
* [Recovering Parametric Scenes from Very Few Time-of-Flight Pixels](http://arxiv.org/abs/2509.16132)
* [Can We Achieve Efficient Diffusion Without Self-Attention Distilling Self-Attention into Convolutions](http://arxiv.org/abs/2504.21292)
* [Predict-Optimize-Distill A Self-Improving Cycle for 4D Object Understanding](https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Predict-Optimize-Distill_A_Self-Improving_Cycle_for_4D_Object_Understanding_ICCV_2025_paper.pdf)
* [ArgoTweak Towards Self-Updating HD Maps through Structured Priors](http://arxiv.org/abs/2509.08764)<br>:house:[project](https://KTH-RPL.github.io/ArgoTweak)
* [ILLUME Illuminating Your LLMs to See Draw and Self-Enhance](http://arxiv.org/abs/2412.06673)
* [Iris Breaking GUI Complexity with Adaptive Focus and Self-Refining](http://arxiv.org/abs/2412.10342)
* [Bridging the Gap between Brain and Machine in Interpreting Visual Semantics Towards Self-adaptive Brain-to-Text Decoding](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Bridging_the_Gap_between_Brain_and_Machine_in_Interpreting_Visual_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/JxuanC/MindSA)
* [Learning Neural Scene Representation from iToF Imaging](https://openaccess.thecvf.com/content/ICCV2025/papers/Chang_Learning_Neural_Scene_Representation_from_iToF_Imaging_ICCV_2025_paper.pdf)
* [ACE-G Improving Generalization of Scene Coordinate Regression Through Query Pre-Training](https://openaccess.thecvf.com/content/ICCV2025/papers/Bruns_ACE-G_Improving_Generalization_of_Scene_Coordinate_Regression_Through_Query_Pre-Training_ICCV_2025_paper.pdf)
* [Teleportraits Training-Free People Insertion into Any Scene](http://arxiv.org/abs/2510.05660)
* [From Image to Video An Empirical Study of Diffusion Representations](https://openaccess.thecvf.com/content/ICCV2025/papers/Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations_ICCV_2025_paper.pdf)
* [REGEN Learning Compact Video Embedding with (Re-)Generative Decoder](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_REGEN_Learning_Compact_Video_Embedding_with_Re-Generative_Decoder_ICCV_2025_paper.pdf)
* [Generative Video Bi-flow](http://arxiv.org/abs/2503.06364)
* [VISION-XL High Definition Video Inverse Problem Solver using Latent Image Diffusion Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Kwon_VISION-XL_High_Definition_Video_Inverse_Problem_Solver_using_Latent_Image_ICCV_2025_paper.pdf)<br>:house:[project](https://vision-xl.github.io/)
* [TACO Taming Diffusion for in-the-wild Video Amodal Completion](http://arxiv.org/abs/2503.12049)
* [Free-MoRef Instantly Multiplexing Context Perception Capabilities of Video-MLLMs within Single Inference](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Free-MoRef_Instantly_Multiplexing_Context_Perception_Capabilities_of_Video-MLLMs_within_Single_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/wkfdb/Free-MoRef)
* [MultiModal Action Conditioned Video Simulation](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_MultiModal_Action_Conditioned_Video_Simulation_ICCV_2025_paper.pdf)
* [Aligning Moments in Time using Video Queries](http://arxiv.org/abs/2508.15439)
* [Make Your Training Flexible Towards Deployment-Efficient Video Models](http://arxiv.org/abs/2503.14237)<br>:star:[code](https://github.com/OpenGVLab/FluxViT)
* [Stereo Any Video Temporally Consistent Stereo Matching](http://arxiv.org/abs/2503.05549)
* [SAFT Shape and Appearance of Fabrics from Template via Differentiable Physical Simulations from Monocular Video](http://arxiv.org/abs/2509.08828)
* [Everything is a Video Unifying Modalities through Next-Frame Prediction](http://arxiv.org/abs/2411.10503)
* [FlowStyler Artistic Video Stylization via Transformation Fields Transports](https://openaccess.thecvf.com/content/ICCV2025/papers/Gong_FlowStyler_Artistic_Video_Stylization_via_Transformation_Fields_Transports_ICCV_2025_paper.pdf)
* [Long-Context State-Space Video World Models](http://arxiv.org/abs/2505.20171)
* [PVChat Personalized Video Chat with One-Shot Learning](http://arxiv.org/abs/2503.17069)
* [Instance-Level Video Depth in Groups Beyond Occlusions](https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_Instance-Level_Video_Depth_in_Groups_Beyond_Occlusions_ICCV_2025_paper.pdf)
* [SKALD Learning-Based Shot Assembly for Coherent Multi-Shot Video Creation](http://arxiv.org/abs/2503.08010)
* [Preacher Paper-to-Video Agentic System](http://arxiv.org/abs/2508.09632)
* [TemCoCo Temporally Consistent Multi-modal Video Fusion with Visual-Semantic Collaboration](http://arxiv.org/abs/2508.17817)<br>:star:[code](https://github.com/Meiqi-Gong/TemCoCo)
* [Spatial Alignment and Temporal Matching Adapter for Video-Radar Remote Physiological Measurement](https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_Spatial_Alignment_and_Temporal_Matching_Adapter_for_Video-Radar_Remote_Physiological_ICCV_2025_paper.pdf)
* [Light-A-Video Training-free Video Relighting via Progressive Light Fusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_Light-A-Video_Training-free_Video_Relighting_via_Progressive_Light_Fusion_ICCV_2025_paper.pdf)
* [VoiceCraft-Dub Automated Video Dubbing with Neural Codec Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Sung-Bin_VoiceCraft-Dub_Automated_Video_Dubbing_with_Neural_Codec_Language_Models_ICCV_2025_paper.pdf)
* [RnGCam High-speed video from rolling  global shutter measurements](http://arxiv.org/abs/2509.18087)
* [AnnofreeOD Detecting All Classes at Low Frame Rates Without Human Annotations](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_AnnofreeOD_Detecting_All_Classes_at_Low_Frame_Rates_Without_Human_ICCV_2025_paper.pdf)
* [Expressive Talking Human from Single-Image with Imperfect Priors](https://openaccess.thecvf.com/content/ICCV2025/papers/Xiang_Expressive_Talking_Human_from_Single-Image_with_Imperfect_Priors_ICCV_2025_paper.pdf)
* [Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling](http://arxiv.org/abs/2503.11806)
* [UniPortrait A Unified Framework for Identity-Preserving Single- and Multi-Human Image Personalization](https://openaccess.thecvf.com/content/ICCV2025/papers/He_UniPortrait_A_Unified_Framework_for_Identity-Preserving_Single-_and_Multi-Human_Image_ICCV_2025_paper.pdf)
* [DreamDance Animating Human Images by Enriching 3D Geometry Cues from 2D Poses](http://arxiv.org/abs/2412.00397)
* [ATLAS Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling](http://arxiv.org/abs/2508.15767)
* [CompleteMe Reference-based Human Image Completion](http://arxiv.org/abs/2504.20042)
* [Visual Interestingness Decoded How GPT-4o Mirrors Human Interests](https://openaccess.thecvf.com/content/ICCV2025/papers/Abdullahu_Visual_Interestingness_Decoded_How_GPT-4o_Mirrors_Human_Interests_ICCV_2025_paper.pdf)
* [2HandedAfforder Learning Precise Actionable Bimanual Affordances from Human Videos](http://arxiv.org/abs/2503.09320)
* [Unraveling the Smoothness Properties of Diffusion Models A Gaussian Mixture Perspective](https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_Unraveling_the_Smoothness_Properties_of_Diffusion_Models_A_Gaussian_Mixture_ICCV_2025_paper.pdf)
* [GausSim Foreseeing Reality by Gaussian Simulator for Elastic Objects](http://arxiv.org/abs/2412.17804)
* [3DGS-LM Faster Gaussian-Splatting Optimization with Levenberg-Marquardt](https://openaccess.thecvf.com/content/ICCV2025/papers/Hollein_3DGS-LM_Faster_Gaussian-Splatting_Optimization_with_Levenberg-Marquardt_ICCV_2025_paper.pdf)
* [RhythmGuassian Repurposing Generalizable Gaussian Model For Remote Physiological Measurement](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/LuPaoPao/RhythmGuassian)
* [GaussianReg Rapid 2D3D Registration for Emergency Surgery via Explicit 3D Modeling with Gaussian Primitives](https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_GaussianReg_Rapid_2D3D_Registration_for_Emergency_Surgery_via_Explicit_3D_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/CUHK-AIM-Group/GaussianReg)
* [Learning Efficient and Generalizable Human Representation with Human Gaussian Model](http://arxiv.org/abs/2507.18758)
* [VoluMe - Authentic 3D Video Calls from Live Gaussian Splat Prediction](https://openaccess.thecvf.com/content/ICCV2025/papers/de_La_Gorce_VoluMe_-_Authentic_3D_Video_Calls_from_Live_Gaussian_Splat_ICCV_2025_paper.pdf)
* [MorphoGen Efficient Unconditional Generation of Long-Range Projection Neuronal Morphology via a Global-to-Local Framework](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_MorphoGen_Efficient_Unconditional_Generation_of_Long-Range_Projection_Neuronal_Morphology_via_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Brainsmatics/MorphoGen)
* [Less-to-More Generalization Unlocking More Controllability by In-Context Generation](http://arxiv.org/abs/2504.02160)<br>:star:[code](https://github.com/bytedance/UNO)
* [Hi3DGen High-fidelity 3D Geometry Generation from Images via Normal Bridging](http://arxiv.org/abs/2503.22236)
* [The Curse of Conditions Analyzing and Improving Optimal Transport for Conditional Flow-Based Generation](http://arxiv.org/abs/2503.10636)
* [SynCity Training-Free Generation of 3D Worlds](http://arxiv.org/abs/2503.16420)
* [EvolvingGrasp Evolutionary Grasp Generation via Efficient Preference Alignment](http://arxiv.org/abs/2503.14329)
* [NuiScene Exploring Efficient Generation of Unbounded Outdoor Scenes](http://arxiv.org/abs/2503.16375)
* [RomanTex Decoupling 3D-aware Rotary Positional Embedded Multi-Attention Network for Texture Synthesis](http://arxiv.org/abs/2503.19011)
* [DreamCube RGB-D Panorama Generation via Multi-plane Synchronization](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_DreamCube_RGB-D_Panorama_Generation_via_Multi-plane_Synchronization_ICCV_2025_paper.pdf)
* [Aligning Constraint Generation with Design Intent in Parametric CAD](http://arxiv.org/abs/2504.13178)
* [Multidimensional Byte Pair Encoding Shortened Sequences for Improved Visual Data Generation](http://arxiv.org/abs/2411.10281)
* [Oasis One Image is All You Need for Multimodal Instruction Data Synthesis](http://arxiv.org/abs/2503.08741)
* [MetaMorph Multimodal Understanding and Generation via Instruction Tuning](https://openaccess.thecvf.com/content/ICCV2025/papers/Tong_MetaMorph_Multimodal_Understanding_and_Generation_via_Instruction_Tuning_ICCV_2025_paper.pdf)
* [Training-free Generation of Temporally Consistent Rewards from VLMs](http://arxiv.org/abs/2507.04789)<br>:house:[project](https://t2-vlm.github.io/)
* [Text2Outfit Controllable Outfit Generation with Multimodal Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhai_Text2Outfit_Controllable_Outfit_Generation_with_Multimodal_Language_Models_ICCV_2025_paper.pdf)
* [Can Knowledge be Transferred from Unimodal to Multimodal Investigating the Transitivity of Multimodal Knowledge Editing](https://openaccess.thecvf.com/content/ICCV2025/papers/Fang_Can_Knowledge_be_Transferred_from_Unimodal_to_Multimodal_Investigating_the_ICCV_2025_paper.pdf)
* [Harmonizing Visual Representations for Unified Multimodal Understanding and Generation](http://arxiv.org/abs/2503.21979)<br>:star:[code](https://github.com/wusize/Harmon)
* [HERO Human Reaction Generation from Videos](http://arxiv.org/abs/2503.08270)
* [Diffuman4D 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models](http://arxiv.org/abs/2507.13344)<br>:house:[project](https://diffuman4d.github.io/)
* [PerLDiff Controllable Street View Synthesis Using Perspective-Layout Diffusion Model](http://arxiv.org/abs/2407.06109)
* [Stable Virtual Camera Generative View Synthesis with Diffusion Models](http://arxiv.org/abs/2503.14489)
* [What Makes for Text to 360-degree Panorama Generation with Stable Diffusion](http://arxiv.org/abs/2505.22129)
* [Latent Swap Joint Diffusion for 2D Long-Form Latent Generation](http://arxiv.org/abs/2502.05130)
* [LiON-LoRA Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_LiON-LoRA_Rethinking_LoRA_Fusion_to_Unify_Controllable_Spatial_and_Temporal_ICCV_2025_paper.pdf)
* [DreamLayer Simultaneous Multi-Layer Generation via Diffusion Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model_ICCV_2025_paper.pdf)
* [StreamDiffusion A Pipeline-level Solution for Real-Time Interactive Generation](http://arxiv.org/abs/2312.12491)
* [LayerTracer Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer](http://arxiv.org/abs/2502.01105)
* [MatchDiffusion Training-free Generation of Match-Cuts](http://arxiv.org/abs/2411.18677)
* [SpinMeRound Consistent Multi-View Identity Generation Using Diffusion Models](http://arxiv.org/abs/2504.10716)
* [Omegance A Single Parameter for Various Granularities in Diffusion-Based Synthesis](http://arxiv.org/abs/2411.17769)<br>:star:[code](https://github.com/itsmag11/Omegance)
* [TaxaDiffusion Progressively Trained Diffusion Model for Fine-Grained Species Generation](http://arxiv.org/abs/2506.01923)
* [Ph-GAN Physics-Inspired GAN for Generating SAR Images Under Limited Data](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Ph-GAN_Physics-Inspired_GAN_for_Generating_SAR_Images_Under_Limited_Data_ICCV_2025_paper.pdf)
* [Multimodal Latent Diffusion Model for Complex Sewing Pattern Generation](http://arxiv.org/abs/2412.14453)
* [Orchid Image Latent Diffusion for Joint Appearance and Geometry Generation](http://arxiv.org/abs/2501.13087)
* [RAGDiffusion Faithful Cloth Generation via External Knowledge Assimilation](http://arxiv.org/abs/2411.19528)
* [Controllable Weather Synthesis and Removal with Video Diffusion Models](http://arxiv.org/abs/2505.00704)
* [SG-LDM Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Xiang_SG-LDM_Semantic-Guided_LiDAR_Generation_via_Latent-Aligned_Diffusion_ICCV_2025_paper.pdf)
* [ContraGS Codebook-Condensed and Trainable Gaussian Splatting for Fast Memory-Efficient Reconstruction](http://arxiv.org/abs/2509.03775)
* [StreamGS Online Generalizable Gaussian Splatting Reconstruction for Unposed Image Streams](http://arxiv.org/abs/2503.06235)
* [EvaGaussians Event Stream Assisted Gaussian Splatting from Blurry Images](http://arxiv.org/abs/2405.20224)
* [GeoSplatting Towards Geometry Guided Gaussian Splatting for Physically-based Inverse Rendering](http://arxiv.org/abs/2410.24204)
* [Splat-LOAM Gaussian Splatting LiDAR Odometry and Mapping](https://openaccess.thecvf.com/content/ICCV2025/papers/Giacomini_Splat-LOAM_Gaussian_Splatting_LiDAR_Odometry_and_Mapping_ICCV_2025_paper.pdf)
* [7DGS Unified Spatial-Temporal-Angular Gaussian Splatting](http://arxiv.org/abs/2503.07946)
* [STD-GS Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_STD-GS_Exploring_Frame-Event_Interaction_for_SpatioTemporal-Disentangled_Gaussian_Splatting_to_Reconstruct_ICCV_2025_paper.pdf)
* [GS-ID Illumination Decomposition on Gaussian Splatting via Adaptive Light Aggregation and Diffusion-Guided Material Priors](https://openaccess.thecvf.com/content/ICCV2025/papers/Du_GS-ID_Illumination_Decomposition_on_Gaussian_Splatting_via_Adaptive_Light_Aggregation_ICCV_2025_paper.pdf)
* [Gaussian Splatting with Discretized SDF for Relightable Assets](http://arxiv.org/abs/2507.15629)
* [GS-LIVM Real-Time Photo-Realistic LiDAR-Inertial-Visual Mapping with Gaussian Splatting](https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_GS-LIVM_Real-Time_Photo-Realistic_LiDAR-Inertial-Visual_Mapping_with_Gaussian_Splatting_ICCV_2025_paper.pdf)
* [Self-Calibrating Gaussian Splatting for Large Field-of-View Reconstruction](http://arxiv.org/abs/2502.09563)<br>:house:[project](https://denghilbert.github.io/self-cali) :house:[project](https://denghilbert.github.io/self-cali/)
* [2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update](http://arxiv.org/abs/2507.11069)<br>:house:[project](https://jeongyun0609.github.io/TRAN-D)
* [EVER Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis](http://arxiv.org/abs/2410.01804)
* [X2-Gaussian 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_X2-Gaussian_4D_Radiative_Gaussian_Splatting_for_Continuous-time_Tomographic_Reconstruction_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/CUHK-AIM-Group/X2-Gaussian)
* [GaussianVideo Efficient Video Representation via Hierarchical Gaussian Splatting](http://arxiv.org/abs/2501.04782)
* [LUDVIG Learning-Free Uplifting of 2D Visual Features to Gaussian Splatting Scenes](http://arxiv.org/abs/2410.14462)
* [RogSplat Robust Gaussian Splatting via Generative Priors](https://openaccess.thecvf.com/content/ICCV2025/papers/Kong_RogSplat_Robust_Gaussian_Splatting_via_Generative_Priors_ICCV_2025_paper.pdf)
* [Instant GaussianImage A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting](http://arxiv.org/abs/2506.23479)
* [Seam360GS Seamless 360deg Gaussian Splatting from Real-World Omnidirectional Images](https://openaccess.thecvf.com/content/ICCV2025/papers/Shin_Seam360GS_Seamless_360deg_Gaussian_Splatting_from_Real-World_Omnidirectional_Images_ICCV_2025_paper.pdf)
* [Tile-wise vs Image-wise Random-Tile Loss and Training Paradigm for Gaussian Splatting](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Tile-wise_vs._Image-wise_Random-Tile_Loss_and_Training_Paradigm_for_Gaussian_ICCV_2025_paper.pdf)
* [Subjective Camera 10 Bridging Human Cognition and Visual Reconstruction through Sequence-Aware Sketch-Guided Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Subjective_Camera_1.0_Bridging_Human_Cognition_and_Visual_Reconstruction_through_ICCV_2025_paper.pdf)
* [LeanVAE An Ultra-Efficient Reconstruction VAE for Video Diffusion Models](http://arxiv.org/abs/2503.14325)<br>:star:[code](https://github.com/westlake-repl/LeanVAE)
* [ReassembleNet Learnable Keypoints and Diffusion for 2D Fresco Reconstruction](http://arxiv.org/abs/2505.21117)
* [Event-guided HDR Reconstruction with Diffusion Priors](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_Event-guided_HDR_Reconstruction_with_Diffusion_Priors_ICCV_2025_paper.pdf)
* [CryoFastAR Fast Cryo-EM Ab initio Reconstruction Made Easy](http://arxiv.org/abs/2506.05864)
* [Physical Degradation Model-Guided Interferometric Hyperspectral Reconstruction with Unfolding Transformer](http://arxiv.org/abs/2506.21880)<br>:star:[code](https://github.com/bit1120203554/IHRUT)
* [NGD Neural Gradient Based Deformation for Monocular Garment Reconstruction](http://arxiv.org/abs/2508.17712)
* [Discretized Gaussian Representation for Tomographic Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Discretized_Gaussian_Representation_for_Tomographic_Reconstruction_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/wskingdom/DGR)
* [CO2-Net A Physics-Informed Spatio-Temporal Model for Global Surface CO2 Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_CO2-Net_A_Physics-Informed_Spatio-Temporal_Model_for_Global_Surface_CO2_Reconstruction_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Leamonz/CORE)
* [Sparfels Fast Reconstruction from Sparse Unposed Imagery](http://arxiv.org/abs/2505.02178)
* [Teeth Reconstruction and Performance Capture Using a Phone Camera](https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera_ICCV_2025_paper.pdf)
* [PRM Photometric Stereo based Large Reconstruction Model](http://arxiv.org/abs/2412.07371)
* [Dual-S3D Hierarchical Dual-Path Selective SSM-CNN for High-Fidelity Implicit Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Dual-S3D_Hierarchical_Dual-Path_Selective_SSM-CNN_for_High-Fidelity_Implicit_Reconstruction_ICCV_2025_paper.pdf)
* [Long-LRM Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats](https://openaccess.thecvf.com/content/ICCV2025/papers/Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats_ICCV_2025_paper.pdf)<br>:house:[project](http://arthurhero.github.io/projects) :house:[project](http://arthurhero.github.io/projects/llrm/)
* [Neurons Emulating the Human Visual Cortex Improves Fidelity and Interpretability in fMRI-to-Video Reconstruction](http://arxiv.org/abs/2503.11167)<br>:star:[code](https://github.com/xmed-lab/NEURONS)
* [PhysTwin Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos](http://arxiv.org/abs/2503.17973)<br>:house:[project](https://jianghanxiao.github.io/phystwin-web/) :house:[project](https://jianghanxiao.github.io/phystwin-web)
* [Boundary Probing for Input Privacy Protection When Using LMM Services](https://openaccess.thecvf.com/content/ICCV2025/papers/Hui_Boundary_Probing_for_Input_Privacy_Protection_When_Using_LMM_Services_ICCV_2025_paper.pdf)
* [FG-OrIU Towards Better Forgetting via Feature-Gradient Orthogonality for Incremental Unlearning](https://openaccess.thecvf.com/content/ICCV2025/papers/Feng_FG-OrIU_Towards_Better_Forgetting_via_Feature-Gradient_Orthogonality_for_Incremental_Unlearning_ICCV_2025_paper.pdf)
* [CopyrightShield Enhancing Diffusion Model Security Against Copyright Infringement Attacks](http://arxiv.org/abs/2412.01528)
* [StolenLoRA Exploring LoRA Extraction Attacks via Synthetic Data](http://arxiv.org/abs/2509.23594)
* [Membership Inference Attacks with False Discovery Rate Control](http://arxiv.org/abs/2508.07066)
* [FastJSMA Accelerating Jacobian-based Saliency Map Attacks through Gradient Decoupling](https://openaccess.thecvf.com/content/ICCV2025/papers/Gao_FastJSMA_Accelerating_Jacobian-based_Saliency_Map_Attacks_through_Gradient_Decoupling_ICCV_2025_paper.pdf)
* [On the Robustness Tradeoff in Fine-Tuning](http://arxiv.org/abs/2503.14836)
* [MOSAIC Generating Consistent Privacy-Preserving Scenes from Multiple Depth Views in Multi-Room Environments](http://arxiv.org/abs/2503.13816)
* [AutoComPose Automatic Generation of Pose Transition Descriptions for Composed Pose Retrieval Using Multimodal LLMs](http://arxiv.org/abs/2503.22884)
* [REDUCIO Generating 1K Video within 16 Seconds using Extremely Compressed Motion Latents](http://arxiv.org/abs/2411.13552)<br>:star:[code](https://github.com/microsoft/Reducio-VAE)
* [Video Motion Graphs](http://arxiv.org/abs/2503.20218)
* [DisCoRD Discrete Tokens to Continuous Motion via Rectified Flow Decoding](http://arxiv.org/abs/2411.19527)
* [Global Motion Corresponder for 3D Point-Based Scene Interpolation under Large Motion](http://arxiv.org/abs/2508.20136)
* [Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_Long-term_Traffic_Simulation_with_Interleaved_Autoregressive_Motion_and_Scenario_Generation_ICCV_2025_paper.pdf)<br>:house:[project](https://orangesodahub.github.io/InfGen)
* [Easi3R Estimating Disentangled Motion from DUSt3R Without Training](http://arxiv.org/abs/2503.24391)
* [Sequential Gaussian Avatars with Hierarchical Motion Context](http://arxiv.org/abs/2411.16768)<br>:house:[project](https://zezeaaa.github.io/projects) :house:[project](https://zezeaaa.github.io/projects/SeqAvatar/)
* [PoseSyn Synthesizing Diverse 3D Pose Data from In-the-Wild 2D Data](http://arxiv.org/abs/2503.13025)
* [PS-Mamba Spatial-Temporal Graph Mamba for Pose Sequence Refinement](https://openaccess.thecvf.com/content/ICCV2025/papers/Dong_PS-Mamba_Spatial-Temporal_Graph_Mamba_for_Pose_Sequence_Refinement_ICCV_2025_paper.pdf)
* [STaR Seamless Spatial-Temporal Aware Motion Retargeting with Penetration and Consistency Constraints](http://arxiv.org/abs/2504.06504)<br>:star:[code](https://github.com/XiaohangYang829/STaR)
* [EMD Explicit Motion Modeling for High-Quality Street Gaussian Splatting](http://arxiv.org/abs/2411.15582)<br>:house:[project](https://qingpowuwu.github.io/emd)
* [CoMoGaussian Continuous Motion-Aware Gaussian Splatting from Motion-Blurred Images](http://arxiv.org/abs/2503.05332)
* [Less is More Improving Motion Diffusion Models with Sparse Keyframes](http://arxiv.org/abs/2503.13859)
* [Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video](http://arxiv.org/abs/2509.08376)
* [Uncalibrated Structure from Motion on a Sphere](https://openaccess.thecvf.com/content/ICCV2025/papers/Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/jonathanventura/spherical-sfm)
* [A Linear N-Point Solver for Structure and Motion from Asynchronous Tracks](https://openaccess.thecvf.com/content/ICCV2025/papers/Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous_ICCV_2025_paper.pdf)
* [MikuDance Animating Character Art with Mixed Motion Dynamics](http://arxiv.org/abs/2411.08656)
* [What If Understanding Motion Through Sparse Interactions](https://openaccess.thecvf.com/content/ICCV2025/papers/Baumann_What_If_Understanding_Motion_Through_Sparse_Interactions_ICCV_2025_paper.pdf)
* [Disrupting Model Merging A Parameter-Level Defense Without Sacrificing Accuracy](http://arxiv.org/abs/2503.07661)<br>:star:[code](https://github.com/ISCT-W/PaRaMS)
* [RadGPT Constructing 3D Image-Text Tumor Datasets](https://openaccess.thecvf.com/content/ICCV2025/papers/Bassi_RadGPT_Constructing_3D_Image-Text_Tumor_Datasets_ICCV_2025_paper.pdf)
* [DDB Diffusion Driven Balancing to Address Spurious Correlations](http://arxiv.org/abs/2503.17226)<br>:star:[code](https://github.com/ArianYp/DDB)
* [Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond](https://openaccess.thecvf.com/content/ICCV2025/papers/Qiao_Learnable_Fractional_Reaction-Diffusion_Dynamics_for_Under-Display_ToF_Imaging_and_Beyond_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/wudiqx106/LFRD2)
* [G2PDiffusion Cross-Species Genotype-to-Phenotype Prediction via Evolutionary Diffusion](http://arxiv.org/abs/2502.04684)
* [Spatial-Temporal Aware Visuomotor Diffusion Policy Learning](http://arxiv.org/abs/2507.06710)
* [CHORDS Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers](http://arxiv.org/abs/2507.15260)
* [EDiT Efficient Diffusion Transformers with Linear Compressed Attention](http://arxiv.org/abs/2503.16726)
* [GenHancer Imperfect Generative Models are Secretly Strong Vision-Centric Enhancers](http://arxiv.org/abs/2503.19480)
* [Diffusion Image Prior](http://arxiv.org/abs/2503.21410)
* [Diffusion Curriculum Synthetic-to-Real Data Curriculum via Image-Guided Diffusion](http://arxiv.org/abs/2410.13674)
* [Towards Stabilized and Efficient Diffusion Transformers through Long-Skip-Connections with Spectral Constraints](http://arxiv.org/abs/2411.17616)
* [IntroStyle Training-Free Introspective Style Attribution using Diffusion Features](http://arxiv.org/abs/2412.14432)
* [GeometryCrafter Consistent Geometry Estimation for Open-world Videos with Diffusion Priors](http://arxiv.org/abs/2504.01016)
* [Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification](http://arxiv.org/abs/2509.13922)
* [Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers](http://arxiv.org/abs/2506.07986)<br>:star:[code](https://github.com/Vchitect/TACA)
* [Accelerating Diffusion Sampling via Exploiting Local Transition Coherence](http://arxiv.org/abs/2503.09675)<br>:house:[project](https://zhushangwen.github.io/LTC-accel.io)
* [Flow to the Mode Mode-Seeking Diffusion Autoencoders for State-of-the-Art Image Tokenization](http://arxiv.org/abs/2503.11056)
* [UniPhys Unified Planner and Controller with Diffusion for Flexible Physics-Based Character Control](http://arxiv.org/abs/2504.12540)
* [A Simple yet Mighty Hartley Diffusion Versatilist for Generalizable Dense Vision Tasks](https://openaccess.thecvf.com/content/ICCV2025/papers/Bi_A_Simple_yet_Mighty_Hartley_Diffusion_Versatilist_for_Generalizable_Dense_ICCV_2025_paper.pdf)
* [JointDiT Enhancing RGB-Depth Joint Modeling with Diffusion Transformers](http://arxiv.org/abs/2505.00482)
* [REPA-E Unlocking VAE for End-to-End Tuning of Latent Diffusion Transformers](https://openaccess.thecvf.com/content/ICCV2025/papers/Leng_REPA-E_Unlocking_VAE_for_End-to-End_Tuning_of_Latent_Diffusion_Transformers_ICCV_2025_paper.pdf)
* [Textured 3D Regenerative Morphing with 3D Diffusion Prior](http://arxiv.org/abs/2502.14316)
* [Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching](http://arxiv.org/abs/2506.14605)
* [DICE Staleness-Centric Optimizations for Parallel Diffusion MoE Inference](https://openaccess.thecvf.com/content/ICCV2025/papers/Luo_DICE_Staleness-Centric_Optimizations_for_Parallel_Diffusion_MoE_Inference_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Cobalt-27/DICE)
* [GameFactory Creating New Games with Generative Interactive Videos](http://arxiv.org/abs/2501.08325)
* [SA-LUT Spatial Adaptive 4D Look-Up Table for Photorealistic Style Transfer](https://openaccess.thecvf.com/content/ICCV2025/papers/Gong_SA-LUT_Spatial_Adaptive_4D_Look-Up_Table_for_Photorealistic_Style_Transfer_ICCV_2025_paper.pdf)
* [SparseVILA Decoupling Visual Sparsity for Efficient VLM Inference](https://openaccess.thecvf.com/content/ICCV2025/papers/Khaki_SparseVILA_Decoupling_Visual_Sparsity_for_Efficient_VLM_Inference_ICCV_2025_paper.pdf)
* [Trust but Verify Programmatic VLM Evaluation in the Wild](http://arxiv.org/abs/2410.13121)<br>:house:[project](https://prove-explorer-anon.netlify.app/)
* [GTR Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training](http://arxiv.org/abs/2503.08525)
* [Token-Efficient VLM High-Resolution Image Understanding via Dynamic Region Proposal](https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal_ICCV_2025_paper.pdf)
* [TerraMind Large-Scale Generative Multimodality for Earth Observation](http://arxiv.org/abs/2504.11171)<br>:star:[code](https://github.com/ibm/terramind)
* [MUSE-VL Modeling Unified VLM through Semantic Discrete Encoding](https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_MUSE-VL_Modeling_Unified_VLM_through_Semantic_Discrete_Encoding_ICCV_2025_paper.pdf)
* [Radiant Foam Real-Time Differentiable Ray Tracing](http://arxiv.org/abs/2502.01157)
* [Attention to the Burstiness in Visual Prompt Tuning](http://arxiv.org/abs/2506.22908)
* [Enhancing Transformers Through Conditioned Embedded Tokens](http://arxiv.org/abs/2505.12789)
* [RALoc Enhancing Outdoor LiDAR Localization via Rotation Awareness](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness_ICCV_2025_paper.pdf)
* [HouseTour A Virtual Real Estate A(I)gent](https://openaccess.thecvf.com/content/ICCV2025/papers/Celen_HouseTour_A_Virtual_Real_Estate_AIgent_ICCV_2025_paper.pdf)
* [Towards Performance Consistency in Multi-Level Model Collaboration](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Towards_Performance_Consistency_in_Multi-Level_Model_Collaboration_ICCV_2025_paper.pdf)
* [Polarimetric Neural Field via Unified Complex-Valued Wave Representation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_Polarimetric_Neural_Field_via_Unified_Complex-Valued_Wave_Representation_ICCV_2025_paper.pdf)
* [Visual Intention Grounding for Egocentric Assistants](http://arxiv.org/abs/2504.13621)
* [TinyViM Frequency Decoupling for Tiny Hybrid Vision Mamba](http://arxiv.org/abs/2411.17473)<br>:star:[code](https://github.com/xwmaxwma/TinyViM)
* [A Recipe for Generating 3D Worlds from a Single Image](https://openaccess.thecvf.com/content/ICCV2025/papers/Schwarz_A_Recipe_for_Generating_3D_Worlds_from_a_Single_Image_ICCV_2025_paper.pdf)
* [Social Debiasing for Fair Multi-modal LLMs](http://arxiv.org/abs/2408.06569)
* [DOGR Towards Versatile Visual Document Grounding and Referring](http://arxiv.org/abs/2411.17125)<br>:star:[code](https://github.com/zyinan99/DOGR)
* [Meta-Learning Dynamic Center Distance Hard Sample Mining for Learning with Noisy Labels](https://openaccess.thecvf.com/content/ICCV2025/papers/Mu_Meta-Learning_Dynamic_Center_Distance_Hard_Sample_Mining_for_Learning_with_ICCV_2025_paper.pdf)
* [AstroLoc Robust Space to Ground Image Localizer](http://arxiv.org/abs/2502.07003)
* [Chimera Improving Generalist Model with Domain-Specific Experts](http://arxiv.org/abs/2412.05983)
* [Less is More Empowering GUI Agent with Context-Aware Simplification](http://arxiv.org/abs/2507.03730)
* [Learning Hierarchical Line Buffer for Image Processing](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Learning_Hierarchical_Line_Buffer_for_Image_Processing_ICCV_2025_paper.pdf)
* [Find Any Part in 3D](http://arxiv.org/abs/2411.13550)<br>:house:[project](https://ziqi-ma.github.io/find3dsite/) :house:[project](https://ziqi-ma.github.io/find3dsite)
* [VA-MoE Variables-Adaptive Mixture of Experts for Incremental Weather Forecasting](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_VA-MoE_Variables-Adaptive_Mixture_of_Experts_for_Incremental_Weather_Forecasting_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/chenhao-zju/VAMoE)
* [Aether Geometric-Aware Unified World Modeling](http://arxiv.org/abs/2503.18945)
* [Any2AnyTryon Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks](http://arxiv.org/abs/2501.15891)
* [Met2Net A Decoupled Two-Stage Spatio-Temporal Forecasting Model for Complex Meteorological Systems](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Met2Net_A_Decoupled_Two-Stage_Spatio-Temporal_Forecasting_Model_for_Complex_Meteorological_ICCV_2025_paper.pdf)
* [Memory-Efficient 4-bit Preconditioned Stochastic Optimization](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Memory-Efficient_4-bit_Preconditioned_Stochastic_Optimization_ICCV_2025_paper.pdf)
* [On the Recovery of Cameras from Fundamental Matrices](https://openaccess.thecvf.com/content/ICCV2025/papers/Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices_ICCV_2025_paper.pdf)
* [Visual-RFT Visual Reinforcement Fine-Tuning](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Visual-RFT_Visual_Reinforcement_Fine-Tuning_ICCV_2025_paper.pdf)
* [E-SAM Training-Free Segment Every Entity Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model_ICCV_2025_paper.pdf)
* [SpatialSplat Efficient Semantic 3D from Sparse Unposed Images](http://arxiv.org/abs/2505.23044)
* [Is Less More Exploring Token Condensation as Training-free Test-time Adaptation](http://arxiv.org/abs/2410.14729)<br>:star:[code](https://github.com/Jo-wang/TCA)
* [CVPT Cross Visual Prompt Tuning](http://arxiv.org/abs/2408.14961)<br>:star:[code](https://github.com/Lingyun0419/CVPT)
* [Unfolding-Associative Encoder-Decoder Network with Progressive Alignment for Pansharpening](https://openaccess.thecvf.com/content/ICCV2025/papers/Fang_Unfolding-Associative_Encoder-Decoder_Network_with_Progressive_Alignment_for_Pansharpening_ICCV_2025_paper.pdf)
* [CAD-Assistant Tool-Augmented VLLMs as Generic CAD Task Solvers](https://openaccess.thecvf.com/content/ICCV2025/papers/Mallis_CAD-Assistant_Tool-Augmented_VLLMs_as_Generic_CAD_Task_Solvers_ICCV_2025_paper.pdf)
* [HERMES temporal-coHERent long-forM understanding with Episodes and Semantics](http://arxiv.org/abs/2408.17443)<br>:house:[project](https://joslefaure.github.io/assets)
* [ImHead A Large-scale Implicit Morphable Model for Localized Head Modeling](http://arxiv.org/abs/2510.10793)
* [Sim-DETR Unlock DETR for Temporal Sentence Grounding](https://openaccess.thecvf.com/content/ICCV2025/papers/Tang_Sim-DETR_Unlock_DETR_for_Temporal_Sentence_Grounding_ICCV_2025_paper.pdf)
* [Wavelet Policy Lifting Scheme for Policy Learning in Long-Horizon Tasks](http://arxiv.org/abs/2507.04331)
* [SeqGrowGraph Learning Lane Topology as a Chain of Graph Expansions](http://arxiv.org/abs/2507.04822)
* [CARP Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction](http://arxiv.org/abs/2412.06782)
* [Transformer-based Tooth Alignment Prediction with Occlusion and Collision Constraints](http://arxiv.org/abs/2410.20806)<br>:house:[project](https://californiachen.github.io/publications)
* [Multi-modal Identity Extraction](https://openaccess.thecvf.com/content/ICCV2025/papers/Webster_Multi-modal_Identity_Extraction_ICCV_2025_paper.pdf)
* [Arti-PG A Toolbox for Procedurally Synthesizing Large-Scale and Diverse Articulated Objects with Rich Annotations](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Arti-PG_A_Toolbox_for_Procedurally_Synthesizing_Large-Scale_and_Diverse_Articulated_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Analytic-Concept-Group/ArtiPG)
* [FlowChef Steering of Rectified Flow Models for Controlled Generations](https://openaccess.thecvf.com/content/ICCV2025/papers/Patel_FlowChef_Steering_of_Rectified_Flow_Models_for_Controlled_Generations_ICCV_2025_paper.pdf)<br>:house:[project](https://flowchef.github.io/)
* [Semantic Equitable Clustering A Simple and Effective Strategy for Clustering Vision Tokens](http://arxiv.org/abs/2405.13337)
* [Towards Safer and Understandable Driver Intention Prediction](http://arxiv.org/abs/2510.09200)
* [AffordDexGrasp Open-set Language-guided Dexterous Grasp with Generalizable-Instructive Affordance](http://arxiv.org/abs/2503.07360)
* [Beyond the Limits Overcoming Negative Correlation of Activation-Based Training-Free NAS](https://openaccess.thecvf.com/content/ICCV2025/papers/Kang_Beyond_the_Limits_Overcoming_Negative_Correlation_of_Activation-Based_Training-Free_NAS_ICCV_2025_paper.pdf)
* [Cross-Subject Mind Decoding from Inaccurate Representations](http://arxiv.org/abs/2507.19071)
* [UINavBench A Framework for Comprehensive Evaluation of Interactive Digital Agents](https://openaccess.thecvf.com/content/ICCV2025/papers/Agrawal_UINavBench_A_Framework_for_Comprehensive_Evaluation_of_Interactive_Digital_Agents_ICCV_2025_paper.pdf)
* [Knowledge Transfer from Interaction Learning](http://arxiv.org/abs/2509.18733)
* [GEOPARD Geometric Pretraining for Articulation Prediction in 3D Shapes](http://arxiv.org/abs/2504.02747)
* [ARMO Autoregressive Rigging for Multi-Category Objects](http://arxiv.org/abs/2503.20663)
* [HyperGCT A Dynamic Hyper-GNN-Learned Geometric Constraint for 3D Registration](http://arxiv.org/abs/2503.02195)
* [Improving Rectified Flow with Boundary Conditions](http://arxiv.org/abs/2506.15864)
* [Clink Chop Thud - Learning Object Sounds from Real-World Interactions](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_Clink_Chop_Thud_-_Learning_Object_Sounds_from_Real-World_Interactions_ICCV_2025_paper.pdf)
* [ISP2HRNet Learning to Reconstruct High Resolution Image from Irregularly Sampled Pixels via Hierarchical Gradient Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/yuanlinwang/ISP2HRNet)
* [Decoupled Multi-Predictor Optimization for Inference-Efficient Model Tuning](https://openaccess.thecvf.com/content/ICCV2025/papers/Luo_Decoupled_Multi-Predictor_Optimization_for_Inference-Efficient_Model_Tuning_ICCV_2025_paper.pdf)
* [GARF Learning Generalizable 3D Reassembly for Real-World Fractures](http://arxiv.org/abs/2504.05400)
* [Parameter-Efficient Adaptation of Geospatial Foundation Models through Embedding Deflection](http://arxiv.org/abs/2503.09493)<br>:star:[code](https://github.com/VMarsocci/DEFLECT)
* [MOSCATO Predicting Multiple Object State Change Through Actions](https://openaccess.thecvf.com/content/ICCV2025/papers/Zameni_MOSCATO_Predicting_Multiple_Object_State_Change_Through_Actions_ICCV_2025_paper.pdf)
* [GCAV A Global Concept Activation Vector Framework for Cross-Layer Consistency in Interpretability](http://arxiv.org/abs/2508.21197)<br>:star:[code](https://github.com/Zhenghao-He/GCAV)
* [ModSkill Physical Character Skill Modularization](http://arxiv.org/abs/2502.14140)
* [DAMap Distance-aware MapNet for High Quality HD Map Construction](https://openaccess.thecvf.com/content/ICCV2025/papers/Dong_DAMap_Distance-aware_MapNet_for_High_Quality_HD_Map_Construction_ICCV_2025_paper.pdf)
* [Text2VDM Text to Vector Displacement Maps for Expressive and Interactive 3D Sculpting](http://arxiv.org/abs/2502.20045)
* [SuperMat Physically Consistent PBR Material Estimation at Interactive Rates](http://arxiv.org/abs/2411.17515)<br>:house:[project](https://hyj542682306.github.io/SuperMat)
* [Deep Adaptive Unfolded Network via Spatial Morphology Stripping and Spectral Filtration for Pan-sharpening](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Deep_Adaptive_Unfolded_Network_via_Spatial_Morphology_Stripping_and_Spectral_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Baixuzx7/DAPNet)
* [Learning Normal Flow Directly From Events](http://arxiv.org/abs/2412.11284)<br>:star:[code](https://github.com/dhyuan99/VecKM_flow)
* [FuXi-RTM A Physics-Guided Prediction Framework with Radiative Transfer Modeling](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling_ICCV_2025_paper.pdf)
* [Enhanced Pansharpening via Quaternion Spatial-Spectral Interactions](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Enhanced_Pansharpening_via_Quaternion_Spatial-Spectral_Interactions_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/dongli8/QuatPanNet)
* [From Holistic to Localized Local Enhanced Adapters for Efficient Visual Instruction Fine-Tuning](http://arxiv.org/abs/2411.12787)
* [Spectral Image Tokenizer](http://arxiv.org/abs/2412.09607)
* [ObjectRelator Enabling Cross-View Object Relation Understanding Across Ego-Centric and Exo-Centric Perspectives](http://arxiv.org/abs/2411.19083)
* [Mitigating Object Hallucinations via Sentence-Level Early Intervention](http://arxiv.org/abs/2507.12455)<br>:star:[code](https://github.com/pspdada/SENTINEL)
* [NETracer A Topology-Aware Iterative Tracing Approach for Tubular Structure Extraction](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_NETracer_A_Topology-Aware_Iterative_Tracing_Approach_for_Tubular_Structure_Extraction_ICCV_2025_paper.pdf)
* [SRefiner Soft-Braid Attention for Multi-Agent Trajectory Refinement](http://arxiv.org/abs/2507.04263)
* [Planar Affine Rectification from Local Change of Scale and Orientation](https://openaccess.thecvf.com/content/ICCV2025/papers/Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation_ICCV_2025_paper.pdf)
* [Co-Painter Fine-Grained Controllable Image Stylization via Implicit Decoupling and Adaptive Injection](https://openaccess.thecvf.com/content/ICCV2025/papers/Fu_Co-Painter_Fine-Grained_Controllable_Image_Stylization_via_Implicit_Decoupling_and_Adaptive_ICCV_2025_paper.pdf)
* [TAR3D Creating High-Quality 3D Assets via Next-Part Prediction](http://arxiv.org/abs/2412.16919)
* [MeasureXpert Automatic Anthropometric Measurement Extraction from Two Unregistered Partial Posed and Dressed Body Scans](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_MeasureXpert_Automatic_Anthropometric_Measurement_Extraction_from_Two_Unregistered_Partial_Posed_ICCV_2025_paper.pdf)
* [AIRA Activation-Informed Low-Rank Adaptation for Large Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_AIRA_Activation-Informed_Low-Rank_Adaptation_for_Large_Models_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/lliai/LoRA-Zoo)
* [Generating Physically Stable and Buildable Brick Structures from Text](http://arxiv.org/abs/2505.05469)<br>:house:[project](https://avalovelace1.github.io/BrickGPT) :house:[project](https://avalovelace1.github.io/BrickGPT/)
* [3D Test-time Adaptation via Graph Spectral Driven Point Shift](http://arxiv.org/abs/2507.18225)
* [PlaceIt3D Language-Guided Object Placement in Real 3D Scenes](http://arxiv.org/abs/2505.05288)
* [TR-PTS Task-Relevant Parameter and Token Selection for Efficient Tuning](https://openaccess.thecvf.com/content/ICCV2025/papers/Luo_TR-PTS_Task-Relevant_Parameter_and_Token_Selection_for_Efficient_Tuning_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/synbol/TR-PTS)
* [Addressing Representation Collapse in Vector Quantized Models with One Linear Layer](http://arxiv.org/abs/2411.02038)
* [Jigsaw Imagining Complete Shape Priors for Object Reassembly](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_Jigsaw_Imagining_Complete_Shape_Priors_for_Object_Reassembly_ICCV_2025_paper.pdf)
* [WildSeg3D Segment Any 3D Objects in the Wild from 2D Images](http://arxiv.org/abs/2503.08407)<br>:star:[code](https://github.com/Ethan16162/WildSeg3D)
* [INSTINCT Instance-Level Interaction Architecture for Query-Based Collaborative Perception](http://arxiv.org/abs/2509.23700)<br>:star:[code](https://github.com/CrazyShout/INSTINCT)
* [AnimeGamer Infinite Anime Life Simulation with Next Game State Prediction](http://arxiv.org/abs/2504.01014)
* [UIPro Unleashing Superior Interaction Capability For GUI Agents](http://arxiv.org/abs/2509.17328)
* [O-MaMa Learning Object Mask Matching between Egocentric and Exocentric Views](https://openaccess.thecvf.com/content/ICCV2025/papers/Mur-Labadia_O-MaMa_Learning_Object_Mask_Matching_between_Egocentric_and_Exocentric_Views_ICCV_2025_paper.pdf)
* [EvRT-DETR Latent Space Adaptation of Image Detectors for Event-based Vision](https://openaccess.thecvf.com/content/ICCV2025/papers/Torbunov_EvRT-DETR_Latent_Space_Adaptation_of_Image_Detectors_for_Event-based_Vision_ICCV_2025_paper.pdf)
* [LIFT Latent Implicit Functions for Task- and Data-Agnostic Encoding](https://openaccess.thecvf.com/content/ICCV2025/papers/Kazerouni_LIFT_Latent_Implicit_Functions_for_Task-_and_Data-Agnostic_Encoding_ICCV_2025_paper.pdf)
* [Manual-PA Learning 3D Part Assembly from Instruction Diagrams](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Manual-PA_Learning_3D_Part_Assembly_from_Instruction_Diagrams_ICCV_2025_paper.pdf)
* [PLMP - Point-Line Minimal Problems for Projective SfM](https://openaccess.thecvf.com/content/ICCV2025/papers/Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM_ICCV_2025_paper.pdf)
* [Coordinate-based Speed of Sound Recovery for Aberration-Corrected Photoacoustic Computed Tomography](http://arxiv.org/abs/2409.10876)<br>:house:[project](https://lukeli0425.github.io/Coord-SoS-PACT) :house:[project](https://lukeli0425.github.io/Coord-SoS-PACT/)
* [Spatio-Spectral Pattern Illumination for Direct and Indirect Separation from a Single Hyperspectral Image](https://openaccess.thecvf.com/content/ICCV2025/papers/Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a_ICCV_2025_paper.pdf)
* [Neural Solver of Dichromatic Reflection Model for Specular Highlight Removal](https://openaccess.thecvf.com/content/ICCV2025/papers/Fu_Neural_Solver_of_Dichromatic_Reflection_Model_for_Specular_Highlight_Removal_ICCV_2025_paper.pdf)
* [Att-Adapter A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder](https://openaccess.thecvf.com/content/ICCV2025/papers/Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter_ICCV_2025_paper.pdf)
* [CE-FAM Concept-Based Explanation via Fusion of Activation Maps](https://openaccess.thecvf.com/content/ICCV2025/papers/Kuroki_CE-FAM_Concept-Based_Explanation_via_Fusion_of_Activation_Maps_ICCV_2025_paper.pdf)
* [Recover Biological Structure from Sparse-View Diffraction Images with Neural Volumetric Prior](https://openaccess.thecvf.com/content/ICCV2025/papers/He_Recover_Biological_Structure_from_Sparse-View_Diffraction_Images_with_Neural_Volumetric_ICCV_2025_paper.pdf)
* [EgoAgent A Joint Predictive Agent Model in Egocentric Worlds](http://arxiv.org/abs/2502.05857)<br>:star:[code](https://github.com/zju3dv/EgoAgent)
* [Controllable Latent Space Augmentation for Digital Pathology](http://arxiv.org/abs/2508.14588)
* [Straighten Viscous Rectified Flow via Noise Optimization](http://arxiv.org/abs/2507.10218)
* [SL2A-INR Single-Layer Learnable Activation for Implicit Neural Representation](https://openaccess.thecvf.com/content/ICCV2025/papers/Rezaeian_SL2A-INR_Single-Layer_Learnable_Activation_for_Implicit_Neural_Representation_ICCV_2025_paper.pdf)
* [LLaVA-3D A Simple yet Effective Pathway to Empowering LMMs with 3D Capabilities](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_LLaVA-3D_A_Simple_yet_Effective_Pathway_to_Empowering_LMMs_with_ICCV_2025_paper.pdf)
* [RESCUE Crowd Evacuation Simulation via Controlling SDM-United Characters](http://arxiv.org/abs/2507.20117)
* [WonderTurbo Generating Interactive 3D World in 072 Seconds](https://openaccess.thecvf.com/content/ICCV2025/papers/Ni_WonderTurbo_Generating_Interactive_3D_World_in_0.72_Seconds_ICCV_2025_paper.pdf)
* [Mastering Collaborative Multi-modal Data Selection A Focus on Informativeness Uniqueness and Representativeness](http://arxiv.org/abs/2412.06293)
* [PanoLlama Generating Endless and Coherent Panoramas with Next-Token-Prediction LLMs](http://arxiv.org/abs/2411.15867)
* [Dense Policy Bidirectional Autoregressive Learning of Actions](http://arxiv.org/abs/2503.13217)
* [Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating](http://arxiv.org/abs/2508.00330)
* [Removing Out-of-Focus Reflective Flares via Color Alignment](https://openaccess.thecvf.com/content/ICCV2025/papers/Lan_Removing_Out-of-Focus_Reflective_Flares_via_Color_Alignment_ICCV_2025_paper.pdf)
* [CuMPerLay Learning Cubical Multiparameter Persistence Vectorizations](https://openaccess.thecvf.com/content/ICCV2025/papers/Korkmaz_CuMPerLay_Learning_Cubical_Multiparameter_Persistence_Vectorizations_ICCV_2025_paper.pdf)
* [Can Generative Geospatial Diffusion Models Excel as Discriminative Geospatial Foundation Models](http://arxiv.org/abs/2503.07890)<br>:star:[code](https://github.com/yurujaja/SatDiFuser)
* [Improved Noise Schedule for Diffusion Training](https://openaccess.thecvf.com/content/ICCV2025/papers/Hang_Improved_Noise_Schedule_for_Diffusion_Training_ICCV_2025_paper.pdf)
* [Sculpting Memory Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization](http://arxiv.org/abs/2504.09039)<br>:star:[code](https://github.com/coulsonlee/Sculpting-Memory-ICCV-2025)
* [From Reusing to Forecasting Accelerating Diffusion Models with TaylorSeers](http://arxiv.org/abs/2503.06923)<br>:star:[code](https://github.com/Shenyi-Z/TaylorSeer)
* [Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography](http://arxiv.org/abs/2506.22753)<br>:house:[project](https://dmdiff.github.io/)
* [MamTiff-CAD Multi-Scale Latent Diffusion with Mamba for Complex Parametric Sequence](https://openaccess.thecvf.com/content/ICCV2025/papers/Deng_MamTiff-CAD_Multi-Scale_Latent_Diffusion_with_Mamba_for_Complex_Parametric_Sequence_ICCV_2025_paper.pdf)
* [Rethinking DPO-style Diffusion Aligning Frameworks](https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks_ICCV_2025_paper.pdf)
* [Unified Multi-Agent Trajectory Modeling with Masked Trajectory Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_Unified_Multi-Agent_Trajectory_Modeling_with_Masked_Trajectory_Diffusion_ICCV_2025_paper.pdf)
* [Understanding Flatness in Generative Models Its Role and Benefits](http://arxiv.org/abs/2503.11078)
* [SliderSpace Decomposing the Visual Capabilities of Diffusion Models](http://arxiv.org/abs/2502.01639)
* [FreeScale Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion](http://arxiv.org/abs/2412.09626)
* [Representing 3D Shapes with 64 Latent Vectors for 3D Diffusion Models](http://arxiv.org/abs/2503.08737)
* [End-to-End Multi-Modal Diffusion Mamba](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_End-to-End_Multi-Modal_Diffusion_Mamba_ICCV_2025_paper.pdf)
* [Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction](https://openaccess.thecvf.com/content/ICCV2025/papers/Cong_Guiding_Noisy_Label_Conditional_Diffusion_Models_with_Score-based_Discriminator_Correction_ICCV_2025_paper.pdf)
* [Make Me Happier Evoking Emotions Through Image Diffusion Models](http://arxiv.org/abs/2403.08255)
* [Guiding Diffusion Models with Adaptive Negative Sampling Without External Resources](https://openaccess.thecvf.com/content/ICCV2025/papers/Desai_Guiding_Diffusion_Models_with_Adaptive_Negative_Sampling_Without_External_Resources_ICCV_2025_paper.pdf)
* [Differentially Private Fine-Tuning of Diffusion Models](http://arxiv.org/abs/2406.01355)<br>:star:[code](https://github.com/EzzzLi/DP-LORA)
* [One-Step Specular Highlight Removal with Adapted Diffusion Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Atmis_One-Step_Specular_Highlight_Removal_with_Adapted_Diffusion_Models_ICCV_2025_paper.pdf)
* [Entropy-Adaptive Diffusion Policy Optimization with Dynamic Step Alignment](https://openaccess.thecvf.com/content/ICCV2025/papers/Yan_Entropy-Adaptive_Diffusion_Policy_Optimization_with_Dynamic_Step_Alignment_ICCV_2025_paper.pdf)
* [Beyond Blur A Fluid Perspective on Generative Diffusion Models](http://arxiv.org/abs/2506.16827)
* [ConceptSplit Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement](http://arxiv.org/abs/2510.04668)<br>:star:[code](https://github.com/KU-VGI/ConceptSplit)
* [IMG Calibrating Diffusion Models via Implicit Multimodal Guidance](http://arxiv.org/abs/2509.26231)<br>:star:[code](https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment)
* [PEFTDiff Diffusion-Guided Transferability Estimation for Parameter-Efficient Fine-Tuning](https://openaccess.thecvf.com/content/ICCV2025/papers/Khoba_PEFTDiff_Diffusion-Guided_Transferability_Estimation_for_Parameter-Efficient_Fine-Tuning_ICCV_2025_paper.pdf)
* [HiGarment Cross-modal Harmony Based Diffusion Model for Flat Sketch to Realistic Garment Image](http://arxiv.org/abs/2505.23186)<br>:star:[code](https://github.com/Maple498/HiGarment)
* [TREAD Token Routing for Efficient Architecture-agnostic Diffusion Training](http://arxiv.org/abs/2501.04765)
* [DiMO Distilling Masked Diffusion Models into One-step Generator](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_DiMO_Distilling_Masked_Diffusion_Models_into_One-step_Generator_ICCV_2025_paper.pdf)
* [An Inversion-based Measure of Memorization for Diffusion Models](http://arxiv.org/abs/2405.05846)
* [Adding Additional Control to One-Step Diffusion with Joint Distribution Matching](http://arxiv.org/abs/2503.06652)
* [PLADIS Pushing the Limits of Attention in Diffusion Models at Inference Time by Leveraging Sparsity](http://arxiv.org/abs/2503.07677)<br>:star:[code](https://github.com/cubeyoung/PLADIS) :house:[project](https://github.com/cubeyoung/PLADIS)
* [X-Prompt Generalizable Auto-Regressive Visual Learning with In-Context Prompting](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_X-Prompt_Generalizable_Auto-Regressive_Visual_Learning_with_In-Context_Prompting_ICCV_2025_paper.pdf)
* [X-Fusion Introducing New Modality to Frozen Large Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Mo_X-Fusion_Introducing_New_Modality_to_Frozen_Large_Language_Models_ICCV_2025_paper.pdf)
* [PRIMAL Physically Reactive and Interactive Motor Model for Avatar Learning](http://arxiv.org/abs/2503.17544)
* [Toward Material-Agnostic System Identification from Videos](http://arxiv.org/abs/2508.01112)
* [TransiT Transient Transformer for Non-line-of-sight Videography](http://arxiv.org/abs/2503.11328)
* [An Empirical Study of Autoregressive Pre-training from Videos](http://arxiv.org/abs/2501.05453)
* [Open-World Skill Discovery from Unsegmented Demonstration Videos](https://openaccess.thecvf.com/content/ICCV2025/papers/Deng_Open-World_Skill_Discovery_from_Unsegmented_Demonstration_Videos_ICCV_2025_paper.pdf)<br>:house:[project](https://craftjarvis.github.io/SkillDiscovery)
* [Error Recognition in Procedural Videos using Generalized Task Graph](https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_Error_Recognition_in_Procedural_Videos_using_Generalized_Task_Graph_ICCV_2025_paper.pdf)
* [From Gallery to Wrist Realistic 3D Bracelet Insertion in Videos](http://arxiv.org/abs/2507.20331)
* [Synchronization of Multiple Videos](https://openaccess.thecvf.com/content/ICCV2025/papers/Naaman_Synchronization_of_Multiple_Videos_ICCV_2025_paper.pdf)<br>:house:[project](https://bgu-cs-vil.github.io/TPL)
* [Vamba Understanding Hour-Long Videos with Hybrid Mamba-Transformers](http://arxiv.org/abs/2503.11579)
* [VideoOrion Tokenizing Object Dynamics in Videos](http://arxiv.org/abs/2411.16156)
* [Snakes and Ladders Two Steps Up for VideoMamba](http://arxiv.org/abs/2406.19006)
* [ViSpeak Visual Instruction Feedback in Streaming Videos](http://arxiv.org/abs/2503.12769)
* [Layer-wise Vision Injection with Disentangled Attention for Efficient LVLMs](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Layer-wise_Vision_Injection_with_Disentangled_Attention_for_Efficient_LVLMs_ICCV_2025_paper.pdf)<br>:house:[project](https://xuange923.github.io/LVIDA/) :house:[project](https://xuange923.github.io/LVIDA)
* [Teaching VLMs to Localize Specific Objects from In-context Examples](http://arxiv.org/abs/2411.13317)
* [PS3 A Multimodal Transformer Integrating Pathology Reports with Histology Images and Biological Pathways for Cancer Survival Prediction](http://arxiv.org/abs/2509.20022)
* [Benchmarking Multimodal CoT Reward Model Stepwise by Visual Program](http://arxiv.org/abs/2504.06606)
* [Exploring The Visual Feature Space for Multimodal Neural Decoding](http://arxiv.org/abs/2505.15755)
* [InfoBridge Balanced Multimodal Integration through Conditional Dependency Modeling](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_InfoBridge_Balanced_Multimodal_Integration_through_Conditional_Dependency_Modeling_ICCV_2025_paper.pdf)<br>:house:[project](https://cuhk-aim-group.github.io/InfoBridge/) :house:[project](https://cuhk-aim-group.github.io/InfoBridge)
* [Switch-a-View View Selection Learned from Unlabeled In-the-wild Videos](https://openaccess.thecvf.com/content/ICCV2025/papers/Majumder_Switch-a-View_View_Selection_Learned_from_Unlabeled_In-the-wild_Videos_ICCV_2025_paper.pdf)
* [VideoMiner Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization](http://arxiv.org/abs/2510.06040)<br>:star:[code](https://github.com/caoxinye/VideoMiner)
* [Beyond the Frame Generating 360deg Panoramic Videos from Perspective Videos](https://openaccess.thecvf.com/content/ICCV2025/papers/Luo_Beyond_the_Frame_Generating_360deg_Panoramic_Videos_from_Perspective_Videos_ICCV_2025_paper.pdf)
* [YOLOE Real-Time Seeing Anything](http://arxiv.org/abs/2503.07465)<br>:star:[code](https://github.com/THU-MIG/yoloe)
* [CAFA a Controllable Automatic Foley Artist](http://arxiv.org/abs/2504.06778)
* [Neuroverse3D Developing In-Context Learning Universal Model for Neuroimaging in 3D](http://arxiv.org/abs/2503.02410)
* [COSTARR Consolidated Open Set Technique with Attenuation for Robust Recognition](http://arxiv.org/abs/2508.01087)
* [Agreement aware and dissimilarity oriented GLOM](https://openaccess.thecvf.com/content/ICCV2025/papers/Zeng_Agreement_aware_and_dissimilarity_oriented_GLOM_ICCV_2025_paper.pdf)
* [Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer](http://arxiv.org/abs/2508.14187)
* [SP2T Sparse Proxy Attention for Dual-stream Point Transformer](https://openaccess.thecvf.com/content/ICCV2025/papers/Wan_SP2T_Sparse_Proxy_Attention_for_Dual-stream_Point_Transformer_ICCV_2025_paper.pdf)
* [Hallucinatory Image Tokens A Training-free EAZY Approach to Detecting and Mitigating Object Hallucinations in LVLMs](https://openaccess.thecvf.com/content/ICCV2025/papers/Che_Hallucinatory_Image_Tokens_A_Training-free_EAZY_Approach_to_Detecting_and_ICCV_2025_paper.pdf)
* [BabyVLM Data-Efficient Pretraining of VLMs Inspired by Infant Learning](http://arxiv.org/abs/2504.09426)
* [R1-Onevision Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_R1-Onevision_Advancing_Generalized_Multimodal_Reasoning_through_Cross-Modal_Formalization_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Fancy-MLLM/R1-Onevision)
* [EgoM2P Egocentric Multimodal Multitask Pretraining](http://arxiv.org/abs/2506.07886)
* [Scaling Laws for Native Multimodal Models](http://arxiv.org/abs/2504.07951)
* [Generate Transduct Adapt Iterative Transduction with VLMs](http://arxiv.org/abs/2501.06031)
* [LLaVA-PruMerge Adaptive Token Reduction for Efficient Large Multimodal Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Shang_LLaVA-PruMerge_Adaptive_Token_Reduction_for_Efficient_Large_Multimodal_Models_ICCV_2025_paper.pdf)
* [HRScene How Far Are VLMs from Effective High-Resolution Image Understanding](http://arxiv.org/abs/2504.18406)
* [Unified Multimodal Understanding via Byte-Pair Visual Encoding](http://arxiv.org/abs/2506.23639)
* [FinMMR Make Financial Numerical Reasoning More Multimodal Comprehensive and Challenging](http://arxiv.org/abs/2508.04625)
* [Efficient Visual Place Recognition Through Multimodal Semantic Knowledge Integration](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Efficient_Visual_Place_Recognition_Through_Multimodal_Semantic_Knowledge_Integration_ICCV_2025_paper.pdf)
* [RMultiplex200K Toward Reliable Multimodal Process Supervision for Visual Language Models on Telecommunications](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_RMultiplex200K_Toward_Reliable_Multimodal_Process_Supervision_for_Visual_Language_Models_ICCV_2025_paper.pdf)
* [ERNet Efficient Non-Rigid Registration Network for Point Sequences](https://openaccess.thecvf.com/content/ICCV2025/papers/He_ERNet_Efficient_Non-Rigid_Registration_Network_for_Point_Sequences_ICCV_2025_paper.pdf)
* [Not all Views are Created Equal Analyzing Viewpoint Instabilities in Vision Foundation Models](http://arxiv.org/abs/2412.19920)
* [HumorDB Can AI understand graphical humor](http://arxiv.org/abs/2406.13564)
* [InfiniteYou Flexible Photo Recrafting While Preserving Your Identity](http://arxiv.org/abs/2503.16418)
* [Synchronizing Task Behavior Aligning Multiple Tasks during Test-Time Training](http://arxiv.org/abs/2507.07778)
* [Correspondence-Free Fast and Robust Spherical Point Pattern Registration](http://arxiv.org/abs/2508.02339)
* [Voyaging into Perpetual Dynamic Scenes from a Single View](http://arxiv.org/abs/2507.04183)<br>:house:[project](https://tianfr.github.io/DynamicVoyager)
* [C4D 4D Made from 3D through Dual Correspondences](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_C4D_4D_Made_from_3D_through_Dual_Correspondences_ICCV_2025_paper.pdf)
* [Soft Local Completeness Rethinking Completeness in XAI](https://openaccess.thecvf.com/content/ICCV2025/papers/Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI_ICCV_2025_paper.pdf)
* [When and Where do Data Poisons Attack Textual Inversion](http://arxiv.org/abs/2507.10578)<br>:star:[code](https://github.com/JStyborski/Diff_Lab) :star:[code2](https://github.com/JStyborski/NC10)
* [Quanta Neural Networks From Photons to Perception](https://openaccess.thecvf.com/content/ICCV2025/papers/Sundar_Quanta_Neural_Networks_From_Photons_to_Perception_ICCV_2025_paper.pdf)
* [Auto-Regressive Transformation for Image Alignment](http://arxiv.org/abs/2505.04864)
* [A Unified Framework to BRIDGE Complete and Incomplete Deep Multi-View Clustering under Non-IID Missing Patterns](https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_A_Unified_Framework_to_BRIDGE_Complete_and_Incomplete_Deep_Multi-View_ICCV_2025_paper.pdf)
* [CODA Repurposing Continuous VAEs for Discrete Tokenization](http://arxiv.org/abs/2503.17760)
* [AnyCalib On-Manifold Learning for Model-Agnostic Single-View Camera Calibration](https://openaccess.thecvf.com/content/ICCV2025/papers/Tirado-Garin_AnyCalib_On-Manifold_Learning_for_Model-Agnostic_Single-View_Camera_Calibration_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/javrtg/AnyCalib)
* [VAGUE Visual Contexts Clarify Ambiguous Expressions](http://arxiv.org/abs/2411.14137)
* [WIPES Wavelet-based Visual Primitives](http://arxiv.org/abs/2508.12615)
* [CanFields Consolidating Diffeomorphic Flows for Non-Rigid 4D Interpolation from Arbitrary-Length Sequences](http://arxiv.org/abs/2406.18582)<br>:house:[project](https://wangmiaowei.github.io/CanFields.github.io)
* [Towards Foundational Models for Single-Chip Radar](http://arxiv.org/abs/2509.12482)
* [VALLR Visual ASR Language Model for Lip Reading](http://arxiv.org/abs/2503.21408)
* [Enpowering Your Pansharpening Models with Generalizability Unified Distribution is All You Need](https://openaccess.thecvf.com/content/ICCV2025/papers/Cui_Enpowering_Your_Pansharpening_Models_with_Generalizability_Unified_Distribution_is_All_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/yc-cui/UniPAN)
* [Laboring on less labors RPCA Paradigm for Pan-sharpening](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_Laboring_on_less_labors_RPCA_Paradigm_for_Pan-sharpening_ICCV_2025_paper.pdf)
* [DiMPLe - Disentangled Multi-Modal Prompt Learning Enhancing Out-Of-Distribution Alignment with Invariant and Spurious Feature Separation](https://openaccess.thecvf.com/content/ICCV2025/papers/Rahman_DiMPLe_-_Disentangled_Multi-Modal_Prompt_Learning_Enhancing_Out-Of-Distribution_Alignment_with_ICCV_2025_paper.pdf)
* [From Panels to Prose Generating Literary Narratives from Comics](http://arxiv.org/abs/2503.23344)
* [SparseFlex High-Resolution and Arbitrary-Topology 3D Shape Modeling](http://arxiv.org/abs/2503.21732)
* [Lidar Waveforms are Worth 40x128x33 Words](https://openaccess.thecvf.com/content/ICCV2025/papers/Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words_ICCV_2025_paper.pdf)
* [Balanced Sharpness-Aware Minimization for Imbalanced Regression](http://arxiv.org/abs/2508.16973)
* [PRVQL Progressive Knowledge-guided Refinement for Robust Egocentric Visual Query Localization](http://arxiv.org/abs/2502.07707)<br>:star:[code](https://github.com/fb-reps/PRVQL)
* [Bayesian-Inspired Space-Time Superpixels](https://openaccess.thecvf.com/content/ICCV2025/papers/Gauen_Bayesian-Inspired_Space-Time_Superpixels_ICCV_2025_paper.pdf)
* [Hypergraph Clustering Network with Partial Attribute Imputation](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Hypergraph_Clustering_Network_with_Partial_Attribute_Imputation_ICCV_2025_paper.pdf)
* [GECO Geometrically Consistent Embedding with Lightspeed Inference](http://arxiv.org/abs/2508.00746)
* [LoftUp Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models](http://arxiv.org/abs/2504.14032)<br>:star:[code](https://github.com/andrehuang/loftup)
* [Taming Flow Matching with Unbalanced Optimal Transport into Fast Pansharpening](http://arxiv.org/abs/2503.14975)
* [LLaFEA Frame-Event Complementary Fusion for Fine-Grained Spatiotemporal Understanding in LMMs](http://arxiv.org/abs/2503.06934)
* [Beyond cls Exploring the True Potential of Masked Image Modeling Representations](https://openaccess.thecvf.com/content/ICCV2025/papers/Przewiezlikowski_Beyond_cls_Exploring_the_True_Potential_of_Masked_Image_Modeling_ICCV_2025_paper.pdf)
* [SpikePack Enhanced Information Flow in Spiking Neural Networks with High Hardware Compatibility](http://arxiv.org/abs/2501.14484)
* [After the Party Navigating the Mapping From Color to Ambient Lighting](http://arxiv.org/abs/2508.02168)<br>:star:[code](https://github.com/fvasluianu97/RLN2)
* [A Differentiable Wave Optics Model for End-to-End Computational Imaging System Optimization](http://arxiv.org/abs/2412.09774)<br>:star:[code](https://github.com/JerryHoTaiwan/DeepWaveOptics)
* [Efficient Fine-Tuning of Large Models via Nested Low-Rank Adaptation](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Efficient_Fine-Tuning_of_Large_Models_via_Nested_Low-Rank_Adaptation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/lliai/LoRA-Zoo)
* [SCAN Bootstrapping Contrastive Pre-training for Data Efficiency](http://arxiv.org/abs/2411.09126)<br>:star:[code](https://github.com/guoyang9/SCAN)
* [Visual Surface Wave Elastography Revealing Subsurface Physical Properties via Visible Surface Waves](http://arxiv.org/abs/2507.09207)
* [Discontinuity-aware Normal Integration for Generic Central Camera Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models_ICCV_2025_paper.pdf)
* [LazyMAR Accelerating Masked Autoregressive Models via Feature Caching](http://arxiv.org/abs/2503.12450)<br>:star:[code](https://github.com/feihongyan1/LazyMAR)
* [SAC-GNC SAmple Consensus for adaptive Graduated Non-Convexity](https://openaccess.thecvf.com/content/ICCV2025/papers/Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity_ICCV_2025_paper.pdf)
* [Generalized Tensor-based Parameter-Efficient Fine-Tuning via Lie Group Transformations](http://arxiv.org/abs/2504.00851)
* [Granular Concept Circuits Toward a Fine-Grained Circuit Discovery for Concept Representations](http://arxiv.org/abs/2508.01728)<br>:star:[code](https://github.com/daheekwon/GCC)
* [Efficient Event Camera Data Pretraining with Adaptive Prompt Fusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_Efficient_Event_Camera_Data_Pretraining_with_Adaptive_Prompt_Fusion_ICCV_2025_paper.pdf)
* [Augmented Mass-Spring Model for Real-Time Dense Hair Simulation](https://openaccess.thecvf.com/content/ICCV2025/papers/H._Augmented_Mass-Spring_Model_for_Real-Time_Dense_Hair_Simulation_ICCV_2025_paper.pdf)
* [EYE3Turn Anything into Naked-eye 3D](https://openaccess.thecvf.com/content/ICCV2025/papers/Song_EYE3Turn_Anything_into_Naked-eye_3D_ICCV_2025_paper.pdf)
* [Boosting Class Representation via Semantically Related Instances for Robust Long-Tailed Learning with Noisy Labels](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Boosting_Class_Representation_via_Semantically_Related_Instances_for_Robust_Long-Tailed_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/yhli-ml/IBC)
* [Fast Globally Optimal and Geometrically Consistent 3D Shape Matching](http://arxiv.org/abs/2504.06385)
* [Is CLIP ideal No Can we fix it Yes](http://arxiv.org/abs/2503.08723)<br>:star:[code](https://github.com/Raphoo/DCSM_Ideal_CLIP)
* [SpiLiFormer Enhancing Spiking Transformers with Lateral Inhibition](http://arxiv.org/abs/2503.15986)<br>:star:[code](https://github.com/KirinZheng/SpiLiFormer)
* [GroundingSuite Measuring Complex Multi-Granular Pixel Grounding](http://arxiv.org/abs/2503.10596)<br>:star:[code](https://github.com/hustvl/GroundingSuite)
* [TorchAdapt Towards Light-Agnostic Real-Time Visual Perception](https://openaccess.thecvf.com/content/ICCV2025/papers/Hashmi_TorchAdapt_Towards_Light-Agnostic_Real-Time_Visual_Perception_ICCV_2025_paper.pdf)
* [Edit360 2D Image Edits to 3D Assets from Any Angle](http://arxiv.org/abs/2506.10507)
* [RANKCLIP Ranking-Consistent Language-Image Pretraining](http://arxiv.org/abs/2404.09387)
* [Visual Test-time Scaling for GUI Agent Grounding](http://arxiv.org/abs/2505.00684)<br>:star:[code](https://github.com/tiangeluo/RegionFocus)
* [Neural Shell Texture Splatting More Details and Fewer Primitives](http://arxiv.org/abs/2507.20200)
* [SILO Solving Inverse Problems with Latent Operators](http://arxiv.org/abs/2501.11746)
* [High-Precision 3D Measurement of Complex Textured Surfaces Using Multiple Filtering Approach](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_High-Precision_3D_Measurement_of_Complex_Textured_Surfaces_Using_Multiple_Filtering_ICCV_2025_paper.pdf)
* [DALIP Distribution Alignment-based Language-Image Pre-Training for Domain-Specific Data](http://arxiv.org/abs/2504.01386)
* [mmCooper A Multi-agent Multi-stage Communication-efficient and Collaboration-robust Cooperative Perception Framework](http://arxiv.org/abs/2501.12263)
* [Scendi Score Prompt-Aware Diversity Evaluation via Schur Complement of CLIP Embeddings](http://arxiv.org/abs/2412.18645)<br>:star:[code](https://github.com/aziksh-ospanov/scendi-score)
* [Referring to Any Person](http://arxiv.org/abs/2503.08507)
* [More Reliable Pseudo-labels Better Performance A Generalized Approach to Single Positive Multi-label Learning](http://arxiv.org/abs/2508.20381)
* [TopicGeo An Efficient Unified Framework for Geolocation](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_TopicGeo_An_Efficient_Unified_Framework_for_Geolocation_ICCV_2025_paper.pdf)
* [PVMamba Parallelizing Vision Mamba via Dynamic State Aggregation](https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_PVMamba_Parallelizing_Vision_Mamba_via_Dynamic_State_Aggregation_ICCV_2025_paper.pdf)
* [Staining and Locking Computer Vision Models Without Retraining](http://arxiv.org/abs/2507.22000)
* [Time-Aware Auto White Balance in Mobile Photography](http://arxiv.org/abs/2504.05623)
* [Background Invariance Testing According to Semantic Proximity](http://arxiv.org/abs/2208.09286)
* [ViT-Split Unleashing the Power of Vision Foundation Models via Efficient Splitting Heads](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_ViT-Split_Unleashing_the_Power_of_Vision_Foundation_Models_via_Efficient_ICCV_2025_paper.pdf)
* [VITAL More Understandable Feature Visualization through Distribution Alignment and Relevant Information Flow](https://openaccess.thecvf.com/content/ICCV2025/papers/Gorgun_VITAL_More_Understandable_Feature_Visualization_through_Distribution_Alignment_and_Relevant_ICCV_2025_paper.pdf)
* [LATINO-PRO LAtent consisTency INverse sOlver with PRompt Optimization](https://openaccess.thecvf.com/content/ICCV2025/papers/Spagnoletti_LATINO-PRO_LAtent_consisTency_INverse_sOlver_with_PRompt_Optimization_ICCV_2025_paper.pdf)
* [Towards a Universal Image Degradation Model via Content-Degradation Disentanglement](http://arxiv.org/abs/2505.12860)
* [Temperature in Cosine-based Softmax Loss](https://openaccess.thecvf.com/content/ICCV2025/papers/Kobayashi_Temperature_in_Cosine-based_Softmax_Loss_ICCV_2025_paper.pdf)
* [GaussianProperty Integrating Physical Properties to 3D Gaussians with LMMs](http://arxiv.org/abs/2412.11258)
* [Auto-Regressively Generating Multi-View Consistent Images](http://arxiv.org/abs/2506.18527)<br>:star:[code](https://github.com/MILab-PKU/MVAR)
* [I Am Big You Are Little I Am Right You Are Wrong](http://arxiv.org/abs/2507.23509)
* [Spatially-Varying Autofocus](https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Spatially-Varying_Autofocus_ICCV_2025_paper.pdf)
* [LayerD Decomposing Raster Graphic Designs into Layers](http://arxiv.org/abs/2509.25134)
* [LangBridge Interpreting Image as a Combination of Language Embeddings](http://arxiv.org/abs/2503.19404)<br>:house:[project](https://curryx-001.github.io/LangBridge.github.io)
* [LLaVA-CoT Let Vision Language Models Reason Step-by-Step](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_LLaVA-CoT_Let_Vision_Language_Models_Reason_Step-by-Step_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/PKU-YuanGroup/LLaVA-CoT)
* [ETCH Generalizing Body Fitting to Clothed Humans via Equivariant Tightness](http://arxiv.org/abs/2503.10624)
* [FRET Feature Redundancy Elimination for Test Time Adaptation](http://arxiv.org/abs/2505.10641)
* [PlaneRAS Learning Planar Primitives for 3D Plane Recovery](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_PlaneRAS_Learning_Planar_Primitives_for_3D_Plane_Recovery_ICCV_2025_paper.pdf)
* [Certifiably Optimal Anisotropic Rotation Averaging](http://arxiv.org/abs/2503.07353)
* [LaCoOT Layer Collapse through Optimal Transport](https://openaccess.thecvf.com/content/ICCV2025/papers/Quetu_LaCoOT_Layer_Collapse_through_Optimal_Transport_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/VGCQ/LaCoOT)
* [Completing 3D Partial Assemblies with View-Consistent 2D-3D Correspondence](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Completing_3D_Partial_Assemblies_with_View-Consistent_2D-3D_Correspondence_ICCV_2025_paper.pdf)
* [SteerX Creating Any Camera-Free 3D and 4D Scenes with Geometric Steering](http://arxiv.org/abs/2503.12024)
* [Hierarchical Material Recognition from Local Appearance](http://arxiv.org/abs/2505.22911)
* [Mixture-of-Scores Robust Image-Text Data Valuation via Three Lines of Code](https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Mixture-of-Scores_Robust_Image-Text_Data_Valuation_via_Three_Lines_of_Code_ICCV_2025_paper.pdf)
* [DMesh An Efficient Differentiable Mesh for Complex Shapes](https://openaccess.thecvf.com/content/ICCV2025/papers/Son_DMesh_An_Efficient_Differentiable_Mesh_for_Complex_Shapes_ICCV_2025_paper.pdf)<br>:house:[project](https://sonsang.github.io/dmesh2-project)
* [Registration beyond Points General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold](http://arxiv.org/abs/2507.17998)<br>:star:[code](https://github.com/joomeok/GrassmannRegistration)
* [Online Language Splatting](http://arxiv.org/abs/2503.09447)
* [Lyra An Efficient and Speech-Centric Framework for Omni-Cognition](http://arxiv.org/abs/2412.09501)
* [Geometry Distributions](http://arxiv.org/abs/2411.16076)
* [Evading Data Provenance in Deep Neural Networks](http://arxiv.org/abs/2508.01074)
* [CA2C A Prior-Knowledge-Free Approach for Robust Label Noise Learning via Asymmetric Co-learning and Co-training](https://openaccess.thecvf.com/content/ICCV2025/papers/Sheng_CA2C_A_Prior-Knowledge-Free_Approach_for_Robust_Label_Noise_Learning_via_ICCV_2025_paper.pdf)
* [StyleKeeper Prevent Content Leakage using Negative Visual Query Guidance](http://arxiv.org/abs/2510.06827)
* [Faster and Better 3D Splatting via Group Training](http://arxiv.org/abs/2412.07608)<br>:house:[project](https://chengbo-wang.github.io/3DGS-with-Group-Training) :house:[project](https://chengbo-wang.github.io/3DGS-with-Group-Training/)
* [Event-based Visual Vibrometry](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_Event-based_Visual_Vibrometry_ICCV_2025_paper.pdf)
* [Robust Multi-View Learning via Representation Fusion of Sample-Level Attention and Alignment of Simulated Perturbation](http://arxiv.org/abs/2503.04151)<br>:star:[code](https://github.com/SubmissionsIn/RML)
* [VSSD Vision Mamba with Non-Causal State Space Duality](http://arxiv.org/abs/2407.18559)<br>:star:[code](https://github.com/YuHengsss/VSSD)
* [Know No Better A Data-Driven Approach for Enhancing Negation Awareness in CLIP](http://arxiv.org/abs/2501.10913)
* [MAVias Mitigate any Visual Bias](http://arxiv.org/abs/2412.06632)
* [Generalizable Non-Line-of-Sight Imaging with Learnable Physical Priors](http://arxiv.org/abs/2409.14011)<br>:star:[code](https://github.com/ssd1051/NLOS-LPP)
* [Contrastive Flow Matching](http://arxiv.org/abs/2506.05350)<br>:star:[code](https://github.com/gstoica27/DeltaFM.git)
* [Trial-Oriented Visual Rearrangement](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Trial-Oriented_Visual_Rearrangement_ICCV_2025_paper.pdf)
* [FusionPhys A Flexible Framework for Fusing Complementary Sensing Modalities in Remote Physiological Measurement](https://openaccess.thecvf.com/content/ICCV2025/papers/Ying_FusionPhys_A_Flexible_Framework_for_Fusing_Complementary_Sensing_Modalities_in_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/chh-ying/fusionphys)
* [NeuFrameQ Neural Frame Fields for Scalable and Generalizable Anisotropic Quadrangulation](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation_ICCV_2025_paper.pdf)
* [Principal Components Enable A New Language of Images](http://arxiv.org/abs/2503.08685)
* [Always Skip Attention](http://arxiv.org/abs/2505.01996)
* [FREE-Merging Fourier Transform for Efficient Model Merging](https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_FREE-Merging_Fourier_Transform_for_Efficient_Model_Merging_ICCV_2025_paper.pdf)
* [JPEG Processing Neural Operator for Backward-Compatible Coding](http://arxiv.org/abs/2507.23521)<br>:star:[code](https://github.com/WooKyoungHan/JPNeO)
* [FlowTok Flowing Seamlessly Across Text and Image Tokens](http://arxiv.org/abs/2503.10772)<br>:star:[code](https://github.com/TACJu/FlowTok)
* [Magic Insert Style-Aware Drag-and-Drop](http://arxiv.org/abs/2407.02489)
* [PseudoMapTrainer Learning Online Mapping without HD Maps](https://openaccess.thecvf.com/content/ICCV2025/papers/Lowens_PseudoMapTrainer_Learning_Online_Mapping_without_HD_Maps_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/boschresearch/PseudoMapTrainer)
* [Optical Model-Driven Sharpness Mapping for Autofocus in Small Depth-of-Field and Severe Defocus Scenarios](https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_Optical_Model-Driven_Sharpness_Mapping_for_Autofocus_in_Small_Depth-of-Field_and_ICCV_2025_paper.pdf)
* [RoCo-Sim Enhancing Roadside Collaborative Perception through Foreground Simulation](https://openaccess.thecvf.com/content/ICCV2025/papers/Du_RoCo-Sim_Enhancing_Roadside_Collaborative_Perception_through_Foreground_Simulation_ICCV_2025_paper.pdf)
* [Metric Convolutions A Unifying Theory to Adaptive Image Convolutions](https://openaccess.thecvf.com/content/ICCV2025/papers/Dages_Metric_Convolutions_A_Unifying_Theory_to_Adaptive_Image_Convolutions_ICCV_2025_paper.pdf)
* [Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension](http://arxiv.org/abs/2412.03704)
* [Transparent Vision A Theory of Hierarchical Invariant Representations](https://openaccess.thecvf.com/content/ICCV2025/papers/Qi_Transparent_Vision_A_Theory_of_Hierarchical_Invariant_Representations_ICCV_2025_paper.pdf)
* [PRO-VPT Distribution-Adaptive Visual Prompt Tuning via Prompt Relocation](https://openaccess.thecvf.com/content/ICCV2025/papers/Shang_PRO-VPT_Distribution-Adaptive_Visual_Prompt_Tuning_via_Prompt_Relocation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ckshang/PRO-VPT)
* [Do It Yourself Learning Semantic Correspondence from Pseudo-Labels](https://openaccess.thecvf.com/content/ICCV2025/papers/Dunkel_Do_It_Yourself_Learning_Semantic_Correspondence_from_Pseudo-Labels_ICCV_2025_paper.pdf)
* [BATCLIP Bimodal Online Test-Time Adaptation for CLIP](https://openaccess.thecvf.com/content/ICCV2025/papers/Maharana_BATCLIP_Bimodal_Online_Test-Time_Adaptation_for_CLIP_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/sarthaxxxxx/BATCLIP)
* [Align Your Rhythm Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation](http://arxiv.org/abs/2503.17340)
* [Neuromanifold-Regularized KANs for Shape-fair Feature Representations](https://openaccess.thecvf.com/content/ICCV2025/papers/Arslan_Neuromanifold-Regularized_KANs_for_Shape-fair_Feature_Representations_ICCV_2025_paper.pdf)<br>:star:[code](http://www.github.com/kaptres/NMR-KAN) :star:[code2](https://github.com/kaptres/NMR-KAN)
* [Free-running vs Synchronous Single-Photon Lidar for High-flux 3D Imaging](http://arxiv.org/abs/2507.09386)
* [On the Complexity-Faithfulness Trade-off of Gradient-Based Explanations](http://arxiv.org/abs/2508.10490)
* [DuoLoRA  Cycle-consistent and Rank-disentangled Content-Style Personalization](http://arxiv.org/abs/2504.13206)
* [GFPack Attention-Driven Gradient Fields for Optimizing 2D Irregular Packing](https://openaccess.thecvf.com/content/ICCV2025/papers/Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/TimHsue/GFPack-pp)
* [ShadowHack Hacking Shadows via Luminance-Color Divide and Conquer](http://arxiv.org/abs/2412.02545)
* [Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery](http://arxiv.org/abs/2507.12029)
* [Pi-GPS Enhancing Geometry Problem Solving by Unleashing the Power of Diagrammatic Information](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_Pi-GPS_Enhancing_Geometry_Problem_Solving_by_Unleashing_the_Power_of_ICCV_2025_paper.pdf)
* [Stochastic Interpolants for Revealing Stylistic Flows across the History of Art](https://openaccess.thecvf.com/content/ICCV2025/papers/Ma_Stochastic_Interpolants_for_Revealing_Stylistic_Flows_across_the_History_of_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/CompVis/Art-fm)
* [Thermal Polarimetric Multi-view Stereo](https://openaccess.thecvf.com/content/ICCV2025/papers/Kushida_Thermal_Polarimetric_Multi-view_Stereo_ICCV_2025_paper.pdf)
* [Image Intrinsic Scale Assessment Bridging the Gap Between Quality and Resolution](http://arxiv.org/abs/2502.06476)
* [Relative Illumination Fields Learning Medium and Light Independent Underwater Scenes](https://openaccess.thecvf.com/content/ICCV2025/papers/She_Relative_Illumination_Fields_Learning_Medium_and_Light_Independent_Underwater_Scenes_ICCV_2025_paper.pdf)
* [Color Matching Using Hypernetwork-Based Kolmogorov-Arnold Networks](http://arxiv.org/abs/2503.11781)
* [SITE towards Spatial Intelligence Thorough Evaluation](http://arxiv.org/abs/2505.05456)
* [FW-Merging Scaling Model Merging with Frank-Wolfe Optimization](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_FW-Merging_Scaling_Model_Merging_with_Frank-Wolfe_Optimization_ICCV_2025_paper.pdf)
* [On the Provable Importance of Gradients for Autonomous Language-Assisted Image Clustering](https://openaccess.thecvf.com/content/ICCV2025/papers/Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image_ICCV_2025_paper.pdf)
* [You Share Beliefs I Adapt Progressive Heterogeneous Collaborative Perception](http://arxiv.org/abs/2509.09310)
* [C2MIL Synchronizing Semantic and Topological Causalities in Multiple Instance Learning for Robust and Interpretable Survival Analysis](https://openaccess.thecvf.com/content/ICCV2025/papers/Cen_C2MIL_Synchronizing_Semantic_and_Topological_Causalities_in_Multiple_Instance_Learning_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/mimic0127/C2MIL)
* [SAMO A Lightweight Sharpness-Aware Approach for Multi-Task Optimization with Joint Global-Local Perturbation](http://arxiv.org/abs/2507.07883)
* [ViT-Linearizer Distilling Quadratic Knowledge into Linear-Time Vision Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Wei_ViT-Linearizer_Distilling_Quadratic_Knowledge_into_Linear-Time_Vision_Models_ICCV_2025_paper.pdf)
* [Generalized Deep Multi-view Clustering via Causal Learning with Partially Aligned Cross-view Correspondence](http://arxiv.org/abs/2509.16022)
* [Large Multi-modal Models Can Interpret Features in Large Multi-modal Models](http://arxiv.org/abs/2411.14982)
* [Deep Incomplete Multi-view Clustering with Distribution Dual-Consistency Recovery Guidance](http://arxiv.org/abs/2503.11017)
* [Acknowledging Focus Ambiguity in Visual Questions](http://arxiv.org/abs/2501.02201)
* [Combinative Matching for Geometric Shape Assembly](http://arxiv.org/abs/2508.09780)








## 2020 年论文分类汇总戳这里
↘️[CVPR-2020-Papers](https://github.com/52CV/CVPR-2020-Papers) 
↘️[ECCV-2020-Papers](https://github.com/52CV/ECCV-2020-Papers)

<a name="00"/>

## 2021 年论文分类汇总戳这里
↘️[ICCV-2021-Papers](https://github.com/52CV/ICCV-2021-Papers)
↘️[CVPR-2021-Papers](https://github.com/52CV/CVPR-2021-Papers)

<a name="000"/>

## 2022 年论文分类汇总戳这里
↘️[CVPR-2022-Papers](https://github.com/52CV/CVPR-2022-Papers/blob/main/README.md)
↘️[WACV-2022-Papers](https://github.com/52CV/WACV-2022-Papers)
↘️[ECCV-2022-Papers](https://github.com/52CV/ECCV-2022-Papers/blob/main/README.md)

<a name="0000"/>

## 2023 年论文分类汇总戳这里
↘️[CVPR-2023-Papers](https://github.com/52CV/CVPR-2023-Papers)
↘️[WACV-2023-Papers](https://github.com/52CV/WACV-2023-Papers)
↘️[ICCV-2023-Papers](https://github.com/52CV/ICCV-2023-Papers)
↘️[2023-CV-Surveys](https://github.com/52CV/CV-Surveys/blob/main/2023-CV-Surveys.md)

### 扫码CV君微信(注明：CVPR)入微信交流群：
![9475fa20fd5e95235d9fa23ae9587a2](https://user-images.githubusercontent.com/62801906/156720309-de92964f-a6da-464a-b21f-cfb270c13e27.png)
