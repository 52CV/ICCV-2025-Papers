# ICCV-2025-Papers
![image](https://github.com/user-attachments/assets/0b93ce8a-4383-46ba-9672-6c746728c9f9)

## 会议时间：2025年10月19日至23日
## 会议网址：https://iccv.thecvf.com/



## 查看2025年综述文献点这里↘️[2025-CV-Surveys](https://github.com/52CV/CV-Surveys)

## 2025 年论文分类汇总戳这里
↘️[WACV-2025-Papers](https://github.com/52CV/WACV-2025-Papers)
↘️[CVPR-2025-Papers](https://github.com/52CV/CVPR-2025-Papers)
↘️[ICCV-2025-Papers](https://github.com/52CV/ICCV-2025-Papers)

## 2024 年论文分类汇总戳这里
↘️[WACV-2024-Papers](https://github.com/52CV/WACV-2024-Papers)
↘️[CVPR-2024-Papers](https://github.com/52CV/CVPR-2024-Papers)
↘️[ECCV-2024-Papers](https://github.com/52CV/ECCV-2024-Papers)


## [2023 年论文分类汇总戳这里](#0000)
## [2022 年论文分类汇总戳这里](#000)
## [2021 年论文分类汇总戳这里](#00)
## [2020 年论文分类汇总戳这里](#0)



## 10月24日更新 177 篇，共计 450+177 篇。
* [SummDiff Generative Modeling of Video Summarization with Diffusion](http://arxiv.org/abs/2510.08458)
* [VideoLLaMB Long Streaming Video Understanding with Recurrent Memory Bridges](http://arxiv.org/abs/2409.01071)
* [HiERO Understanding the Hierarchy of Human Behavior Enhances Reasoning on Egocentric Videos](http://arxiv.org/abs/2505.12911)
* [FVGen Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation](http://arxiv.org/abs/2508.06392)
* [Open-Vocabulary Octree-Graph for 3D Scene Understanding](http://arxiv.org/abs/2411.16253)<br>:star:[code](https://github.com/yifeisu/OV-Octree-Graph)
* [FlexGen Flexible Multi-View Generation from Text and Image Inputs](http://arxiv.org/abs/2410.10745)
* [OminiControl Minimal and Universal Control for Diffusion Transformer](http://arxiv.org/abs/2411.15098)
* [Zeroth-Order Fine-Tuning of LLMs in Random Subspaces](http://arxiv.org/abs/2410.08989)<br>:star:[code](https://github.com/zimingyy/SubZero)
* [Pinco Position-induced Consistent Adapter for Diffusion Transformer in Foreground-conditioned Inpainting](http://arxiv.org/abs/2412.03812)
* [SyncDiff Synchronized Motion Diffusion for Multi-Body Human-Object Interaction Synthesis](http://arxiv.org/abs/2412.20104)
* [GSOT3D Towards Generic 3D Single Object Tracking in the Wild](http://arxiv.org/abs/2412.02129)<br>:star:[code](https://github.com/ailovejinx/GSOT3D)
* [UnZipLoRA Separating Content and Style from a Single Image](http://arxiv.org/abs/2412.04465)
* [FlowDPS  Flow-Driven Posterior Sampling for Inverse Problems](http://arxiv.org/abs/2503.08136)
* [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://openaccess.thecvf.com/content/ICCV2025/papers/Tang_Closed-Loop_Transfer_for_Weakly-supervised_Affordance_Grounding_ICCV_2025_paper.pdf)
* [ReconDreamer Harmonizing Generative and Reconstructive Models for Driving Scene Representation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_ReconDreamer_Harmonizing_Generative_and_Reconstructive_Models_for_Driving_Scene_Representation_ICCV_2025_paper.pdf)
* [AdaDCP Learning an Adapter with Discrete Cosine Prior for Clear-to-Adverse Domain Generalization](https://openaccess.thecvf.com/content/ICCV2025/papers/Bi_AdaDCP_Learning_an_Adapter_with_Discrete_Cosine_Prior_for_Clear-to-Adverse_ICCV_2025_paper.pdf)
* [HERMES A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation](http://arxiv.org/abs/2501.14729)<br>:star:[code](https://github.com/LMD0311/HERMES)
* [Enhancing Image Restoration Transformer via Adaptive Translation Equivariance](http://arxiv.org/abs/2506.18520)
* [ScenePainter Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment](http://arxiv.org/abs/2507.19058)
* [Free4D Tuning-free 4D Scene Generation with Spatial-Temporal Consistency](http://arxiv.org/abs/2503.20785)
* [Generative Zoo](http://arxiv.org/abs/2412.08101)
* [Instruction-Oriented Preference Alignment for Enhancing Multi-Modal Comprehension Capability of MLLMs](http://arxiv.org/abs/2503.20309)
* [RapVerse Coherent Vocals and Whole-Body Motion Generation from Text](http://arxiv.org/abs/2405.20336)
* [MoFRR Mixture of Diffusion Models for Face Retouching Restoration](http://arxiv.org/abs/2507.19770)
* [SFUOD Source-Free Unknown Object Detection](http://arxiv.org/abs/2507.17373)
* [Event-based Tiny Object Detection A Benchmark Dataset and Baseline](http://arxiv.org/abs/2506.23575)
* [ESCNetEdge-Semantic Collaborative Network for Camouflaged Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Ye_ESCNetEdge-Semantic_Collaborative_Network_for_Camouflaged_Object_Detection_ICCV_2025_paper.pdf)
* [UniEgoMotion A Unified Model for Egocentric Motion Reconstruction Forecasting and Generation](http://arxiv.org/abs/2508.01126)
* [ToolVQA A Dataset for Multi-step Reasoning VQA with External Tools](http://arxiv.org/abs/2508.03284)<br>:star:[code](https://github.com/Fugtemypt123/ToolVQA-release)
* [PixelStitch Structure-Preserving Pixel-Wise Bidirectional Warps for Unsupervised Image Stitching](https://openaccess.thecvf.com/content/ICCV2025/papers/Jin_PixelStitch_Structure-Preserving_Pixel-Wise_Bidirectional_Warps_for_Unsupervised_Image_Stitching_ICCV_2025_paper.pdf)
* [Ross3D Reconstructive Visual Instruction Tuning with 3D-Awareness](http://arxiv.org/abs/2504.01901)
* [LVFace Progressive Cluster Optimization for Large Vision Models in Face Recognition](http://arxiv.org/abs/2501.13420)<br>:star:[code](https://github.com/bytedance/LVFace)
* [VLR-Driver Large Vision-Language-Reasoning Models for Embodied Autonomous Driving](https://openaccess.thecvf.com/content/ICCV2025/papers/Kong_VLR-Driver_Large_Vision-Language-Reasoning_Models_for_Embodied_Autonomous_Driving_ICCV_2025_paper.pdf)
* [ResGS Residual Densification of 3D Gaussian for Efficient Detail Recovery](http://arxiv.org/abs/2412.07494)
* [Language Driven Occupancy Prediction](http://arxiv.org/abs/2411.16072)
* [E-NeMF Event-based Neural Motion Field for Novel Space-time View Synthesis of Dynamic Scenes](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_E-NeMF_Event-based_Neural_Motion_Field_for_Novel_Space-time_View_Synthesis_ICCV_2025_paper.pdf)
* [Optimal Transport for Brain-Image Alignment Unveiling Redundancy and Synergy in Neural Information Processing](http://arxiv.org/abs/2503.10663)<br>:star:[code](https://github.com/NKUShaw/OT-Alignment4brain-to-image)
* [Erasing More Than Intended How Concept Erasure Degrades the Generation of Non-Target Concepts](http://arxiv.org/abs/2501.09833)
-----------------------------------------------------
* [From Gaze to Movement Predicting Visual Attention for Autonomous Driving Human-Machine Interaction based on Programmatic Imitation Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_From_Gaze_to_Movement_Predicting_Visual_Attention_for_Autonomous_Driving_ICCV_2025_paper.pdf)
* [Unsupervised RGB-D Point Cloud Registration for Scenes with Low Overlap and Photometric Inconsistency](https://openaccess.thecvf.com/content/ICCV2025/papers/Shou_Unsupervised_RGB-D_Point_Cloud_Registration_for_Scenes_with_Low_Overlap_ICCV_2025_paper.pdf)
* [MM-IFEngine Towards Multimodal Instruction Following](https://openaccess.thecvf.com/content/ICCV2025/papers/Ding_MM-IFEngine_Towards_Multimodal_Instruction_Following_ICCV_2025_paper.pdf)
* [ROADWork A Dataset and Benchmark for Learning to Recognize Observe Analyze and Drive Through Work Zones](https://openaccess.thecvf.com/content/ICCV2025/papers/Ghosh_ROADWork_A_Dataset_and_Benchmark_for_Learning_to_Recognize_Observe_ICCV_2025_paper.pdf)
* [Gradient Decomposition and Alignment for Incremental Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Luo_Gradient_Decomposition_and_Alignment_for_Incremental_Object_Detection_ICCV_2025_paper.pdf)
* [Why LVLMs Are More Prone to Hallucinations in Longer Responses The Role of Context](https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_Why_LVLMs_Are_More_Prone_to_Hallucinations_in_Longer_Responses_ICCV_2025_paper.pdf)
* [VisionMath Vision-Form Mathematical Problem-Solving](https://openaccess.thecvf.com/content/ICCV2025/papers/Ma_VisionMath_Vision-Form_Mathematical_Problem-Solving_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/mengqiDyangge/VisionMath)
* [Demeter A Parametric Model of Crop Plant Morphology from the Real World](https://openaccess.thecvf.com/content/ICCV2025/papers/Cheng_Demeter_A_Parametric_Model_of_Crop_Plant_Morphology_from_the_ICCV_2025_paper.pdf)
* [Automated Red Teaming for Text-to-Image Models through Feedback-Guided Prompt Iteration with Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_Automated_Red_Teaming_for_Text-to-Image_Models_through_Feedback-Guided_Prompt_Iteration_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Weiww-Xu/FGPI)
* [CoA-VLA Improving Vision-Language-Action Models via Visual-Text Chain-of-Affordance](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_CoA-VLA_Improving_Vision-Language-Action_Models_via_Visual-Text_Chain-of-Affordance_ICCV_2025_paper.pdf)
* [ZFusion Efficient Deep Compositional Zero-shot Learning for Blind Image Super-Resolution with Generative Diffusion Prior](https://openaccess.thecvf.com/content/ICCV2025/papers/Esmaeilzehi_ZFusion_Efficient_Deep_Compositional_Zero-shot_Learning_for_Blind_Image_Super-Resolution_ICCV_2025_paper.pdf)
* [Head2Body Body Pose Generation from Multi-sensory Head-mounted Inputs](https://openaccess.thecvf.com/content/ICCV2025/papers/Tran_Head2Body_Body_Pose_Generation_from_Multi-sensory_Head-mounted_Inputs_ICCV_2025_paper.pdf)
* [PAN-Crafter Learning Modality-Consistent Alignment for PAN-Sharpening](https://openaccess.thecvf.com/content/ICCV2025/papers/Do_PAN-Crafter_Learning_Modality-Consistent_Alignment_for_PAN-Sharpening_ICCV_2025_paper.pdf)
* [CoStoDet-DDPM Collaborative Training of Stochastic and Deterministic Models Improves Surgical Workflow Anticipation and Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_CoStoDet-DDPM_Collaborative_Training_of_Stochastic_and_Deterministic_Models_Improves_Surgical_ICCV_2025_paper.pdf)
* [RareCLIP Rarity-aware Online Zero-shot Industrial Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/He_RareCLIP_Rarity-aware_Online_Zero-shot_Industrial_Anomaly_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/hjf02/RareCLIP)
* [ArgMatch Adaptive Refinement Gathering for Efficient Dense Matching](https://openaccess.thecvf.com/content/ICCV2025/papers/Deng_ArgMatch_Adaptive_Refinement_Gathering_for_Efficient_Dense_Matching_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ACuOoOoO/argmatch)
* [Any-SSR How Recursive Least Squares Works in Continual Learning of Large Language Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Tong_Any-SSR_How_Recursive_Least_Squares_Works_in_Continual_Learning_of_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ZHUANGHP/Any-SSR)
* [Spherical Epipolar Rectification for Deep Two-View Absolute Depth Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Brousseau_Spherical_Epipolar_Rectification_for_Deep_Two-View_Absolute_Depth_Estimation_ICCV_2025_paper.pdf)
* [SANA-Sprint One-Step Diffusion with Continuous-Time Consistency Distillation](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation_ICCV_2025_paper.pdf)
* [Prior2Former - Evidential Modeling of Mask Transformers for Assumption-Free Open-World Panoptic Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World_ICCV_2025_paper.pdf)
-------------------------------------------------------------------------------------------------
* [Scaling Action Detection AdaTAD with Transformer-Enhanced Temporal-Spatial Adaptation](https://openaccess.thecvf.com/content/ICCV2025/papers/Agrawal_Scaling_Action_Detection_AdaTAD_with_Transformer-Enhanced_Temporal-Spatial_Adaptation_ICCV_2025_paper.pdf)
* [MMAD Multi-label Micro-Action Detection in Videos](http://arxiv.org/abs/2407.05311)<br>:star:[code](https://github.com/VUT-HFUT/Micro-Action)
* [DyGS-SLAM Real-Time Accurate Localization and Gaussian Reconstruction for Dynamic Scenes](https://openaccess.thecvf.com/content/ICCV2025/papers/Hu_DyGS-SLAM_Real-Time_Accurate_Localization_and_Gaussian_Reconstruction_for_Dynamic_Scenes_ICCV_2025_paper.pdf)
* [4D Gaussian Splatting SLAM](http://arxiv.org/abs/2503.16710)
* [ToF-Splatting Dense SLAM using Sparse Time-of-Flight Depth and Multi-Frame Integration](https://openaccess.thecvf.com/content/ICCV2025/papers/Conti_ToF-Splatting_Dense_SLAM_using_Sparse_Time-of-Flight_Depth_and_Multi-Frame_Integration_ICCV_2025_paper.pdf)
* [Benchmarking Egocentric Visual-Inertial SLAM at City Scale](http://arxiv.org/abs/2509.26639)
* [SuperEvent Cross-Modal Learning of Event-based Keypoint Detection for SLAM](http://arxiv.org/abs/2504.00139)<br>:house:[project](https://ethz-mrl.github.io/SuperEvent)
* [SEGS-SLAM Structure-enhanced 3D Gaussian Splatting SLAM with Appearance Embedding](https://openaccess.thecvf.com/content/ICCV2025/papers/Wen_SEGS-SLAM_Structure-enhanced_3D_Gaussian_Splatting_SLAM_with_Appearance_Embedding_ICCV_2025_paper.pdf)<br>:house:[project](https://segs-slam.github.io/)
* [Underwater Visual SLAM with Depth Uncertainty and Medium Modeling](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling_ICCV_2025_paper.pdf)
-------------------------------------------------------------------------------
* [Learning Precise Affordances from Egocentric Videos for Robotic Manipulation](http://arxiv.org/abs/2408.10123)<br>:house:[project](https://reagan1311.github.io/affgrasp)
* [iManip Skill-Incremental Learning for Robotic Manipulation](http://arxiv.org/abs/2503.07087)
* [RoBridge A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation](http://arxiv.org/abs/2505.01709)
* [EC-Flow Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_EC-Flow_Enabling_Versatile_Robotic_Manipulation_from_Action-Unlabeled_Videos_via_Embodiment-Centric_ICCV_2025_paper.pdf)<br>:house:[project](https://ec-flow1.github.io/)
* [A0 An Affordance-Aware Hierarchical Model for General Robotic Manipulation](http://arxiv.org/abs/2504.12636)
* [Rethinking Bimanual Robotic Manipulation Learning with Decoupled Interaction Framework](http://arxiv.org/abs/2503.09186)
* [GWM Towards Scalable Gaussian World Models for Robotic Manipulation](http://arxiv.org/abs/2508.17600)
-----------------------------------------------------------------------------------
* [Embodied Navigation with Auxiliary Task of Action Description Prediction](https://openaccess.thecvf.com/content/ICCV2025/papers/Kondoh_Embodied_Navigation_with_Auxiliary_Task_of_Action_Description_Prediction_ICCV_2025_paper.pdf)
* [EmbodiedSplat Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device](http://arxiv.org/abs/2509.17430)<br>:house:[project](https://gchhablani.github.io/embodied-splat)
* [GUIOdyssey A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices](http://arxiv.org/abs/2406.08451)
* [Learning on the Go A Meta-learning Object Navigation Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Learning_on_the_Go_A_Meta-learning_Object_Navigation_Model_ICCV_2025_paper.pdf)
* [RoboTrom-Nav A Unified Framework for Embodied Navigation Integrating Perception Planning and Prediction](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhong_RoboTrom-Nav_A_Unified_Framework_for_Embodied_Navigation_Integrating_Perception_Planning_ICCV_2025_paper.pdf)
* [MoMa-Kitchen A 100K Benchmark for Affordance-Grounded Last-Mile Navigation in Mobile Manipulation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_MoMa-Kitchen_A_100K_Benchmark_for_Affordance-Grounded_Last-Mile_Navigation_in_Mobile_ICCV_2025_paper.pdf)
* [Active Perception Meets Rule-Guided RL A Two-Phase Approach for Precise Object Navigation in Complex Environments](https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Active_Perception_Meets_Rule-Guided_RL_A_Two-Phase_Approach_for_Precise_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/qinliangql/APRR)
* [SAME Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts](http://arxiv.org/abs/2412.05552)
* [Collaborative Instance Object Navigation Leveraging Uncertainty-Awareness to Minimize Human-Agent Dialogues](http://arxiv.org/abs/2412.01250)
* [CogNav Cognitive Process Modeling for Object Goal Navigation with LLMs](http://arxiv.org/abs/2412.10439)
* [DialNav Multi-turn Dialog Navigation with a Remote Guide](http://arxiv.org/abs/2509.12894)<br>:house:[project](https://happilee12.github.io/DialNav)
* [monoVLN Bridging the Observation Gap between Monocular and Panoramic Vision and Language Navigation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_monoVLN_Bridging_the_Observation_Gap_between_Monocular_and_Panoramic_Vision_ICCV_2025_paper.pdf)
* [Moto Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos](http://arxiv.org/abs/2412.04445)
* [IRASim A Fine-Grained World Model for Robot Manipulation](http://arxiv.org/abs/2406.14540)<br>:house:[project](https://gen-irasim.github.io/)
* [G-DexGrasp Generalizable Dexterous Grasping Synthesis Via Part-Aware Prior Retrieval and Prior-Assisted Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Jian_G-DexGrasp_Generalizable_Dexterous_Grasping_Synthesis_Via_Part-Aware_Prior_Retrieval_and_ICCV_2025_paper.pdf)
* [DexH2R A Benchmark for Dynamic Dexterous Grasping in Human-to-Robot Handover](http://arxiv.org/abs/2506.23152)
* [OVA-Fields Weakly Supervised Open-Vocabulary Affordance Fields for Robot Operational Part Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Su_OVA-Fields_Weakly_Supervised_Open-Vocabulary_Affordance_Fields_for_Robot_Operational_Part_ICCV_2025_paper.pdf)
* [Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control](http://arxiv.org/abs/2505.15304)
* [Learning 4D Embodied World Models](http://arxiv.org/abs/2504.20995)
* [RobAVA A Large-scale Dataset and Baseline Towards Video based Robotic Arm Action Understanding](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_RobAVA_A_Large-scale_Dataset_and_Baseline_Towards_Video_based_Robotic_ICCV_2025_paper.pdf)
* [RoboAnnotatorX A Comprehensive and Universal Annotation Framework for Accurate Understanding of Long-horizon Robot Demonstration](https://openaccess.thecvf.com/content/ICCV2025/papers/Kou_RoboAnnotatorX_A_Comprehensive_and_Universal_Annotation_Framework_for_Accurate_Understanding_ICCV_2025_paper.pdf)
* [4D Visual Pre-training for Robot Learning](http://arxiv.org/abs/2508.17230)<br>:house:[project](https://4d-visual-pretraining.github.io/)
* [LookOut Real-World Humanoid Egocentric Navigation](https://openaccess.thecvf.com/content/ICCV2025/papers/Pan_LookOut_Real-World_Humanoid_Egocentric_Navigation_ICCV_2025_paper.pdf)
* [CityNav A Large-Scale Dataset for Real-World Aerial Navigation](http://arxiv.org/abs/2406.14240)
* [Function-centric Bayesian Network for Zero-Shot Object Goal Navigation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Function-centric_Bayesian_Network_for_Zero-Shot_Object_Goal_Navigation_ICCV_2025_paper.pdf)
-------------------------------------------------------------------
* [Environment-Agnostic Pose Generating Environment-independent Object Representations for 6D Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Environment-Agnostic_Pose_Generating_Environment-independent_Object_Representations_for_6D_Pose_Estimation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/acmff22/EA6D) :house:[project](https://github.com/acmff22/EA6D)
* [Ultra-Precision 6DoF Pose Estimation Using 2-D Interpolated Discrete Fourier Transform](https://openaccess.thecvf.com/content/ICCV2025/papers/Shi_Ultra-Precision_6DoF_Pose_Estimation_Using_2-D_Interpolated_Discrete_Fourier_Transform_ICCV_2025_paper.pdf)
* [Prior-aware Dynamic Temporal Modeling Framework for Sequential 3D Hand Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Ren_Prior-aware_Dynamic_Temporal_Modeling_Framework_for_Sequential_3D_Hand_Pose_ICCV_2025_paper.pdf)
* [HccePose(BF) Predicting Front  Back Surfaces to Construct Ultra-Dense 2D-3D Correspondences for Pose Estimation](http://arxiv.org/abs/2510.10177)
* [RePoseD Efficient Relative Pose Estimation With Known Depth Information](https://openaccess.thecvf.com/content/ICCV2025/papers/Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/kocurvik/mdrp)
* [Scaling 3D Compositional Models for Robust Classification and Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Yuan_Scaling_3D_Compositional_Models_for_Robust_Classification_and_Pose_Estimation_ICCV_2025_paper.pdf)
* [DRaM-LHM A Quaternion Framework for Iterative Camera Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lin_DRaM-LHM_A_Quaternion_Framework_for_Iterative_Camera_Pose_Estimation_ICCV_2025_paper.pdf)
* [ForestFormer3D A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds](http://arxiv.org/abs/2506.16991)<br>:house:[project](https://bxiang233.github.io/FF3D)
* [Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds](http://arxiv.org/abs/2508.11265)<br>:star:[code](https://github.com/ChicalH/DCGL)
* [GroundFlow A Plug-in Module for Temporal Reasoning on 3D Point Cloud Sequential Grounding](http://arxiv.org/abs/2506.21188)
* [Tree Skeletonization from 3D Point Clouds by Denoising Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Marks_Tree_Skeletonization_from_3D_Point_Clouds_by_Denoising_Diffusion_ICCV_2025_paper.pdf)
* [TrackAny3D Transferring Pretrained 3D Models for Category-unified 3D Point Cloud Tracking](http://arxiv.org/abs/2507.19908)
* [BUFFER-X Towards Zero-Shot Point Cloud Registration in Diverse Scenes](https://openaccess.thecvf.com/content/ICCV2025/papers/Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/MIT-SPARK/BUFFER-X)
* [Revisiting Point Cloud Completion Are We Ready For The Real-World](http://arxiv.org/abs/2411.17580)
* [Geometric Alignment and Prior Modulation for View-Guided Point Cloud Completion on Unseen Categories](https://openaccess.thecvf.com/content/ICCV2025/papers/Xiu_Geometric_Alignment_and_Prior_Modulation_for_View-Guided_Point_Cloud_Completion_ICCV_2025_paper.pdf)
* [Purge-Gate Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token purging](https://openaccess.thecvf.com/content/ICCV2025/papers/Yazdanpanah_Purge-Gate_Backpropagation-Free_Test-Time_Adaptation_for_Point_Clouds_Classification_via_Token_ICCV_2025_paper.pdf)
* [Efficient Spiking Point Mamba for Point Cloud Analysis](http://arxiv.org/abs/2504.14371)<br>:star:[code](https://github.com/PeppaWu/SPM)
* [Noise2Score3D Tweedies Approach for Unsupervised Point Cloud Denoising](https://openaccess.thecvf.com/content/ICCV2025/papers/Wei_Noise2Score3D_Tweedies_Approach_for_Unsupervised_Point_Cloud_Denoising_ICCV_2025_paper.pdf)
* [Generalized Few-Shot Point Cloud Segmentation via LLM-Assisted Hyper-Relation Matching](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Generalized_Few-Shot_Point_Cloud_Segmentation_via_LLM-Assisted_Hyper-Relation_Matching_ICCV_2025_paper.pdf)
* [Mitigating Geometric Degradation in Fast DownSampling via FastAdapter for Point Cloud Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Mitigating_Geometric_Degradation_in_Fast_DownSampling_via_FastAdapter_for_Point_ICCV_2025_paper.pdf)
* [UST-SSM Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_UST-SSM_Unified_Spatio-Temporal_State_Space_Models_for_Point_Cloud_Video_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/wangzy01/UST-SSM)
* [Omni-scene Perception-oriented Point Cloud Geometry Enhancement for Coordinate Quantization](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Omni-scene_Perception-oriented_Point_Cloud_Geometry_Enhancement_for_Coordinate_Quantization_ICCV_2025_paper.pdf)
* [A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds](http://arxiv.org/abs/2504.09129)
* [GenFlow3D Generative Scene Flow Estimation and Prediction on Point Cloud Sequences](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_GenFlow3D_Generative_Scene_Flow_Estimation_and_Prediction_on_Point_Cloud_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ustc-hlli/GenFlow3D)
* [Feature Extraction and Representation of Pre-training Point Cloud Based on Diffusion Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Qiu_Feature_Extraction_and_Representation_of_Pre-training_Point_Cloud_Based_on_ICCV_2025_paper.pdf)
* [Leaps and Bounds An Improved Point Cloud Winding Number Formulation for Fast Normal Estimation and Surface Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Koneputugodage_Leaps_and_Bounds_An_Improved_Point_Cloud_Winding_Number_Formulation_ICCV_2025_paper.pdf)
* [Serialization based Point Cloud Oversegmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_Serialization_based_Point_Cloud_Oversegmentation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/CHL-glitch/SPCNet)
* [Towards More Diverse and Challenging Pre-training for Point Cloud Learning Self-Supervised Cross Reconstruction with Decoupled Views](http://arxiv.org/abs/2509.01250)<br>:star:[code](https://github.com/aHapBean/Point-PQAE)
* [Liberated-GS 3D Gaussian Splatting Independent from SfM Point Clouds](https://openaccess.thecvf.com/content/ICCV2025/papers/Pan_Liberated-GS_3D_Gaussian_Splatting_Independent_from_SfM_Point_Clouds_ICCV_2025_paper.pdf)
* [CAD-Recode Reverse Engineering CAD Code from Point Clouds](https://openaccess.thecvf.com/content/ICCV2025/papers/Rukhovich_CAD-Recode_Reverse_Engineering_CAD_Code_from_Point_Clouds_ICCV_2025_paper.pdf)
* [Constraint-Aware Feature Learning for Parametric Point Cloud](http://arxiv.org/abs/2411.07747)
* [Egocentric Action-aware Inertial Localization in Point Clouds with Vision-Language Guidance](http://arxiv.org/abs/2505.14346)
* [DiffPCI Large Motion Point Cloud frame Interpolation with Diffusion Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_DiffPCI_Large_Motion_Point_Cloud_frame_Interpolation_with_Diffusion_Model_ICCV_2025_paper.pdf)
* [Mixed Signals A Diverse Point Cloud Dataset for Heterogeneous LiDAR V2X Collaboration](http://arxiv.org/abs/2502.14156)
* [DAP-MAE Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/CVI-SZU/DAP-MAE)
* [Interpretable point cloud classification using multiple instance learning](https://openaccess.thecvf.com/content/ICCV2025/papers/De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning_ICCV_2025_paper.pdf)
* [CounterPC Counterfactual Feature Realignment for Unsupervised Domain Adaptation on Point Clouds](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point_ICCV_2025_paper.pdf)
* [Partially Matching Submap Helps Uncertainty Modeling and Propagation for Text to Point Cloud Localization](https://openaccess.thecvf.com/content/ICCV2025/papers/Feng_Partially_Matching_Submap_Helps_Uncertainty_Modeling_and_Propagation_for_Text_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Afoolbird/PMSH)
* [DiffRefine Diffusion-based Proposal Specific Point Cloud Densification for Cross-Domain Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object_ICCV_2025_paper.pdf)
* [Point Cloud Self-supervised Learning via 3D to Multi-view Masked Learner](http://arxiv.org/abs/2311.10887)
* [RARE Refine Any Registration of Pairwise Point Clouds via Zero-Shot Learning](http://arxiv.org/abs/2507.19950)<br>:star:[code](https://github.com/zhengcy-lambo/RARE.git)
* [FlowSeek Optical Flow Made Easier with Depth Foundation Models and Motion Bases](http://arxiv.org/abs/2509.05297)
* [PriOr-Flow Enhancing Primitive Panoramic Optical Flow with Orthogonal View](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View_ICCV_2025_paper.pdf)
* [Flow4Agent Long-form Video Understanding via Motion Prior from Optical Flow](http://arxiv.org/abs/2510.05836)
* [EMatch A Unified Framework for Event-based Optical Flow and Stereo Matching](http://arxiv.org/abs/2407.21735)
* [Removing Cost Volumes from Optical Flow Estimators](https://openaccess.thecvf.com/content/ICCV2025/papers/Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators_ICCV_2025_paper.pdf)
* [Language Decoupling with Fine-grained Knowledge Guidance for Referring Multi-object Tracking](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Language_Decoupling_with_Fine-grained_Knowledge_Guidance_for_Referring_Multi-object_Tracking_ICCV_2025_paper.pdf)
* [VOVTrack Exploring the Potentiality in Raw Videos for Open-Vocabulary Multi-Object Tracking](https://openaccess.thecvf.com/content/ICCV2025/papers/Qian_VOVTrack_Exploring_the_Potentiality_in_Raw_Videos_for_Open-Vocabulary_Multi-Object_ICCV_2025_paper.pdf)
* [LA-MOTR End-to-End Multi-Object Tracking by Learnable Association](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_LA-MOTR_End-to-End_Multi-Object_Tracking_by_Learnable_Association_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/PenK1nG/LA-MOTR)
* [General Compression Framework for Efficient Transformer Object Tracking](http://arxiv.org/abs/2409.17564)<br>:star:[code](https://github.com/LingyiHongfd/CompressTracker)
* [BoxDreamer Dreaming Box Corners for Generalizable Object Pose Estimation](http://arxiv.org/abs/2504.07955)
* [MixRI Mixing Features of Reference Images for Novel Object Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_MixRI_Mixing_Features_of_Reference_Images_for_Novel_Object_Pose_ICCV_2025_paper.pdf)
* [From Easy to Hard Progressive Active Learning Framework for Infrared Small Target Detection with Single Point Supervision](http://arxiv.org/abs/2412.11154)<br>:star:[code](https://github.com/YuChuang1205/PAL)
* [Text-IRSTD Leveraging Semantic Text to Promote Infrared Small Target Detection in Complex Scenes](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_Text-IRSTD_Leveraging_Semantic_Text_to_Promote_Infrared_Small_Target_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Zhengsy0407/Text-IRSTD)
* [Uncertainty-Aware Gradient Stabilization for Small Object Detection](http://arxiv.org/abs/2303.01803)
* [DM-EFS Dynamically Multiplexed Expanded Features Set Form for Robust and Efficient Small Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Sharma_DM-EFS_Dynamically_Multiplexed_Expanded_Features_Set_Form_for_Robust_and_ICCV_2025_paper.pdf)
* [Dual-Rate Dynamic Teacher for Source-Free Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/He_Dual-Rate_Dynamic_Teacher_for_Source-Free_Domain_Adaptive_Object_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/qih96/DDT)
* [Debiased Teacher for Day-to-Night Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Cui_Debiased_Teacher_for_Day-to-Night_Domain_Adaptive_Object_Detection_ICCV_2025_paper.pdf)
* [Benefit From Seen Enhancing Open-Vocabulary Object Detection by Bridging Visual and Textual Co-Occurrence Knowledge](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Benefit_From_Seen_Enhancing_Open-Vocabulary_Object_Detection_by_Bridging_Visual_ICCV_2025_paper.pdf)
* [OpenRSD Towards Open-prompts for Object Detection in Remote Sensing Images](http://arxiv.org/abs/2503.06146)
* [Dual Domain Control via Active Learning for Remote Sensing Domain Incremental Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Dual_Domain_Control_via_Active_Learning_for_Remote_Sensing_Domain_ICCV_2025_paper.pdf)
* [Active Learning Meets Foundation Models Fast Remote Sensing Data Annotation for Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Burges_Active_Learning_Meets_Foundation_Models_Fast_Remote_Sensing_Data_Annotation_ICCV_2025_paper.pdf)
* [LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing Object Detection](http://arxiv.org/abs/2509.16970)
* [Fusion Meets Diverse Conditions A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Fusion_Meets_Diverse_Conditions_A_High-diversity_Benchmark_and_Baseline_for_ICCV_2025_paper.pdf)
* [STEP-DETR Advancing DETR-based Semi-Supervised Object Detection with Super Teacher and Pseudo-Label Guided Text Queries](https://openaccess.thecvf.com/content/ICCV2025/papers/Shehzadi_STEP-DETR_Advancing_DETR-based_Semi-Supervised_Object_Detection_with_Super_Teacher_and_ICCV_2025_paper.pdf)
* [Power of Cooperative Supervision Multiple Teachers Framework for Advanced 3D Semi-Supervised Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_Power_of_Cooperative_Supervision_Multiple_Teachers_Framework_for_Advanced_3D_ICCV_2025_paper.pdf)
* [ASGS Single-Domain Generalizable Open-Set Object Detection via Adaptive Subgraph Searching](https://openaccess.thecvf.com/content/ICCV2025/papers/Yuan_ASGS_Single-Domain_Generalizable_Open-Set_Object_Detection_via_Adaptive_Subgraph_Searching_ICCV_2025_paper.pdf)
* [Gradient-Reweighted Adversarial Camouflage for Physical Object Detection Evasion](https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_Gradient-Reweighted_Adversarial_Camouflage_for_Physical_Object_Detection_Evasion_ICCV_2025_paper.pdf)
* [Unified Category-Level Object Detection and Pose Estimation from RGB Images using 3D Prototypes](http://arxiv.org/abs/2508.02157)
* [Cycle-Consistent Learning for Joint Layout-to-Image Generation and Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Cai_Cycle-Consistent_Learning_for_Joint_Layout-to-Image_Generation_and_Object_Detection_ICCV_2025_paper.pdf)
* [Rethinking Multi-modal Object Detection from the Perspective of Mono-Modality Feature Learning](http://arxiv.org/abs/2503.11780)<br>:star:[code](https://github.com/Zhao-Tian-yi/M2D-LIF)
* [Automated Model Evaluation for Object Detection via Prediction Consistency and Reliability](http://arxiv.org/abs/2508.12082)<br>:star:[code](https://github.com/YonseiML/autoeval-det)
* [Diffusion-based Source-biased Model for Single Domain Generalized Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_Diffusion-based_Source-biased_Model_for_Single_Domain_Generalized_Object_Detection_ICCV_2025_paper.pdf)
* [VISO Accelerating In-orbit Object Detection with Language-Guided Mask Learning and Sparse Inference](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_VISO_Accelerating_In-orbit_Object_Detection_with_Language-Guided_Mask_Learning_and_ICCV_2025_paper.pdf)
* [Beyond RGB Adaptive Parallel Processing for RAW Object Detection](http://arxiv.org/abs/2503.13163)
* [Dark-ISP Enhancing RAW Image Processing for Low-Light Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Guo_Dark-ISP_Enhancing_RAW_Image_Processing_for_Low-Light_Object_Detection_ICCV_2025_paper.pdf)
* [DoppDrive Doppler-Driven Temporal Aggregation for Improved Radar Object Detection](http://arxiv.org/abs/2508.12330)<br>:house:[project](https://yuvalhg.github.io/DoppDrive) :house:[project](https://yuvalhg.github.io/DoppDrive/)
* [Continual Adaptation Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios](http://arxiv.org/abs/2506.24063)
----------------------------------------------------------------------------












## 计算成像
* [Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras](http://arxiv.org/pdf/2506.22069v1)
* [Estimating 2D Camera Motion with Hybrid Motion Basis](https://arxiv.org/pdf/2507.22480v1)<br>:star:[code](https://lhaippp.github.io/CamFlow/)<br>:star:[code](https://github.com/lhaippp/camflow)
* [Super Resolved Imaging with Adaptive Optics](https://arxiv.org/pdf/2508.04648v1)<br>:house:[project](https://www.cs.toronto.edu/~robin/aosr/)
* [HccePose(BF) Predicting Front  Back Surfaces to Construct Ultra-Dense 2D-3D Correspondences for Pose Estimation](http://arxiv.org/abs/2510.10177)
* [RePoseD Efficient Relative Pose Estimation With Known Depth Information](https://openaccess.thecvf.com/content/ICCV2025/papers/Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/kocurvik/mdrp)
* [Scaling 3D Compositional Models for Robust Classification and Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Yuan_Scaling_3D_Compositional_Models_for_Robust_Classification_and_Pose_Estimation_ICCV_2025_paper.pdf)
* [DRaM-LHM A Quaternion Framework for Iterative Camera Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lin_DRaM-LHM_A_Quaternion_Framework_for_Iterative_Camera_Pose_Estimation_ICCV_2025_paper.pdf)

## Feature Matching(特征匹配)
* [Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching](https://arxiv.org/pdf/2507.10318v1)
* [CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance](https://arxiv.org/pdf/2507.17312v1)<br>:star:[code](https://github.com/pq-chen/CasP)
* [DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in Collaborative Perception](https://arxiv.org/pdf/2507.18237v1)<br>:star:[code](https://github.com/ChengchangTian/DATA)

## Dense Prediction
* [Frequency-Dynamic Attention Modulation for Dense Prediction](https://arxiv.org/pdf/2507.12006v1)<br>:star:[code](https://github.com/Linwei-Chen/FDAM)
* [FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment](https://arxiv.org/pdf/2506.22509v1)<br>:star:[code](https://github.com/xuhang07/FreeDNA)

## Gaze
* [Multi-view Gaze Target Estimation](https://arxiv.org/pdf/2508.05857v1)<br>:house:[project](https://www3.cs.stonybrook.edu/~cvl/multiview_gte.html)
* [Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction](https://arxiv.org/pdf/2507.23021v1)<br>:star:[code](https://aimagelab.github.io/ScanDiff)<br>:star:[code](https://github.com/aimagelab/scandiff)视觉注意力预测

## Visual Relationship Detection,VRD(视觉关系检测)
* [ART: Adaptive Relation Tuning for Generalized Relation Prediction](https://arxiv.org/pdf/2507.23543v1)

## Protecting copyright(保护版权)
* [TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity](https://arxiv.org/pdf/2506.23484v1)

## biometric recognition(生物特征识别)
* [A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition](https://arxiv.org/pdf/2508.00053v1)
* 指纹
  * [Training-Free Personalization via Retrieval and Reasoning on Fingerprints](http://arxiv.org/abs/2503.18623)

## Industrial Anomaly Detection(工业异常检测)
* [RareCLIP Rarity-aware Online Zero-shot Industrial Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/He_RareCLIP_Rarity-aware_Online_Zero-shot_Industrial_Anomaly_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/hjf02/RareCLIP)


## Animation(动画)
* [LayerAnimate: Layer-level Control for Animation](http://arxiv.org/abs/2501.08295)
* [Occlusion-robust Stylization for Drawing-based 3D Animation](https://arxiv.org/pdf/2508.00398v1)

## Sound
* [MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing](https://arxiv.org/pdf/2507.01384v1)<br>:star:[code](https://github.com/WangLY136/MUG)
* [What's Making That Sound Right Now? Video-centric Audio-Visual Localization](https://arxiv.org/pdf/2507.04667v1)<br>:star:[code](https://hahyeon610.github.io/Video-centric_Audio_Visual_Localization/)
* [Implicit Counterfactual Learning for Audio-Visual Segmentation](https://arxiv.org/pdf/2507.20740v1)
* [Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation](https://arxiv.org/pdf/2507.22886v1)<br>:house:[project](https://henghuiding.com/OmniAVS/)

## Dataset
* [Context-Aware Academic Emotion Dataset and Benchmark](https://arxiv.org/pdf/2507.00586v1)<br>:star:[code](https://zgsfer.github.io/CAER)
* [ROADWork A Dataset and Benchmark for Learning to Recognize Observe Analyze and Drive Through Work Zones](https://openaccess.thecvf.com/content/ICCV2025/papers/Ghosh_ROADWork_A_Dataset_and_Benchmark_for_Learning_to_Recognize_Observe_ICCV_2025_paper.pdf)
* 基准
  * [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/pdf/2507.14449v1)
  * [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/pdf/2507.15028v1)<br>:star:[code](https://zhangyuanhan-ai.github.io/video-tt/)
* 数据集
  * [Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning](https://arxiv.org/pdf/2507.04790v1)
  * [ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users](https://arxiv.org/pdf/2507.10223v1)<br>:star:[code](https://github.com/pittisl/ProGait)<br>:house:[project](https://huggingface.co/datasets/ericyxy98/ProGait)
  * [CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling](https://arxiv.org/pdf/2507.12591v1)
  * [Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions](https://arxiv.org/pdf/2508.04681v1)<br>:star:[code](https://liangxuy.github.io/InterVLA/)<br>:star:[code](https://github.com/liangxuy/intervla)
  * [HumanOLAT: A Large-Scale Dataset for Full-Body Human Relighting and Novel-View Synthesis](https://arxiv.org/pdf/2508.09137v1)<br>:house:[project](https://vcai.mpi-inf.mpg.de/projects/HumanOLAT/)
* 数据蒸馏
  * [CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation](https://arxiv.org/pdf/2506.22637v1)<br>:star:[code](https://github.com/hatchetProject/CaO2)
  * [Dataset Distillation via Vision-Language Category Prototype](https://arxiv.org/pdf/2506.23580v1)<br>:star:[code](https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/)
  * [Dataset Distillation as Data Compression: A Rate-Utility Perspective](https://arxiv.org/pdf/2507.17221v1)

## Neural Radiance Fields
* [UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields](http://arxiv.org/pdf/2506.21884v1)<br>:house:[project](https://www.factral.co/UnMix-NeRF)
* [LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling](https://arxiv.org/pdf/2507.02363v1)<br>:star:[code](https://wujh2001.github.io/LocalDyGS/)
* [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/pdf/2507.14596v1)
* [A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields](https://arxiv.org/pdf/2507.04408v1)
* [NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement](https://arxiv.org/pdf/2507.12714v1)<br>:star:[code](https://neuraleaf-yang.github.io/)
* [MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction](https://arxiv.org/pdf/2508.04297v1)
* 渲染
  * [InvRGB+L: Inverse Rendering of Complex Scenes with Unified Color and LiDAR Reflectance Modeling](https://arxiv.org/pdf/2507.17613v1)
  * [BokehDiff: Neural Lens Blur with One-Step Diffusion](https://arxiv.org/pdf/2507.18060v1)
  * [OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene Reconstruction and Rendering](http://arxiv.org/abs/2503.16177)<br>:house:[project](https://occlugaussian.github.io)
* 逆向渲染
  * [Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues](https://arxiv.org/pdf/2507.23162v1)
* NVS
  * [FVGen Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation](http://arxiv.org/abs/2508.06392)
  * [E-NeMF Event-based Neural Motion Field for Novel Space-time View Synthesis of Dynamic Scenes](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_E-NeMF_Event-based_Neural_Motion_Field_for_Novel_Space-time_View_Synthesis_ICCV_2025_paper.pdf)


## Vision Language(视觉语言)
* [NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/pdf/2506.23468v1)<br>:star:[code](https://github.com/Feliciaxyao/NavMorph)
* [Improving Large Vision and Language Models by Learning from a Panel of Peers](http://arxiv.org/abs/2509.01610)
* [ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models](https://arxiv.org/pdf/2507.00898v1)<br>:star:[code](https://github.com/zifuwan/ONLY)<br>:star:[code](https://zifuwan.github.io/ONLY/)
* [ViLU: Learning Vision-Language Uncertainties for Failure Prediction](https://arxiv.org/pdf/2507.07620v1)<br>:star:[code](https://github.com/ykrmm/ViLU)
* [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/pdf/2507.08979v1)<br>:star:[code](https://github.com/MahdiyarMM/PRISM)
* [One Last Attention for Your Vision-Language Model](https://arxiv.org/pdf/2507.15480v1)<br>:star:[code](https://github.com/khufia/RAda/tree/main)
* [Hierarchical Cross-modal Prompt Learning for Vision-Language Models](https://arxiv.org/pdf/2507.14976v1)<br>:star:[code](https://github.com/zzeoZheng/HiCroPL)
* [METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models](https://arxiv.org/pdf/2507.20842v1)<br>:star:[code](https://github.com/YuchenLiu98/METEOR)
* [ATCTrack: Aligning Target-Context Cues with Dynamic Target States for Robust Vision-Language Tracking](https://arxiv.org/pdf/2507.19875v1)<br>:star:[code](https://github.com/XiaokunFeng/ATCTrack)
* [AgroBench: Vision-Language Model Benchmark in Agriculture](https://arxiv.org/pdf/2507.20519v1)<br>:star:[code](https://dahlian00.github.io/AgroBenchPage/)
* [MM-IFEngine Towards Multimodal Instruction Following](https://openaccess.thecvf.com/content/ICCV2025/papers/Ding_MM-IFEngine_Towards_Multimodal_Instruction_Following_ICCV_2025_paper.pdf)
* VLN
  * [Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities](https://arxiv.org/pdf/2507.13019v1)<br>:star:[code](https://crystalsixone.github.io/vln_pe.github.io/)
  * [monoVLN Bridging the Observation Gap between Monocular and Panoramic Vision and Language Navigation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_monoVLN_Bridging_the_Observation_Gap_between_Monocular_and_Panoramic_Vision_ICCV_2025_paper.pdf)
* LLM
  * [LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching](https://arxiv.org/pdf/2506.23502v1)
  * [Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs](https://arxiv.org/pdf/2507.07990v1)<br>:house:[project](https://www.jshyun.me/projects/sttm)
  * [Why LVLMs Are More Prone to Hallucinations in Longer Responses The Role of Context](https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_Why_LVLMs_Are_More_Prone_to_Hallucinations_in_Longer_Responses_ICCV_2025_paper.pdf)
  * [Zeroth-Order Fine-Tuning of LLMs in Random Subspaces](http://arxiv.org/abs/2410.08989)<br>:star:[code](https://github.com/zimingyy/SubZero)
  * [Advancing Visual Large Language Model for Multi-granular Versatile Perception](https://arxiv.org/pdf/2507.16213v1)<br>:star:[code](https://github.com/xiangwentao666/MVP-LM)
* MLLM
  * [Token Activation Map to Visually Explain Multimodal LLMs](https://arxiv.org/pdf/2506.23270v1)
  * [DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs](https://arxiv.org/pdf/2507.10302v1)<br>:star:[code](https://github.com/ZJHTerry18/DisCo)
  * [UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding](https://arxiv.org/pdf/2506.23219v1)<br>:star:[code](https://github.com/tsinghua-fib-lab/UrbanLLaVA)
  * [FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging](https://arxiv.org/pdf/2508.04625v1)
  * [Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation](https://arxiv.org/pdf/2507.02859v1)
  * [BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models](https://arxiv.org/pdf/2508.06895v1)
  * [Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning](https://arxiv.org/pdf/2507.07424v1)<br>:star:[code](https://mm-vl.github.io/corvid)
  * [Instruction-Oriented Preference Alignment for Enhancing Multi-Modal Comprehension Capability of MLLMs](http://arxiv.org/abs/2503.20309)

## Vision Transformer
* [Boosting Generative Adversarial Transferability with Self-supervised Vision Transformer Features](http://arxiv.org/pdf/2506.21046v1)<br>:star:[code](https://github.com/spencerwooo/dSVA)
* [Efficient Adaptation of Pre-trained Vision Transformer underpinned by Approximately Orthogonal Fine-Tuning Strategy](https://arxiv.org/pdf/2507.13260v1)
* [EA-ViT: Efficient Adaptation for Elastic Vision Transformer](https://arxiv.org/pdf/2507.19360v1)<br>:star:[code](https://github.com/zcxcf/EA-ViT)
* [MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective](https://arxiv.org/pdf/2507.19131v1)
* [OminiControl Minimal and Universal Control for Diffusion Transformer](http://arxiv.org/abs/2411.15098)
* [Pinco Position-induced Consistent Adapter for Diffusion Transformer in Foreground-conditioned Inpainting](http://arxiv.org/abs/2412.03812)

## Deep learning(深度学习)
* RNN
  * [ResQ: A Novel Framework to Implement Residual Neural Networks on Analog Rydberg Atom Quantum Computers](http://arxiv.org/pdf/2506.21537v1)

## Machine learning(机器学习)
* 主动学习
  * [To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models](https://arxiv.org/pdf/2507.15381v1)<br>:star:[code](https://github.com/juliamachnio/PALM)
  * [Consensus-Driven Active Model Selection](https://arxiv.org/pdf/2507.23771v1)<br>:star:[code](https://github.com/justinkay/coda)
* 对比学习
  * [Vector Contrastive Learning For Pixel-Wise Pretraining In Medical Vision](http://arxiv.org/pdf/2506.20850v1)  
  * [Selective Contrastive Learning for Weakly Supervised Affordance Grounding](https://arxiv.org/pdf/2508.07877v1)
* 强化学习
  * [RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment](http://arxiv.org/pdf/2506.21037v1)
  * [RIPE: Reinforcement Learning on Unlabeled Image Pairs for Robust Keypoint Extraction](https://arxiv.org/pdf/2507.04839v1)<br>:star:[code](https://github.com/fraunhoferhhi/RIPE)
  * [DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding](https://arxiv.org/pdf/2508.08589v1)<br>:star:[code](https://github.com/wenwenyu/DocThinker)
* 持续学习
  * [CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization](http://arxiv.org/pdf/2506.21117v1)<br>:star:[code](https://cl-splats.github.io)
  * [PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning](https://arxiv.org/pdf/2507.12305v1)<br>:star:[code](https://github.com/anwarmaxsum/PROL)
  * [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/pdf/2507.09118v1)<br>:star:[code](https://github.com/linlany/MindtheGap)
  * [RainbowPrompt: Diversity-Enhanced Prompt-Evolving for Continual Learning](https://arxiv.org/pdf/2507.22553v1)
  * [Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models](https://arxiv.org/pdf/2508.00260v1)
  * [Divide-and-Conquer for Enhancing Unlabeled Learning, Stability, and Plasticity in Semi-supervised Continual Learning](https://arxiv.org/pdf/2508.05316v1)<br>:star:[code](https://github.com/NJUyued/USP4SSCL)
  * [Any-SSR How Recursive Least Squares Works in Continual Learning of Large Language Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Tong_Any-SSR_How_Recursive_Least_Squares_Works_in_Continual_Learning_of_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ZHUANGHP/Any-SSR)
* 对抗学习
  * [TITAN: Query-Token based Domain Adaptive Adversarial Learning](http://arxiv.org/pdf/2506.21484v1)
  * [ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models](https://arxiv.org/pdf/2507.21985v1)
  * [DISTIL: Data-Free Inversion of Suspicious Trojan Inputs via Latent Diffusion](https://arxiv.org/pdf/2507.22813v1)<br>:star:[code](https://github.com/AdaptiveMotorControlLab/DISTIL)
  * [Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights](https://arxiv.org/pdf/2508.00649v1)<br>:star:[code](https://github.com/Gandolfczjh/APDE)
  * [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/pdf/2508.05689v1)<br>:star:[code](https://github.com/ZezeTao/ResPA)
* 多模态学习
  * [G$^{2}$D: Boosting Multimodal Learning with Gradient-Guided Distillation](http://arxiv.org/pdf/2506.21514v1)<br>:star:[code](https://github.com/rAIson-Lab/G2D)
  * [Improving Multimodal Learning via Imbalanced Learning](https://arxiv.org/pdf/2507.10203v1)<br>:star:[code](https://github.com/shicaiwei123/ICCV2025-ARL)
  * [SimMLM: A Simple Framework for Multi-modal Learning with Missing Modality](https://arxiv.org/pdf/2507.19264v1)
* 多任务学习
  * [Resolving Token-Space Gradient Conflicts: Token Space Manipulation for Transformer-Based Multi-Task Learning](https://arxiv.org/pdf/2507.07485v1)
  * [Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning](https://arxiv.org/pdf/2507.21049v1)<br>:star:[code](https://jacky1128.github.io/RepMTL/)
* 类增量学习
  * [Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning](https://arxiv.org/pdf/2507.09183v1)<br>:star:[code](https://github.com/Jywsuperman/LGSP)
  * [Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning](https://arxiv.org/pdf/2508.08165v1)<br>:star:[code](https://github.com/LAMDA-CL/ICCV2025-TUNA)
* 增量学习
  * [Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning](https://arxiv.org/pdf/2507.21588v1)<br>:star:[code](https://github.com/ENJOY-Yin-jiong/PHP)
* 联邦学习
  * [Federated Representation Angle Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Yi_Federated_Representation_Angle_Learning_ICCV_2025_paper.pdf)
* 元学习
  * [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/pdf/2508.06301v1)

##
* [Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention](https://arxiv.org/pdf/2507.01417v1)
* [NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection](https://arxiv.org/pdf/2507.09795v1)<br>:star:[code](https://github.com/ah-ansari/NegRefine)
* 异常检测
  * [Toward Long-Tailed Online Anomaly Detection through Class-Agnostic Concepts](https://arxiv.org/pdf/2507.16946v1)<br>:house:[project](https://doi.org/10.5281/zenodo.16283852)

##
* 零样本
  * [Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model](https://arxiv.org/pdf/2506.23822v1)<br>:star:[code](https://github.com/shiming-chen/LaZSL)
  * [OBSER: Object-Based Sub-Environment Recognition for Zero-Shot Environmental Inference](https://arxiv.org/pdf/2507.02929v1)
* DG
  * [Bridging Domain Generalization to Multimodal Domain Generalization via Unified Representations](https://arxiv.org/pdf/2507.03304v1)
  * [Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization](https://arxiv.org/pdf/2507.04302v1)
  * [AdaDCP Learning an Adapter with Discrete Cosine Prior for Clear-to-Adverse Domain Generalization](https://openaccess.thecvf.com/content/ICCV2025/papers/Bi_AdaDCP_Learning_an_Adapter_with_Discrete_Cosine_Prior_for_Clear-to-Adverse_ICCV_2025_paper.pdf)

##
* [Representation Shift: Unifying Token Compression with FlashAttention](https://arxiv.org/pdf/2508.00367v1)<br>:star:[code](https://github.com/mlvlab/Representation-Shift)
* 剪枝
  * [Partial Forward Blocking: A Novel Data Pruning Paradigm for Lossless Training Acceleration](https://arxiv.org/pdf/2506.23674v1)
  * [Variance-Based Pruning for Accelerating and Compressing Trained Networks](https://arxiv.org/pdf/2507.12988v1)
* KD
  * [Frequency-Aligned Knowledge Distillation for Lightweight Spatiotemporal Forecasting](https://arxiv.org/pdf/2507.02939v1)<br>:star:[code](https://github.com/itsnotacie/SDKD)
  * [Local Dense Logit Relations for Enhanced Knowledge Distillation](https://arxiv.org/pdf/2507.15911v1)
  * [Cross-Architecture Distillation Made Simple with Redundancy Suppression](https://arxiv.org/pdf/2507.21844v1)
* 量化
  * [DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization](https://arxiv.org/pdf/2507.12933v1)<br>:star:[code](https://github.com/LeeDongYeun/dmq)

## Scene Graph Generation(场景图生成)
* [SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning](https://arxiv.org/pdf/2507.05798v1)
* [Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images](https://arxiv.org/pdf/2508.06546v1)<br>:star:[code](https://qixun1.github.io/projects/SCRSSG)<br>:star:[code](https://github.com/qixun1/scrsssg)

## Style Transfer(风格迁移)
* [Domain Generalizable Portrait Style Transfer](https://arxiv.org/pdf/2507.04243v1)<br>:star:[code](https://github.com/wangxb29/DGPST)

## Object Pose Estimation(物体姿态估计)
* [Deterministic Object Pose Confidence Region Estimation](https://arxiv.org/pdf/2506.22720v1)
* [BoxDreamer Dreaming Box Corners for Generalizable Object Pose Estimation](http://arxiv.org/abs/2504.07955)
* [MixRI Mixing Features of Reference Images for Novel Object Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_MixRI_Mixing_Features_of_Reference_Images_for_Novel_Object_Pose_ICCV_2025_paper.pdf)
* [Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures](https://arxiv.org/pdf/2507.10265v1)<br>:star:[code](https://wakuwu.github.io/KBA)
* [Environment-Agnostic Pose Generating Environment-independent Object Representations for 6D Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Environment-Agnostic_Pose_Generating_Environment-independent_Object_Representations_for_6D_Pose_Estimation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/acmff22/EA6D) :house:[project](https://github.com/acmff22/EA6D)
* [Ultra-Precision 6DoF Pose Estimation Using 2-D Interpolated Discrete Fourier Transform](https://openaccess.thecvf.com/content/ICCV2025/papers/Shi_Ultra-Precision_6DoF_Pose_Estimation_Using_2-D_Interpolated_Discrete_Fourier_Transform_ICCV_2025_paper.pdf)


##
* 关键点检测
  * [Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection](https://arxiv.org/pdf/2507.07994v1)<br>:house:[project](https://subhajitmaity.me/DYKp)


## Deepfake Detection/AI生成图像检测
* [Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection](https://arxiv.org/pdf/2507.02398v1)<br>:star:[code](https://github.com/rama0126/PwTF-DVD)
* 图像伪造定位
  * [M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization](http://arxiv.org/pdf/2506.20922v1)
* AI生成图片检测
  * [AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models](https://arxiv.org/pdf/2507.02664v1)
  * [Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful Illusions](https://arxiv.org/pdf/2507.22617v1)
* 视频伪造检测
  * [HumanSAM: Classifying Human-centric Forgery Videos in Human Spatial, Appearance, and Motion Anomaly](https://arxiv.org/pdf/2507.19924v1)<br>:star:[code](https://dejian-lc.github.io/humansam/)

## Optical Flow Estimation(光流估计)
* [MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation](https://arxiv.org/pdf/2506.23151v1)<br>:star:[code](https://github.com/msu-video-group/memfof)
* [FlowSeek Optical Flow Made Easier with Depth Foundation Models and Motion Bases](http://arxiv.org/abs/2509.05297)
* [PriOr-Flow Enhancing Primitive Panoramic Optical Flow with Orthogonal View](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View_ICCV_2025_paper.pdf)
* [Flow4Agent Long-form Video Understanding via Motion Prior from Optical Flow](http://arxiv.org/abs/2510.05836)
* [EMatch A Unified Framework for Event-based Optical Flow and Stereo Matching](http://arxiv.org/abs/2407.21735)
* [Removing Cost Volumes from Optical Flow Estimators](https://openaccess.thecvf.com/content/ICCV2025/papers/Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators_ICCV_2025_paper.pdf)

## Visual Question Answering(视觉问答)
* [ReasonVQA: A Multi-hop Reasoning Benchmark with Structural Knowledge for Visual Question Answering](https://arxiv.org/pdf/2507.16403v1)
* [ToolVQA A Dataset for Multi-step Reasoning VQA with External Tools](http://arxiv.org/abs/2508.03284)<br>:star:[code](https://github.com/Fugtemypt123/ToolVQA-release)
* 数学问题解决
  * [VisionMath Vision-Form Mathematical Problem-Solving](https://openaccess.thecvf.com/content/ICCV2025/papers/Ma_VisionMath_Vision-Form_Mathematical_Problem-Solving_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/mengqiDyangge/VisionMath)

## Robot
* [GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization](https://arxiv.org/pdf/2507.10935v1)<br>:star:[code](https://github.com/tongshw/GeoDistill)
* 虚拟试穿
  * [OmniVTON: Training-Free Universal Virtual Try-On](https://arxiv.org/pdf/2507.15037v1)<br>:star:[code](https://github.com/Jerome-Young/OmniVTON)
* 机器人
  * [RoboPearls: Editable Video Simulation for Robot Manipulation](https://arxiv.org/pdf/2506.22756v1)
  * [AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning](https://arxiv.org/pdf/2508.07626v1)
  * [Adaptive Articulated Object Manipulation On The Fly with Foundation Model Reasoning and Part Grounding](https://arxiv.org/pdf/2507.18276v1)
  * [Recognizing Actions from Robotic View for Natural Human-Robot Interaction](https://arxiv.org/pdf/2507.22522v1)<br>:star:[code](https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View)
  * [Moto Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos](http://arxiv.org/abs/2412.04445)
  * [IRASim A Fine-Grained World Model for Robot Manipulation](http://arxiv.org/abs/2406.14540)<br>:house:[project](https://gen-irasim.github.io/)
  * [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/pdf/2508.00697v1)
  * [RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping](https://arxiv.org/pdf/2507.23734v1)<br>:star:[code](https://github.com/wudongming97/AffordanceNet)
  * [PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation](https://arxiv.org/pdf/2508.05976v1)
  * [Learning Precise Affordances from Egocentric Videos for Robotic Manipulation](http://arxiv.org/abs/2408.10123)<br>:house:[project](https://reagan1311.github.io/affgrasp)
  * [iManip Skill-Incremental Learning for Robotic Manipulation](http://arxiv.org/abs/2503.07087)
  * [RoBridge A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation](http://arxiv.org/abs/2505.01709)
  * [EC-Flow Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_EC-Flow_Enabling_Versatile_Robotic_Manipulation_from_Action-Unlabeled_Videos_via_Embodiment-Centric_ICCV_2025_paper.pdf)<br>:house:[project](https://ec-flow1.github.io/)
  * [A0 An Affordance-Aware Hierarchical Model for General Robotic Manipulation](http://arxiv.org/abs/2504.12636)
  * [Rethinking Bimanual Robotic Manipulation Learning with Decoupled Interaction Framework](http://arxiv.org/abs/2503.09186)
  * [GWM Towards Scalable Gaussian World Models for Robotic Manipulation](http://arxiv.org/abs/2508.17600)
  * [Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control](http://arxiv.org/abs/2505.15304)
  * [Learning 4D Embodied World Models](http://arxiv.org/abs/2504.20995)
  * [RobAVA A Large-scale Dataset and Baseline Towards Video based Robotic Arm Action Understanding](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_RobAVA_A_Large-scale_Dataset_and_Baseline_Towards_Video_based_Robotic_ICCV_2025_paper.pdf)
  * [RoboAnnotatorX A Comprehensive and Universal Annotation Framework for Accurate Understanding of Long-horizon Robot Demonstration](https://openaccess.thecvf.com/content/ICCV2025/papers/Kou_RoboAnnotatorX_A_Comprehensive_and_Universal_Annotation_Framework_for_Accurate_Understanding_ICCV_2025_paper.pdf)
  * [4D Visual Pre-training for Robot Learning](http://arxiv.org/abs/2508.17230)<br>:house:[project](https://4d-visual-pretraining.github.io/)
  * [G-DexGrasp Generalizable Dexterous Grasping Synthesis Via Part-Aware Prior Retrieval and Prior-Assisted Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Jian_G-DexGrasp_Generalizable_Dexterous_Grasping_Synthesis_Via_Part-Aware_Prior_Retrieval_and_ICCV_2025_paper.pdf)
  * [DexH2R A Benchmark for Dynamic Dexterous Grasping in Human-to-Robot Handover](http://arxiv.org/abs/2506.23152)
  * [OVA-Fields Weakly Supervised Open-Vocabulary Affordance Fields for Robot Operational Part Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Su_OVA-Fields_Weakly_Supervised_Open-Vocabulary_Affordance_Fields_for_Robot_Operational_Part_ICCV_2025_paper.pdf)
  * Object Discovery
    * [Ensemble Foreground Management for Unsupervised Object Discovery](https://arxiv.org/pdf/2507.20860v1)<br>:star:[code](https://github.com/YFaris/UnionCut)
* SLAM
  * [Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps](https://arxiv.org/pdf/2507.03737v1)<br>:star:[code](https://3dagentworld.github.io/S3PO-GS/)
  * [DyGS-SLAM Real-Time Accurate Localization and Gaussian Reconstruction for Dynamic Scenes](https://openaccess.thecvf.com/content/ICCV2025/papers/Hu_DyGS-SLAM_Real-Time_Accurate_Localization_and_Gaussian_Reconstruction_for_Dynamic_Scenes_ICCV_2025_paper.pdf)
  * [4D Gaussian Splatting SLAM](http://arxiv.org/abs/2503.16710)
  * [ToF-Splatting Dense SLAM using Sparse Time-of-Flight Depth and Multi-Frame Integration](https://openaccess.thecvf.com/content/ICCV2025/papers/Conti_ToF-Splatting_Dense_SLAM_using_Sparse_Time-of-Flight_Depth_and_Multi-Frame_Integration_ICCV_2025_paper.pdf)
  * [Benchmarking Egocentric Visual-Inertial SLAM at City Scale](http://arxiv.org/abs/2509.26639)
  * [SuperEvent Cross-Modal Learning of Event-based Keypoint Detection for SLAM](http://arxiv.org/abs/2504.00139)<br>:house:[project](https://ethz-mrl.github.io/SuperEvent)
  * [SEGS-SLAM Structure-enhanced 3D Gaussian Splatting SLAM with Appearance Embedding](https://openaccess.thecvf.com/content/ICCV2025/papers/Wen_SEGS-SLAM_Structure-enhanced_3D_Gaussian_Splatting_SLAM_with_Appearance_Embedding_ICCV_2025_paper.pdf)<br>:house:[project](https://segs-slam.github.io/)
  * [Underwater Visual SLAM with Depth Uncertainty and Medium Modeling](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling_ICCV_2025_paper.pdf)
* 导航
  * [IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation](https://arxiv.org/pdf/2508.00823v1)<br>:star:[code](https://gwxuan.github.io/IGL-Nav/)<br>:star:[code](https://github.com/gwxuan/igl-nav)
  * [Embodied Navigation with Auxiliary Task of Action Description Prediction](https://openaccess.thecvf.com/content/ICCV2025/papers/Kondoh_Embodied_Navigation_with_Auxiliary_Task_of_Action_Description_Prediction_ICCV_2025_paper.pdf)
  * [EmbodiedSplat Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device](http://arxiv.org/abs/2509.17430)<br>:house:[project](https://gchhablani.github.io/embodied-splat)
  * [GUIOdyssey A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices](http://arxiv.org/abs/2406.08451)
  * [Learning on the Go A Meta-learning Object Navigation Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Learning_on_the_Go_A_Meta-learning_Object_Navigation_Model_ICCV_2025_paper.pdf)
  * [RoboTrom-Nav A Unified Framework for Embodied Navigation Integrating Perception Planning and Prediction](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhong_RoboTrom-Nav_A_Unified_Framework_for_Embodied_Navigation_Integrating_Perception_Planning_ICCV_2025_paper.pdf)
  * [MoMa-Kitchen A 100K Benchmark for Affordance-Grounded Last-Mile Navigation in Mobile Manipulation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_MoMa-Kitchen_A_100K_Benchmark_for_Affordance-Grounded_Last-Mile_Navigation_in_Mobile_ICCV_2025_paper.pdf)
  * [Active Perception Meets Rule-Guided RL A Two-Phase Approach for Precise Object Navigation in Complex Environments](https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Active_Perception_Meets_Rule-Guided_RL_A_Two-Phase_Approach_for_Precise_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/qinliangql/APRR)
  * [SAME Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts](http://arxiv.org/abs/2412.05552)
  * [Collaborative Instance Object Navigation Leveraging Uncertainty-Awareness to Minimize Human-Agent Dialogues](http://arxiv.org/abs/2412.01250)
  * [CogNav Cognitive Process Modeling for Object Goal Navigation with LLMs](http://arxiv.org/abs/2412.10439)
  * [DialNav Multi-turn Dialog Navigation with a Remote Guide](http://arxiv.org/abs/2509.12894)<br>:house:[project](https://happilee12.github.io/DialNav)
  * [LookOut Real-World Humanoid Egocentric Navigation](https://openaccess.thecvf.com/content/ICCV2025/papers/Pan_LookOut_Real-World_Humanoid_Egocentric_Navigation_ICCV_2025_paper.pdf)
  * [CityNav A Large-Scale Dataset for Real-World Aerial Navigation](http://arxiv.org/abs/2406.14240)
  * [Function-centric Bayesian Network for Zero-Shot Object Goal Navigation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Function-centric_Bayesian_Network_for_Zero-Shot_Object_Goal_Navigation_ICCV_2025_paper.pdf)





## Human-Object Interaction Detection(人机交互)
* [Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss](https://arxiv.org/pdf/2507.01630v1)<br>:star:[code](https://github.com/YuxiaoWang-AI/P3HOT)
* [Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection](https://arxiv.org/pdf/2507.06510v1)
* [Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers](https://arxiv.org/pdf/2507.11287v1)<br>:star:[code](https://hcis-lab.github.io/TOHGS/)
* [HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation](https://arxiv.org/pdf/2507.15542v1)<br>:star:[code](https://github.com/ChelsieLei/HOLa)
* [Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting](https://arxiv.org/pdf/2508.00427v1)
* [SyncDiff Synchronized Motion Diffusion for Multi-Body Human-Object Interaction Synthesis](http://arxiv.org/abs/2412.20104)

## Autonomous Driving(自动驾驶)
* [Epona: Autoregressive Diffusion World Model for Autonomous Driving](https://arxiv.org/pdf/2506.24113v1)<br>:star:[code](https://kevin-thu.github.io/Epona/)<br>:star:[code](https://github.com/Kevin-thu/Epona/)
* [AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving](https://arxiv.org/pdf/2507.12137v1)
* [World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model](https://arxiv.org/pdf/2507.00603v1)<br>:star:[code](https://github.com/ucaszyp/World4Drive)
* [3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation](https://arxiv.org/pdf/2507.01367v1)<br>:star:[code](https://github.com/TRLou/PGA)
* [Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge](https://arxiv.org/pdf/2507.04123v1)
* [GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous Driving with Gaussian Splatting](https://arxiv.org/pdf/2507.19451v1)<br>:star:[code](https://gs-occ3d.github.io/)
* [From Gaze to Movement Predicting Visual Attention for Autonomous Driving Human-Machine Interaction based on Programmatic Imitation Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_From_Gaze_to_Movement_Predicting_Visual_Attention_for_Autonomous_Driving_ICCV_2025_paper.pdf)
* [MamV2XCalib: V2X-based Target-less Infrastructure Camera Calibration with State Space Model](https://arxiv.org/pdf/2507.23595v1)<br>:star:[code](https://github.com/zhuyaoye/MamV2XCalib)
* [VLR-Driver Large Vision-Language-Reasoning Models for Embodied Autonomous Driving](https://openaccess.thecvf.com/content/ICCV2025/papers/Kong_VLR-Driver_Large_Vision-Language-Reasoning_Models_for_Embodied_Autonomous_Driving_ICCV_2025_paper.pdf)
* [RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case](https://arxiv.org/pdf/2508.04642v1)<br>:star:[code](https://stars79689.github.io/RoboTron-Sim/)<br>:star:[code](https://github.com/stars79689/robotron-sim)
* 三维占据
  * [Occupancy Learning with Spatiotemporal Memory](https://arxiv.org/pdf/2508.04705v1)<br>:star:[code](https://matthew-leng.github.io/stocc)
* 轨迹预测
  * [Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics](https://arxiv.org/pdf/2507.12083v1)
  * [Generative Active Learning for Long-tail Trajectory Prediction via Controllable Diffusion Model](https://arxiv.org/pdf/2507.22615v1)
* VLA
  * [CoA-VLA Improving Vision-Language-Action Models via Visual-Text Chain-of-Affordance](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_CoA-VLA_Improving_Vision-Language-Action_Models_via_Visual-Text_Chain-of-Affordance_ICCV_2025_paper.pdf)
  * [VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers](https://arxiv.org/pdf/2507.01016v1)<br>:star:[code](https://xiaoxiao0406.github.io/vqvla.github.io)
* 占用预测
  * [Language Driven Occupancy Prediction](http://arxiv.org/abs/2411.16072)

## Point Cloud(点云)
* [GAP: Gaussianize Any Point Clouds with Text Guidance](https://arxiv.org/pdf/2508.05631v1)<br>:star:[code](https://weiqi-zhang.github.io/GAP)<br>:star:[code](https://github.com/weiqi-zhang/gap)
* [StruMamba3D: Exploring Structural Mamba for Self-supervised Point Cloud Representation Learning](http://arxiv.org/pdf/2506.21541v1)
* [PointGAC: Geometric-Aware Codebook for Masked Point Cloud Modeling](https://arxiv.org/pdf/2507.04801v1)<br>:star:[code](https://github.com/LAB123-tech/PointGAC)
* [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/pdf/2507.09102v1)<br>:star:[code](https://github.com/wdttt/PointSD)
* [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/pdf/2507.15686v1)<br>:star:[code](https://huangwenjie2023.github.io/LINR-PCGC/)
* [Blended Point Cloud Diffusion for Localized Text-guided Shape Editing](https://arxiv.org/pdf/2507.15399v1)<br>:star:[code](https://tau-vailab.github.io/BlendedPC/)
* [UPP: Unified Point-Level Prompting for Robust Point Cloud Analysis](https://arxiv.org/pdf/2507.18997v1)<br>:star:[code](https://github.com/zhoujiahuan1991/ICCV2025-UPP)
* [Efficient Spiking Point Mamba for Point Cloud Analysis](http://arxiv.org/abs/2504.14371)<br>:star:[code](https://github.com/PeppaWu/SPM)
* [Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints](https://arxiv.org/pdf/2508.00558v1)
* [Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs](https://arxiv.org/pdf/2508.00169v1)<br>:star:[code](https://bhavyagoyal.github.io/ppc)<br>:star:[code](https://github.com/bhavyagoyal/ppc)
* [UST-SSM Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_UST-SSM_Unified_Spatio-Temporal_State_Space_Models_for_Point_Cloud_Video_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/wangzy01/UST-SSM)
* [Omni-scene Perception-oriented Point Cloud Geometry Enhancement for Coordinate Quantization](https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Omni-scene_Perception-oriented_Point_Cloud_Geometry_Enhancement_for_Coordinate_Quantization_ICCV_2025_paper.pdf)
* [A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds](http://arxiv.org/abs/2504.09129)
* [GenFlow3D Generative Scene Flow Estimation and Prediction on Point Cloud Sequences](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_GenFlow3D_Generative_Scene_Flow_Estimation_and_Prediction_on_Point_Cloud_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ustc-hlli/GenFlow3D)
* [Feature Extraction and Representation of Pre-training Point Cloud Based on Diffusion Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Qiu_Feature_Extraction_and_Representation_of_Pre-training_Point_Cloud_Based_on_ICCV_2025_paper.pdf)
* [Leaps and Bounds An Improved Point Cloud Winding Number Formulation for Fast Normal Estimation and Surface Reconstruction](https://openaccess.thecvf.com/content/ICCV2025/papers/Koneputugodage_Leaps_and_Bounds_An_Improved_Point_Cloud_Winding_Number_Formulation_ICCV_2025_paper.pdf)
* [Serialization based Point Cloud Oversegmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_Serialization_based_Point_Cloud_Oversegmentation_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/CHL-glitch/SPCNet)
* [Towards More Diverse and Challenging Pre-training for Point Cloud Learning Self-Supervised Cross Reconstruction with Decoupled Views](http://arxiv.org/abs/2509.01250)<br>:star:[code](https://github.com/aHapBean/Point-PQAE)
* [Liberated-GS 3D Gaussian Splatting Independent from SfM Point Clouds](https://openaccess.thecvf.com/content/ICCV2025/papers/Pan_Liberated-GS_3D_Gaussian_Splatting_Independent_from_SfM_Point_Clouds_ICCV_2025_paper.pdf)
* [CAD-Recode Reverse Engineering CAD Code from Point Clouds](https://openaccess.thecvf.com/content/ICCV2025/papers/Rukhovich_CAD-Recode_Reverse_Engineering_CAD_Code_from_Point_Clouds_ICCV_2025_paper.pdf)
* [Constraint-Aware Feature Learning for Parametric Point Cloud](http://arxiv.org/abs/2411.07747)
* [Egocentric Action-aware Inertial Localization in Point Clouds with Vision-Language Guidance](http://arxiv.org/abs/2505.14346)
* [DiffPCI Large Motion Point Cloud frame Interpolation with Diffusion Model](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_DiffPCI_Large_Motion_Point_Cloud_frame_Interpolation_with_Diffusion_Model_ICCV_2025_paper.pdf)
* [Mixed Signals A Diverse Point Cloud Dataset for Heterogeneous LiDAR V2X Collaboration](http://arxiv.org/abs/2502.14156)
* [DAP-MAE Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning](https://openaccess.thecvf.com/content/ICCV2025/papers/Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/CVI-SZU/DAP-MAE)
* [Interpretable point cloud classification using multiple instance learning](https://openaccess.thecvf.com/content/ICCV2025/papers/De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning_ICCV_2025_paper.pdf)
* [CounterPC Counterfactual Feature Realignment for Unsupervised Domain Adaptation on Point Clouds](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point_ICCV_2025_paper.pdf)
* [Partially Matching Submap Helps Uncertainty Modeling and Propagation for Text to Point Cloud Localization](https://openaccess.thecvf.com/content/ICCV2025/papers/Feng_Partially_Matching_Submap_Helps_Uncertainty_Modeling_and_Propagation_for_Text_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Afoolbird/PMSH)
* [DiffRefine Diffusion-based Proposal Specific Point Cloud Densification for Cross-Domain Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object_ICCV_2025_paper.pdf)
* [Point Cloud Self-supervised Learning via 3D to Multi-view Masked Learner](http://arxiv.org/abs/2311.10887)
* [RARE Refine Any Registration of Pairwise Point Clouds via Zero-Shot Learning](http://arxiv.org/abs/2507.19950)<br>:star:[code](https://github.com/zhengcy-lambo/RARE.git)
* 3D 点云
  * [Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation](http://arxiv.org/pdf/2506.22375v1)
  * [FastPoint: Accelerating 3D Point Cloud Model Inference via Sample Point Distance Prediction](https://arxiv.org/pdf/2507.23480v1)
  * [ForestFormer3D A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds](http://arxiv.org/abs/2506.16991)<br>:house:[project](https://bxiang233.github.io/FF3D)
  * [Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds](http://arxiv.org/abs/2508.11265)<br>:star:[code](https://github.com/ChicalH/DCGL)
  * [GroundFlow A Plug-in Module for Temporal Reasoning on 3D Point Cloud Sequential Grounding](http://arxiv.org/abs/2506.21188)
  * [Tree Skeletonization from 3D Point Clouds by Denoising Diffusion](https://openaccess.thecvf.com/content/ICCV2025/papers/Marks_Tree_Skeletonization_from_3D_Point_Clouds_by_Denoising_Diffusion_ICCV_2025_paper.pdf)
  * [TrackAny3D Transferring Pretrained 3D Models for Category-unified 3D Point Cloud Tracking](http://arxiv.org/abs/2507.19908)
* 点云配准
  * [TurboReg: TurboClique for Robust and Efficient Point Cloud Registration](https://arxiv.org/pdf/2507.01439v1)<br>:star:[code](https://github.com/Laka-3DV/TurboReg)
  * [Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior](https://arxiv.org/pdf/2507.06651v1)
  * [Unsupervised RGB-D Point Cloud Registration for Scenes with Low Overlap and Photometric Inconsistency](https://openaccess.thecvf.com/content/ICCV2025/papers/Shou_Unsupervised_RGB-D_Point_Cloud_Registration_for_Scenes_with_Low_Overlap_ICCV_2025_paper.pdf)
  * [BUFFER-X Towards Zero-Shot Point Cloud Registration in Diverse Scenes](https://openaccess.thecvf.com/content/ICCV2025/papers/Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/MIT-SPARK/BUFFER-X)
* 点云分割
  * [All in One: Visual-Description-Guided Unified Point Cloud Segmentation](https://arxiv.org/pdf/2507.05211v1)<br>:star:[code](https://github.com/Hanzy1996/VDG-Uni3DSeg)
  * [Generalized Few-Shot Point Cloud Segmentation via LLM-Assisted Hyper-Relation Matching](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Generalized_Few-Shot_Point_Cloud_Segmentation_via_LLM-Assisted_Hyper-Relation_Matching_ICCV_2025_paper.pdf)
  * [Mitigating Geometric Degradation in Fast DownSampling via FastAdapter for Point Cloud Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Mitigating_Geometric_Degradation_in_Fast_DownSampling_via_FastAdapter_for_Point_ICCV_2025_paper.pdf)
* 点云补全
  * [Revisiting Point Cloud Completion Are We Ready For The Real-World](http://arxiv.org/abs/2411.17580)
  * [Geometric Alignment and Prior Modulation for View-Guided Point Cloud Completion on Unseen Categories](https://openaccess.thecvf.com/content/ICCV2025/papers/Xiu_Geometric_Alignment_and_Prior_Modulation_for_View-Guided_Point_Cloud_Completion_ICCV_2025_paper.pdf)
* 点云分类
  * [Purge-Gate Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token purging](https://openaccess.thecvf.com/content/ICCV2025/papers/Yazdanpanah_Purge-Gate_Backpropagation-Free_Test-Time_Adaptation_for_Point_Clouds_Classification_via_Token_ICCV_2025_paper.pdf)
* 点云去噪
  * [Noise2Score3D Tweedies Approach for Unsupervised Point Cloud Denoising](https://openaccess.thecvf.com/content/ICCV2025/papers/Wei_Noise2Score3D_Tweedies_Approach_for_Unsupervised_Point_Cloud_Denoising_ICCV_2025_paper.pdf)

## 3D
* [VertexRegen: Mesh Generation with Continuous Level of Detail](https://arxiv.org/pdf/2508.09062v1)<br>:star:[code](https://vertexregen.github.io/)
* [GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields](https://arxiv.org/pdf/2506.23352v1)<br>:star:[code](https://snskysk.github.io/GeoProg3D/)
* [ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition](https://arxiv.org/pdf/2507.11261v1)
* [Top2Pano: Learning to Generate Indoor Panoramas from Top-Down View](https://arxiv.org/pdf/2507.21371v1)<br>:star:[code](https://top2pano.github.io/)
* [PanoSplatt3R: Leveraging Perspective Pretraining for Generalized Unposed Wide-Baseline Panorama Reconstruction](https://arxiv.org/pdf/2507.21960v1)<br>:star:[code](https://npucvr.github.io/PanoSplatt3R)
* [Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion](https://arxiv.org/pdf/2507.23483v1)<br>:star:[code](https://mutianxu.github.io/stable-sim2real/)<br>:star:[code](https://github.com/gap-lab-cuhk-sz/stable-sim2real)
* [OmniDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment](https://arxiv.org/pdf/2508.04611v1)<br>:star:[code](https://github.com/aeolusguan/OmniDepth)
* [LightSwitch: Multi-view Relighting with Material-guided Diffusion](https://arxiv.org/pdf/2508.06494v1)<br>:star:[code](https://yehonathanlitman.github.io/light_switch/)<br>:star:[code](https://github.com/yehonathanlitman/lightswitch)
* [How Far are AI-generated Videos from Simulating the 3D Visual World: A Learned 3D Evaluation Approach](http://arxiv.org/abs/2406.19568)
* 表面重建
  * [SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies](https://arxiv.org/pdf/2508.00366v1)<br>:star:[code](https://hanl2010.github.io/SparseRecon/)
  * [RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians](https://arxiv.org/pdf/2508.09830v1)<br>:star:[code](https://github.com/vLAR-group/RayletDF)
* 三维重建
  * [Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction](http://arxiv.org/pdf/2506.21401v1)<br>:star:[code](https://github.com/zhirui-gao/Curve-Gaussian)
  * [InstaScene: Towards Complete 3D Instance Decomposition and Reconstruction from Cluttered Scenes](https://arxiv.org/pdf/2507.08416v1)<br>:star:[code](https://zju3dv.github.io/instascene/)
  * [Image-Guided Shape-from-Template Using Mesh Inextensibility Constraints](https://arxiv.org/pdf/2507.22699v1)<br>:star:[code](https://github.com/dvttran/nsft)
  * [MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction](https://arxiv.org/pdf/2507.15212v1)
  * [LONG3R: Long Sequence Streaming 3D Reconstruction](https://arxiv.org/pdf/2507.18255v1)<br>:star:[code](https://zgchen33.github.io/LONG3R/)
  * [H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction](https://arxiv.org/pdf/2508.03118v1)<br>:star:[code](https://github.com/JiaHeng-DLUT/H3R)
  * [Ross3D Reconstructive Visual Instruction Tuning with 3D-Awareness](http://arxiv.org/abs/2504.01901)
* 场景重建
  * [BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting](http://arxiv.org/pdf/2506.22099v1)<br>:star:[code](https://github.com/fudan-zvg/BezierGS)
  * [ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting](https://arxiv.org/pdf/2507.15454v1)<br>:star:[code](https://ruijiezhu94.github.io/ObjectGS_page)
  * [DepR: Depth Guided Single-view Scene Reconstruction with Instance-level Diffusion](https://arxiv.org/pdf/2507.22825v1)
  * [ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors](https://arxiv.org/pdf/2508.06014v1)<br>:star:[code](https://exploregs.github.io)<br>:star:[code](https://github.com/minsu1206/exploregs)
* 三维场景理解
  * [VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding](https://arxiv.org/pdf/2506.22799v1)<br>:star:[code](https://sy-ja.github.io/votesplat/)
  * [Open-Vocabulary Octree-Graph for 3D Scene Understanding](http://arxiv.org/abs/2411.16253)<br>:star:[code](https://github.com/yifeisu/OV-Octree-Graph)
  * [HERMES A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation](http://arxiv.org/abs/2501.14729)<br>:star:[code](https://github.com/LMD0311/HERMES)
* 深度估计
  * [DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation](https://arxiv.org/pdf/2507.01603v1)
  * [Spherical Epipolar Rectification for Deep Two-View Absolute Depth Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Brousseau_Spherical_Epipolar_Rectification_for_Deep_Two-View_Absolute_Depth_Estimation_ICCV_2025_paper.pdf)
* 深度补全
  * [PacGDC: Label-Efficient Generalizable Depth Completion with Projection Ambiguity and Consistency](https://arxiv.org/pdf/2507.07374v1)<br>:star:[code](https://github.com/Wang-xjtu/PacGDC)
* SM
  * [RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather](https://arxiv.org/pdf/2507.01653v1)
  * [Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts](https://arxiv.org/pdf/2507.04631v1)<br>:star:[code](https://github.com/cocowy1/SMoE-Stereo)
* MVS
  * [MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network](https://arxiv.org/pdf/2507.11333v1)<br>:star:[code](https://github.com/JianfeiJ/MonoMVSNet)
* 3DGS
  * [RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration](https://arxiv.org/pdf/2507.08136v1)<br>:star:[code](https://3dagentworld.github.io/reggs/)
  * [PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations](https://arxiv.org/pdf/2507.13891v1)
  * [GaussianUpdate: Continual 3D Gaussian Splatting Update for Changing Environments](https://arxiv.org/pdf/2508.08867v1)
  * [ResGS Residual Densification of 3D Gaussian for Efficient Detail Recovery](http://arxiv.org/abs/2412.07494)
* Semantic Scene Completion(语义场景补全)
  * [Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion](https://arxiv.org/pdf/2507.06230v1)<br>:star:[code](https://visinf.github.io/scenedino)<br>:star:[code](https://github.com/tum-vision/scenedino)
  * [Disentangling Instance and Scene Contexts for 3D Semantic Scene Completion](https://arxiv.org/pdf/2507.08555v1)<br>:star:[code](https://github.com/Enyu-Liu/DISC)
  * [Monocular Semantic Scene Completion via Masked Recurrent Networks](https://arxiv.org/pdf/2507.17661v1)<br>:star:[code](https://github.com/alanWXZ/MonoMRN)
* 4D重建
  * [MonoFusion: Sparse-View 4D Reconstruction via Monocular Fusion](https://arxiv.org/pdf/2507.23782v1)<br>:star:[code](https://imnotprepared.github.io/research/25_DSR/)<br>:star:[code](https://github.com/ImNotPrepared/MonoFusion)
* 场景生成
  * [ScenePainter Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment](http://arxiv.org/abs/2507.19058)
  * [Free4D Tuning-free 4D Scene Generation with Spatial-Temporal Consistency](http://arxiv.org/abs/2503.20785)


## UAV/RS/Satellite Image(无人机/遥感/卫星图像)
* [UAVScenes: A Multi-Modal Dataset for UAVs](https://arxiv.org/pdf/2507.22412v1)<br>:star:[code](https://github.com/sijieaaa/UAVScenes)
* [Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method](http://arxiv.org/pdf/2506.22027v1)<br>:star:[code](https://github.com/Alioth2000/Hoss-ReID)
* [LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment](https://arxiv.org/pdf/2507.00659v1)
* [SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing](https://arxiv.org/pdf/2507.13812v1)
* [Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with Weak Supervision](https://arxiv.org/pdf/2507.20976v1)<br>:star:[code](https://humansensinglab.github.io/AGenDA)
* [OpenRSD Towards Open-prompts for Object Detection in Remote Sensing Images](http://arxiv.org/abs/2503.06146)
* [Dual Domain Control via Active Learning for Remote Sensing Domain Incremental Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Dual_Domain_Control_via_Active_Learning_for_Remote_Sensing_Domain_ICCV_2025_paper.pdf)
* [Active Learning Meets Foundation Models Fast Remote Sensing Data Annotation for Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Burges_Active_Learning_Meets_Foundation_Models_Fast_Remote_Sensing_Data_Annotation_ICCV_2025_paper.pdf)
* [LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing Object Detection](http://arxiv.org/abs/2509.16970)
* [Fusion Meets Diverse Conditions A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Fusion_Meets_Diverse_Conditions_A_High-diversity_Benchmark_and_Baseline_for_ICCV_2025_paper.pdf)
* 变化检测
  * [Information-Bottleneck Driven Binary Neural Network for Change Detection](https://arxiv.org/pdf/2507.03504v1)
* 目标检测
  * [Measuring the Impact of Rotation Equivariance on Aerial Object Detection](https://arxiv.org/pdf/2507.09896v1)
* 分割
  * [SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation](https://arxiv.org/pdf/2507.12857v1)<br>:star:[code](https://github.com/HuangShiqi128/SCORE)

## OCR
* [Beyond Isolated Words: Diffusion Brush for Handwritten Text-Line Generation](https://arxiv.org/pdf/2508.03256v1)<br>:star:[code](https://github.com/dailenson/DiffBrush)
* [CTC Transcription Alignment of the Bullinger Letters: Automatic Improvement of Annotation Quality](https://arxiv.org/pdf/2508.07904v1)<br>:star:[code](https://github.com/andreas-fischer-unifr/nntp)
* 文本生成
  * [UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis](https://arxiv.org/pdf/2507.00992v1)
* 甲骨文解读
  * [OracleFusion: Assisting the Decipherment of Oracle Bone Script with Structurally Constrained Semantic Typography](http://arxiv.org/pdf/2506.21101v1)
* 文档矫正
  * [ForCenNet: Foreground-Centric Network for Document Image Rectification](https://arxiv.org/pdf/2507.19804v1)<br>:star:[code](https://github.com/caipeng328/ForCenNet)
* 表格理解
  * [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/pdf/2508.06492v1)<br>:star:[code](https://github.com/yuweiyang-anu/ECD)

## Video
* [SciVid: Cross-Domain Evaluation of Video Models in Scientific Applications](https://arxiv.org/pdf/2507.03578v1)<br>:star:[code](https://github.com/google-deepmind/scivid)
* [Fine-grained Spatiotemporal Grounding on Egocentric Videos](https://arxiv.org/pdf/2508.00518v1)<br>:star:[code](https://github.com/LaVi-Lab/EgoMask)
* 视频理解
  * [Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs](http://arxiv.org/pdf/2506.22139v1)
  * [Flash-VStream: Efficient Real-Time Understanding for Long Video Streams](https://arxiv.org/pdf/2506.23825v1)<br>:star:[code](https://github.com/IVGSZ/Flash-VStream)
  * [AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding](https://arxiv.org/pdf/2507.02591v1)
  * [MCAM: Multimodal Causal Analysis Model for Ego-Vehicle-Level Driving Video Understanding](https://arxiv.org/pdf/2507.06072v1)<br>:star:[code](https://github.com/SixCorePeach/MCAM)
  * [DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding](https://arxiv.org/pdf/2507.15569v1)
  * [VideoLLaMB Long Streaming Video Understanding with Recurrent Memory Bridges](http://arxiv.org/abs/2409.01071)
* 视频摘要
  * [SummDiff Generative Modeling of Video Summarization with Diffusion](http://arxiv.org/abs/2510.08458)
* 视频时序定位
  * [Hierarchical Event Memory for Accurate and Low-latency Online Video Temporal Grounding](https://arxiv.org/pdf/2508.04546v1)<br>:star:[code](https://github.com/minghangz/OnVTG)

## Person Re-Identification(行人重识别)
* 基于视频的重识别
  * [HAMoBE: Hierarchical and Adaptive Mixture of Biometric Experts for Video-based Person ReID](https://arxiv.org/pdf/2508.05038v1)
* 换衣重识别
  * [Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement](https://arxiv.org/pdf/2507.07230v1)<br>:star:[code](https://github.com/ppriyank/ICCV-CSCI-Person-ReID)
* 终身重识别
  * [Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification](https://arxiv.org/pdf/2507.01884v1)<br>:star:[code](https://github.com/zhoujiahuan1991/ICCV2025-SPRED)
* 红外可见光
  * [Weakly Supervised Visible-Infrared Person Re-Identification via Heterogeneous Expert Collaborative Consistency Learning](https://arxiv.org/pdf/2507.12942v1)
* 行为理解
  * [ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video](https://arxiv.org/pdf/2508.09818v1)
  * [HiERO Understanding the Hierarchy of Human Behavior Enhances Reasoning on Egocentric Videos](http://arxiv.org/abs/2505.12911)

## Action Recognition(动作识别)
* [Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition](http://arxiv.org/pdf/2506.22179v1)
* [DeSPITE Exploring Contrastive Deep Skeleton-Pointcloud-IMU-Text Embeddings for Advanced Point Cloud Human Activity Understanding](https://openaccess.thecvf.com/content/ICCV2025/papers/Kreutz_DeSPITE_Exploring_Contrastive_Deep_Skeleton-Pointcloud-IMU-Text_Embeddings_for_Advanced_Point_Cloud_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/thkreutz/despite)
* [Punching Bag vs. Punching Person: Motion Transferability in Videos](https://arxiv.org/pdf/2508.00085v1)<br>:star:[code](https://github.com/raiyaan-abdullah/Motion-Transfer)
* 动作预测
  * [Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions](https://arxiv.org/pdf/2507.09446v1)<br>:star:[code](https://github.com/Yuanhong-Zheng/EMPMP)
* 小样本动作识别
  * [Beyond Label Semantics: Language-Guided Action Anatomy for Few-shot Action Recognition](https://arxiv.org/pdf/2507.16287v1)
  * [Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition](https://arxiv.org/pdf/2508.03695v1)<br>:star:[code](https://trokens-iccv25.github.io)<br>:star:[code](https://github.com/pulkitkumar95/trokens)
* 时序动作分割
  * [Skeleton Motion Words for Unsupervised Skeleton-Based Temporal Action Segmentation](https://arxiv.org/pdf/2508.04513v1)<br>:star:[code](https://github.com/bachlab/SMQ)
* 动作检测
  * [Scaling Action Detection AdaTAD with Transformer-Enhanced Temporal-Spatial Adaptation](https://openaccess.thecvf.com/content/ICCV2025/papers/Agrawal_Scaling_Action_Detection_AdaTAD_with_Transformer-Enhanced_Temporal-Spatial_Adaptation_ICCV_2025_paper.pdf)
  * [MMAD Multi-label Micro-Action Detection in Videos](http://arxiv.org/abs/2407.05311)<br>:star:[code](https://github.com/VUT-HFUT/Micro-Action)


## Human Motion
* 人体运动分割
  * [Temporal Rate Reduction Clustering for Human Motion Segmentation](http://arxiv.org/pdf/2506.21249v1)
* 运动生成
  * [PINO: Person-Interaction Noise Optimization for Long-Duration and Customizable Motion Generation of Arbitrary-Sized Groups](https://arxiv.org/pdf/2507.19292v1)<br>:star:[code](https://sinc865.github.io/pino/)
  * [RapVerse Coherent Vocals and Whole-Body Motion Generation from Text](http://arxiv.org/abs/2405.20336)
  * [PUMPS: Skeleton-Agnostic Point-based Universal Motion Pre-Training for Synthesis in Human Motion Tasks](https://arxiv.org/pdf/2507.20170v1)
* 运动重建
  * [UniEgoMotion A Unified Model for Egocentric Motion Reconstruction Forecasting and Generation](http://arxiv.org/abs/2508.01126)

## pose
* 人体重建
  * [PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single Image](https://arxiv.org/pdf/2507.17332v1)<br>:star:[code](https://hygenie1228.github.io/PARTE/)
* 手势合成
  * [SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning](https://arxiv.org/pdf/2507.19359v1)<br>:star:[code](https://semgesture.github.io/)
* 3D HPE
  * [DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior](https://arxiv.org/pdf/2508.00599v1)<br>:star:[code](https://github.com/moonbow721/DPoser)
* 人体姿态生成
  * [Head2Body Body Pose Generation from Multi-sensory Head-mounted Inputs](https://openaccess.thecvf.com/content/ICCV2025/papers/Tran_Head2Body_Body_Pose_Generation_from_Multi-sensory_Head-mounted_Inputs_ICCV_2025_paper.pdf)
* 手部姿态
  * [Prior-aware Dynamic Temporal Modeling Framework for Sequential 3D Hand Pose Estimation](https://openaccess.thecvf.com/content/ICCV2025/papers/Ren_Prior-aware_Dynamic_Temporal_Modeling_Framework_for_Sequential_3D_Hand_Pose_ICCV_2025_paper.pdf)

## Object Track
* [SpatialTrackerV2: 3D Point Tracking Made Easy](https://arxiv.org/pdf/2507.12462v1)<br>:house:[project](https://huggingface.co/spaces/Yuxihenry/SpatialTrackerV2)<br>:star:[code](https://github.com/henry123-boy/SpaTrackerV2)
* [GSOT3D Towards Generic 3D Single Object Tracking in the Wild](http://arxiv.org/abs/2412.02129)<br>:star:[code](https://github.com/ailovejinx/GSOT3D)
* [UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions](https://arxiv.org/pdf/2507.00648v1)<br>:star:[code](https://github.com/Z-Z188/UMDATrack)
* [Is Tracking really more challenging in First Person Egocentric Vision?](https://arxiv.org/pdf/2507.16015v1)
* [What You Have is What You Track: Adaptive and Robust Multimodal Tracking](https://arxiv.org/pdf/2507.05899v1)<br>:star:[code](https://github.com/supertyd/FlexTrack/tree/main)
* [Temporal Unlearnable Examples: Preventing Personal Video Data from Unauthorized Exploitation by Object Tracking](https://arxiv.org/pdf/2507.07483v1)
* [General Compression Framework for Efficient Transformer Object Tracking](http://arxiv.org/abs/2409.17564)<br>:star:[code](https://github.com/LingyiHongfd/CompressTracker)
* 多目标跟踪
  * [Language Decoupling with Fine-grained Knowledge Guidance for Referring Multi-object Tracking](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Language_Decoupling_with_Fine-grained_Knowledge_Guidance_for_Referring_Multi-object_Tracking_ICCV_2025_paper.pdf)
  * [VOVTrack Exploring the Potentiality in Raw Videos for Open-Vocabulary Multi-Object Tracking](https://openaccess.thecvf.com/content/ICCV2025/papers/Qian_VOVTrack_Exploring_the_Potentiality_in_Raw_Videos_for_Open-Vocabulary_Multi-Object_ICCV_2025_paper.pdf)
  * [LA-MOTR End-to-End Multi-Object Tracking by Learnable Association](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_LA-MOTR_End-to-End_Multi-Object_Tracking_by_Learnable_Association_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/PenK1nG/LA-MOTR)






## Object Detection(目标检测) 
* [DuET: Dual Incremental Object Detection via Exemplar-Free Task Arithmetic](http://arxiv.org/pdf/2506.21260v1)
* [Gradient Decomposition and Alignment for Incremental Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Luo_Gradient_Decomposition_and_Alignment_for_Incremental_Object_Detection_ICCV_2025_paper.pdf)
* [Visual Textualization for Image Prompted Object Detection](https://arxiv.org/pdf/2506.23785v1)<br>:star:[code](https://github.com/WitGotFlg/VisTex-OVLM)
* [Task-Specific Zero-shot Quantization-Aware Training for Object Detection](https://arxiv.org/pdf/2507.16782v1)<br>:star:[code](https://github.com/DFQ-Dojo/dfq-toolkit)
* [PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection](https://arxiv.org/pdf/2506.23581v1)
* [UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement](https://arxiv.org/pdf/2507.00721v1)<br>:star:[code](https://github.com/AMAP-ML/UPRE)
* [SFUOD: Source-Free Unknown Object Detection](https://arxiv.org/pdf/2507.17373v1)
* [LMM-Det: Make Large Multimodal Models Excel in Object Detection](https://arxiv.org/pdf/2507.18300v1)<br>:star:[code](https://github.com/360CVGroup/LMM-Det)
* [Adversarial Attention Perturbations for Large Object Detection Transformers](https://arxiv.org/pdf/2508.02987v1)<br>:star:[code](https://github.com/zacharyyahn/AFOG)
* [ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting](https://arxiv.org/pdf/2508.07089v1)
* [Gradient-Reweighted Adversarial Camouflage for Physical Object Detection Evasion](https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_Gradient-Reweighted_Adversarial_Camouflage_for_Physical_Object_Detection_Evasion_ICCV_2025_paper.pdf)
* [Unified Category-Level Object Detection and Pose Estimation from RGB Images using 3D Prototypes](http://arxiv.org/abs/2508.02157)
* [Cycle-Consistent Learning for Joint Layout-to-Image Generation and Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Cai_Cycle-Consistent_Learning_for_Joint_Layout-to-Image_Generation_and_Object_Detection_ICCV_2025_paper.pdf)
* [Rethinking Multi-modal Object Detection from the Perspective of Mono-Modality Feature Learning](http://arxiv.org/abs/2503.11780)<br>:star:[code](https://github.com/Zhao-Tian-yi/M2D-LIF)
* [Automated Model Evaluation for Object Detection via Prediction Consistency and Reliability](http://arxiv.org/abs/2508.12082)<br>:star:[code](https://github.com/YonseiML/autoeval-det)
* [Diffusion-based Source-biased Model for Single Domain Generalized Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_Diffusion-based_Source-biased_Model_for_Single_Domain_Generalized_Object_Detection_ICCV_2025_paper.pdf)
* [VISO Accelerating In-orbit Object Detection with Language-Guided Mask Learning and Sparse Inference](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_VISO_Accelerating_In-orbit_Object_Detection_with_Language-Guided_Mask_Learning_and_ICCV_2025_paper.pdf)
* [Beyond RGB Adaptive Parallel Processing for RAW Object Detection](http://arxiv.org/abs/2503.13163)
* [Dark-ISP Enhancing RAW Image Processing for Low-Light Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Guo_Dark-ISP_Enhancing_RAW_Image_Processing_for_Low-Light_Object_Detection_ICCV_2025_paper.pdf)
* [DoppDrive Doppler-Driven Temporal Aggregation for Improved Radar Object Detection](http://arxiv.org/abs/2508.12330)<br>:house:[project](https://yuvalhg.github.io/DoppDrive) :house:[project](https://yuvalhg.github.io/DoppDrive/)
* [Continual Adaptation Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios](http://arxiv.org/abs/2506.24063)
* 小目标检测
  * [Event-based Tiny Object Detection A Benchmark Dataset and Baseline](http://arxiv.org/abs/2506.23575)
  * [Uncertainty-Aware Gradient Stabilization for Small Object Detection](http://arxiv.org/abs/2303.01803)
  * [DM-EFS Dynamically Multiplexed Expanded Features Set Form for Robust and Efficient Small Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Sharma_DM-EFS_Dynamically_Multiplexed_Expanded_Features_Set_Form_for_Robust_and_ICCV_2025_paper.pdf)
* 开集目标检测
  * [3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection](https://arxiv.org/pdf/2507.23567v1)
  * [ASGS Single-Domain Generalizable Open-Set Object Detection via Adaptive Subgraph Searching](https://openaccess.thecvf.com/content/ICCV2025/papers/Yuan_ASGS_Single-Domain_Generalizable_Open-Set_Object_Detection_via_Adaptive_Subgraph_Searching_ICCV_2025_paper.pdf)
* 三维目标检测
  * [OcRFDet: Object-Centric Radiance Fields for Multi-View 3D Object Detection in Autonomous Driving](https://arxiv.org/pdf/2506.23565v1)<br>:star:[code](https://github.com/Mingqj/OcRFDet)
  * [MambaFusion: Height-Fidelity Dense Global Fusion for Multi-modal 3D Object Detection](https://arxiv.org/pdf/2507.04369v1)
  * [Perspective-Invariant 3D Object Detection](https://arxiv.org/pdf/2507.17665v1)<br>:star:[code](https://pi3det.github.io)
  * [Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume Construction](https://arxiv.org/pdf/2507.18331v1)<br>:star:[code](https://github.com/RM-Zhang/SGCDet)
* 伪装目标检测
  * [Rethinking Detecting Salient and Camouflaged Objects in Unconstrained Scenes](http://arxiv.org/abs/2412.10943)<br>:star:[code](https://github.com/ssecv/USCNet)
  * [ESCNetEdge-Semantic Collaborative Network for Camouflaged Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Ye_ESCNetEdge-Semantic_Collaborative_Network_for_Camouflaged_Object_Detection_ICCV_2025_paper.pdf)
* 半监督目标检测
  * [STEP-DETR Advancing DETR-based Semi-Supervised Object Detection with Super Teacher and Pseudo-Label Guided Text Queries](https://openaccess.thecvf.com/content/ICCV2025/papers/Shehzadi_STEP-DETR_Advancing_DETR-based_Semi-Supervised_Object_Detection_with_Super_Teacher_and_ICCV_2025_paper.pdf)
  * [Power of Cooperative Supervision Multiple Teachers Framework for Advanced 3D Semi-Supervised Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_Power_of_Cooperative_Supervision_Multiple_Teachers_Framework_for_Advanced_3D_ICCV_2025_paper.pdf)
* 域适应目标检测
  * [Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability](http://arxiv.org/pdf/2506.21042v1)<br>:star:[code](https://github.com/heboyong/Fitness-Generalization-Transferability)
  * [Dual-Rate Dynamic Teacher for Source-Free Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/He_Dual-Rate_Dynamic_Teacher_for_Source-Free_Domain_Adaptive_Object_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/qih96/DDT)
  * [Debiased Teacher for Day-to-Night Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Cui_Debiased_Teacher_for_Day-to-Night_Domain_Adaptive_Object_Detection_ICCV_2025_paper.pdf)
* 开放词汇目标检测
  * [Dynamic-DINO Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_Dynamic-DINO_Fine-Grained_Mixture_of_Experts_Tuning_for_Real-time_Open-Vocabulary_Object_ICCV_2025_paper.pdf)
  * [Benefit From Seen Enhancing Open-Vocabulary Object Detection by Bridging Visual and Textual Co-Occurrence Knowledge](https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Benefit_From_Seen_Enhancing_Open-Vocabulary_Object_Detection_by_Bridging_Visual_ICCV_2025_paper.pdf)
* 可见光红外目标检测
  * [WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection](https://arxiv.org/pdf/2507.18173v1)
* 红外小目标检测
  * [From Easy to Hard Progressive Active Learning Framework for Infrared Small Target Detection with Single Point Supervision](http://arxiv.org/abs/2412.11154)<br>:star:[code](https://github.com/YuChuang1205/PAL)
  * [Text-IRSTD Leveraging Semantic Text to Promote Infrared Small Target Detection in Complex Scenes](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_Text-IRSTD_Leveraging_Semantic_Text_to_Promote_Infrared_Small_Target_Detection_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Zhengsy0407/Text-IRSTD)



## Biometrics
* [DisenQ: Disentangling Q-Former for Activity-Biometrics](https://arxiv.org/pdf/2507.07262v1)


## Avatar
* [GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar](https://arxiv.org/pdf/2507.18155v1)<br>:star:[code](https://hahminlew.github.io/geoavatar/)
* [HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars](https://arxiv.org/pdf/2507.19481v1)<br>:star:[code](https://bjkim95.github.io/haircup/)
* [MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar Reconstruction](https://arxiv.org/pdf/2507.23597v1)<br>:star:[code](https://zj-dong.github.io/MoGA/)<br>:star:[code](https://github.com/zj-dong/moga)
* [PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image](https://arxiv.org/pdf/2508.09973v1)<br>:star:[code](https://mks0601.github.io/PERSONA/)<br>:star:[code](https://github.com/mks0601/persona_release)

## Face
* [IDFace: Face Template Protection for Efficient and Secure Identification](https://arxiv.org/pdf/2507.12050v1)
* [FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models](https://arxiv.org/pdf/2507.02714v1)
* 人脸识别
  * [LVFace Progressive Cluster Optimization for Large Vision Models in Face Recognition](http://arxiv.org/abs/2501.13420)<br>:star:[code](https://github.com/bytedance/LVFace)
* 人脸恢复
  * [DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance](https://arxiv.org/pdf/2507.13797v1)
  * [MoFRR Mixture of Diffusion Models for Face Retouching Restoration](http://arxiv.org/abs/2507.19770)
* 人脸表情识别
  * [Multimodal Prompt Alignment for Facial Expression Recognition](http://arxiv.org/pdf/2506.21017v1)
  * [AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation](https://arxiv.org/pdf/2507.12001v1)<br>:star:[code](https://github.com/wslh852/AUBlendNet.git)
* 说话头
  * [GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation](http://arxiv.org/pdf/2506.21513v1)<br>:star:[code](https://vincenthu19.github.io/GGTalker/)
  * [Who is a Better Talker: Subjective and Objective Quality Assessment for AI-Generated Talking Heads](http://arxiv.org/abs/2507.23343)<br>:star:[code](https://github.com/zyj-2000/Talker.)
  * [ARIG: Autoregressive Interactive Head Generation for Real-time Conversations](https://arxiv.org/pdf/2507.00472v1)<br>:star:[code](https://jinyugy21.github.io/ARIG/)
* 人脸交换
  * [CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation](https://arxiv.org/abs/2507.02691)<br>:house:[project](https://luoxyhappy.github.io/CanonSwap/)
* 活体检测
  * [Group-wise Scaling and Orthogonal Decomposition for Domain-Invariant Feature Extraction in Face Anti-Spoofing](https://arxiv.org/pdf/2507.04006v1)<br>:star:[code](https://github.com/SeungjinJung/GD-FAS)
* 三维人脸动画
  * [MemoryTalker: Personalized Speech-Driven 3D Facial Animation via Audio-Guided Stylization](https://arxiv.org/pdf/2507.20562v1)<br>:star:[code](https://cau-irislab.github.io/ICCV25-MemoryTalker/)
* 微表情识别
  * [FED-PsyAU: Privacy-Preserving Micro-Expression Recognition via Psychological AU Coordination and Dynamic Facial Motion Modeling](https://arxiv.org/pdf/2507.20557v1)

## Medical Image Progress(医学图像处理)
* [Is Visual in-Context Learning for Compositional Medical Tasks within Reach?](https://arxiv.org/pdf/2507.00868v1)
* [FB-Diff: Fourier Basis-guided Diffusion for Temporal Interpolation of 4D Medical Imaging](https://arxiv.org/pdf/2507.04547v1)
* [Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines](https://arxiv.org/pdf/2507.10737v1)
* [Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation](https://arxiv.org/pdf/2507.11055v1)
* [M-Net: MRI Brain Tumor Sequential Segmentation Network via Mesh-Cast](https://arxiv.org/pdf/2507.20582v1)
* [MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy](https://arxiv.org/pdf/2508.03596v1)<br>:star:[code](https://cuhk-aim-group.github.io/MetaScope/)<br>:star:[code](https://github.com/cuhk-aim-group/metascope)
* [COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets](https://arxiv.org/pdf/2508.09886v1)
* [CoStoDet-DDPM Collaborative Training of Stochastic and Deterministic Models Improves Surgical Workflow Anticipation and Recognition](https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_CoStoDet-DDPM_Collaborative_Training_of_Stochastic_and_Deterministic_Models_Improves_Surgical_ICCV_2025_paper.pdf)
* [Optimal Transport for Brain-Image Alignment Unveiling Redundancy and Synergy in Neural Information Processing](http://arxiv.org/abs/2503.10663)<br>:star:[code](https://github.com/NKUShaw/OT-Alignment4brain-to-image)
* 医学图像融合
  * [UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments](https://arxiv.org/pdf/2506.22736v1)
* 报告生成
  * [MedRegion-CT: Region-Focused Multimodal LLM for Comprehensive 3D CT Report Generation](https://arxiv.org/pdf/2506.23102v1)
* 切片分析
  * [Continual Multiple Instance Learning with Enhanced Localization for Histopathological Whole Slide Image Analysis](https://arxiv.org/pdf/2507.02395v1)
  * [Cracking Instance Jigsaw Puzzles: An Alternative to Multiple Instance Learning for Whole Slide Image Analysis](https://arxiv.org/pdf/2507.08178v1)<br>:star:[code](https://github.com/xiwenc1/MIL-JigsawPuzzles)
* 3D医学
  * [Structure-aware Semantic Discrepancy and Consistency for 3D Medical Image Self-supervised Learning](https://arxiv.org/pdf/2507.02581v1)
* 息肉分割
  * [One Polyp Identifies All: One-Shot Polyp Segmentation with SAM via Cascaded Priors and Iterative Prompt Evolution](https://arxiv.org/pdf/2507.16337v1)
* 医学影像隐私保护
  * [Semantics versus Identity: A Divide-and-Conquer Approach towards Adjustable Medical Image De-Identification](https://arxiv.org/pdf/2507.21703v1)

## Image/Video Compression(图像/视频压缩)
* [Learned Image Compression with Hierarchical Progressive Context Modeling](https://arxiv.org/pdf/2507.19125v1)<br>:star:[code](https://github.com/lyq133/LIC-HPCM)


## Image/Video Retrieval(图像/视频检索)
* [Adversarial Reconstruction Feedback for Robust Fine-grained Generalization](https://arxiv.org/pdf/2507.21742v1)
* [MobileViCLIP: An Efficient Video-Text Model for Mobile Devices](https://arxiv.org/pdf/2508.07312v1)<br>:star:[code](https://github.com/MCG-NJU/MobileViCLIP)
* [Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object Retrieval](https://arxiv.org/pdf/2507.21489v1)<br>:star:[code](https://github.com/wangzhichuan123/DAC)
* 视频检索
  * [HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning](https://arxiv.org/pdf/2507.17402v1)<br>:star:[code](https://github.com/lijun2005/ICCV25-HLFormer)
* 文本-视频检索
  * [Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization](https://arxiv.org/pdf/2507.15504v1)
  * [Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval](https://arxiv.org/pdf/2507.23284v1)<br>:star:[code](https://github.com/mlvlab/BLiM)

## Image Classification(图像分类)
* [Generate, Refine, and Encode: Leveraging Synthesized Novel Samples for On-the-Fly Fine-Grained Category Discovery](https://arxiv.org/pdf/2507.04051v1)
* [Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification](https://arxiv.org/pdf/2506.23285v1)
* [CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous Nuisance Shifts](https://arxiv.org/pdf/2507.17651v1)<br>:star:[code](https://genintel.github.io/CNS)
* [I Am Big, You Are Little; I Am Right, You Are Wrong](https://arxiv.org/pdf/2507.23509v1)
* 细粒度分类
  * [Vocabulary-free Fine-grained Visual Recognition via Enriched Contextually Grounded Vision-Language Model](https://arxiv.org/pdf/2507.23070v1)<br>:star:[code](https://github.com/demidovd98/e-finer)

## Image Segmentation(图像分割)
* [SAM4D: Segment Anything in Camera and LiDAR Streams](http://arxiv.org/pdf/2506.21547v1)<br>:star:[code](https://SAM4D-Project.github.io)
* [Flow Stochastic Segmentation Networks](https://arxiv.org/pdf/2507.18838v1)<br>:star:[code](https://github.com/biomedia-mira/flow-ssn)
* [Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive](https://arxiv.org/pdf/2507.09612v1)
* [SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures](https://arxiv.org/pdf/2508.06127v1)
* 抠图
  * [SDMatte: Grafting Diffusion Models for Interactive Matting](https://arxiv.org/pdf/2508.00443v1)<br>:star:[code](https://github.com/vivoCameraResearch/SDMatte)
* 小样本分割
  * [Balancing Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot Segmentation](https://arxiv.org/pdf/2507.19140v1)<br>:star:[code](https://github.com/tianyu-zou/PAHNet)
* 开放词汇分割
  * [ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation](http://arxiv.org/pdf/2506.21233v1)<br>:star:[code](https://github.com/xiweix/ReME)
* 实例分割
  * [Details Matter for Indoor Open-vocabulary 3D Instance Segmentation](https://arxiv.org/pdf/2507.23134v1)
  * 零样本实例分割
    * [Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation](https://arxiv.org/pdf/2508.04122v1)
* 全景分割
  * [PanSt3R: Multi-view Consistent Panoptic Segmentation](http://arxiv.org/pdf/2506.21348v1)
  * [Prior2Former - Evidential Modeling of Mask Transformers for Assumption-Free Open-World Panoptic Segmentation](https://openaccess.thecvf.com/content/ICCV2025/papers/Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World_ICCV_2025_paper.pdf)
* 语义分割
  * [Exploring Probabilistic Modeling Beyond Domain Generalization for Semantic Segmentation](https://arxiv.org/pdf/2507.21367v1)
  * [Revisiting Efficient Semantic Segmentation: Learning Offsets for Better Spatial and Class Feature Alignment](https://arxiv.org/pdf/2508.08811v1)<br>:star:[code](https://github.com/HVision-NKU/OffSeg)
  * 半监督语义分割
    * [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/pdf/2507.15803v1)
  * 小样本语义分割
    * [Probabilistic Prototype Calibration of Vision-Language Models for Generalized Few-shot Semantic Segmentation](https://arxiv.org/pdf/2506.22979v1)<br>:star:[code](https://github.com/jliu4ai/FewCLIP)
  * 开放词汇语义分割
    * [Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](https://arxiv.org/pdf/2507.11030v1)
    * [Training-Free Class Purification for Open-Vocabulary Semantic Segmentation](https://arxiv.org/pdf/2508.00557v1)
* 指代图像分割
  * [DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy](https://arxiv.org/pdf/2507.01738v1)<br>:star:[code](https://github.com/Dmmm1997/DeRIS)
  * [Latent Expression Generation for Referring Image Segmentation and Grounding](https://arxiv.org/pdf/2508.05123v1)
* 动作分割
  * [CLOT: Closed Loop Optimal Transport for Unsupervised Action Segmentation](https://arxiv.org/pdf/2507.03539v1)
* VIS
  * [Latest Object Memory Management for Temporally Consistent Video Instance Segmentation](https://arxiv.org/pdf/2507.19754v1)<br>:star:[code](https://seung-hun-lee.github.io/projects/LOMM/)<br>:star:[code](https://github.com/Seung-Hun-Lee/LOMM)
  * [Hierarchical Visual Prompt Learning for Continual Video Instance Segmentation](https://arxiv.org/pdf/2508.08612v1)<br>:star:[code](https://github.com/JiahuaDong/HVPL)
* VOS
  * [MOVE: Motion-Guided Few-Shot Video Object Segmentation](https://arxiv.org/pdf/2507.22061v1)<br>:house:[project](https://henghuiding.com/MOVE/)

## Image Generation(图像生成)
* [Rethinking Layered Graphic Design Generation with a Top-Down Approach](https://arxiv.org/pdf/2507.05601v1)
* [Sat2City: 3D City Generation from A Single Satellite Image with Cascaded Latent Diffusion](https://arxiv.org/pdf/2507.04403v1)
* [QR-LoRA: Efficient and Disentangled Fine-tuning via QR Decomposition for Customized Generation](https://arxiv.org/pdf/2507.04599v1)
* [Text Embedding Knows How to Quantize Text-Guided Diffusion Models](https://arxiv.org/pdf/2507.10340v1)
* [Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models](https://arxiv.org/pdf/2507.14797v1)<br>:star:[code](https://github.com/BeierZhu/EPD)
* [Anti-Tamper Protection for Unauthorized Individual Image Generation](https://arxiv.org/pdf/2508.06325v1)<br>:star:[code](https://github.com/Seeyn/Anti-Tamper-Perturbation)
* [Stable Diffusion Models are Secretly Good at Visual In-Context Learning](https://arxiv.org/pdf/2508.09949v1)
* [FlexGen Flexible Multi-View Generation from Text and Image Inputs](http://arxiv.org/abs/2410.10745)
* 布局生成
  * [IGD: Instructional Graphic Design with Multimodal Layer Generation](https://arxiv.org/pdf/2507.09910v1)
* 图像合成
  * [Preserve Anything: Controllable Image Synthesis with Object Preservation](https://arxiv.org/pdf/2506.22531v1)
  * [PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions](https://arxiv.org/pdf/2506.23440v1)
  * [Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis](https://arxiv.org/pdf/2507.01756v1)
  * [AIComposer: Any Style and Content Image Composition via Feature Integration](https://arxiv.org/pdf/2507.20721v1)<br>:star:[code](https://github.com/sherlhw/AIComposer)
* 图像生成
  * [DC-AR: Efficient Masked Autoregressive Image Generation with Deep Compression Hybrid Tokenizer](https://arxiv.org/pdf/2507.04947v1)
  * [Scale Your Instructions: Enhance the Instruction-Following Fidelity of Unified Image Generation Model by Self-Adaptive Attention Scaling](https://arxiv.org/pdf/2507.16240v1)<br>:star:[code](https://github.com/zhouchao-ops/SaaS)
  * [HPSv3: Towards Wide-Spectrum Human Preference Score](https://arxiv.org/pdf/2508.03789v1)
  * [Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment](https://arxiv.org/pdf/2507.19002v1)<br>:star:[code](https://github.com/BarretBa/ICTHP)
  * [LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing](https://arxiv.org/pdf/2507.22627v1)<br>:star:[code](https://intelligolabs.github.io/lots/)<br>:star:[code](https://github.com/intelligolabs/lots)
  * [Trade-offs in Image Generation: How Do Different Dimensions Interact?](https://arxiv.org/pdf/2507.22100v1)<br>:star:[code](https://github.com/fesvhtr/TRIG)
  * [Grouped Speculative Decoding for Autoregressive Image Generation](https://arxiv.org/pdf/2508.07747v1)<br>:star:[code](https://github.com/junhyukso/GSD)
  * [LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering](https://arxiv.org/pdf/2508.07647v1)<br>:star:[code](https://xiaohangzhan.github.io/projects/larender/)
* 文本-图像
  * [Rethink Sparse Signals for Pose-guided Text-to-image Generation](http://arxiv.org/pdf/2506.20983v1)<br>:star:[code](https://github.com/DREAMXFAR/SP-Ctrl)
  * [CharaConsist: Fine-Grained Consistent Character Generation](https://arxiv.org/pdf/2507.11533v1)<br>:star:[code](https://murray-wang.github.io/CharaConsist/)<br>:star:[code](https://github.com/Murray-Wang/CharaConsist)
  * [Multimodal LLMs as Customized Reward Models for Text-to-Image Generation](https://arxiv.org/pdf/2507.21391v1)<br>:star:[code](https://github.com/sjz5202/LLaVA-Reward)
  * [TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance](https://arxiv.org/pdf/2507.18192v1)<br>:star:[code](https://github.com/AIDC-AI/TeEFusion)
  * [T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation](https://arxiv.org/pdf/2507.20536v1)<br>:star:[code](https://github.com/SHI-Labs/T2I-Copilot)
  * [YOLO-Count: Differentiable Object Counting for Text-to-Image Generation](https://arxiv.org/pdf/2508.00728v1)
  * [Steering Guidance for Personalized Text-to-Image Diffusion Models](https://arxiv.org/pdf/2508.00319v1)
  * [PLA: Prompt Learning Attack against Text-to-Image Generative Models](https://arxiv.org/pdf/2508.03696v1)
  * [Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models](https://arxiv.org/pdf/2508.03481v1)
  * [Automated Red Teaming for Text-to-Image Models through Feedback-Guided Prompt Iteration with Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_Automated_Red_Teaming_for_Text-to-Image_Models_through_Feedback-Guided_Prompt_Iteration_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/Weiww-Xu/FGPI)
* 文本-视频
  * [TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models](https://arxiv.org/pdf/2508.00289v1)<br>:star:[code](https://titanguide.github.io)
* 视频合成
  * [Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis](https://arxiv.org/pdf/2506.23263v1)
  * [Democratizing High-Fidelity Co-Speech Gesture Video Generation](https://arxiv.org/pdf/2507.06812v1)
  * [Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis](https://arxiv.org/pdf/2507.18569v1)
  * [V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models](https://arxiv.org/pdf/2508.03254v1)<br>:star:[code](https://jiiiisoo.github.io/VIP.github.io/)<br>:star:[code](https://github.com/jiiiisoo/VIP.github.io)
* 图像编辑
  * [ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation](https://arxiv.org/pdf/2507.01496v1)<br>:star:[code](https://wlaud1001.github.io/ReFlex/)
  * [ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided Image Editing Evaluation](https://arxiv.org/pdf/2507.07317v1)
  * [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](https://arxiv.org/pdf/2508.06033v1)
  * [Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing](https://arxiv.org/pdf/2508.07519v1)<br>:house:[project](https://joonghyuk.com/exploring-mmdit-web/)
* 图像渐变
  * [FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model](https://arxiv.org/pdf/2507.01953v1)<br>:star:[code](https://yukangcao.github.io/FreeMorph/)
* 视频生成
  * [AnyI2V: Animating Any Conditional Image with Motion Control](https://arxiv.org/pdf/2507.02857v1)<br>:house:[project](https://henghuiding.com/AnyI2V/)
* 3D布局
  * [LACONIC: A 3D Layout Adapter for Controllable Image Creation](https://arxiv.org/pdf/2507.03257v1)
* 文本-3D
  * [SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with Segmented Consistency Trajectory Distillation](https://arxiv.org/pdf/2507.05256v1)<br>:star:[code](https://zjhjojo.github.io/)
  * [Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation](https://arxiv.org/pdf/2507.09748v1)
* 视频-4D
  * [Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis](https://arxiv.org/pdf/2507.23785v1)<br>:star:[code](https://gvfdiffusion.github.io/)<br>:star:[code](https://github.com/foreverfancy/gvfdiffusion)
* 故事生成
  * [Lay2Story: Extending Diffusion Transformers for Layout-Togglable Story Generation](https://arxiv.org/pdf/2508.08949v1)
* 图像拼接
  * [PixelStitch Structure-Preserving Pixel-Wise Bidirectional Warps for Unsupervised Image Stitching](https://openaccess.thecvf.com/content/ICCV2025/papers/Jin_PixelStitch_Structure-Preserving_Pixel-Wise_Bidirectional_Warps_for_Unsupervised_Image_Stitching_ICCV_2025_paper.pdf)

## Image Captioning(图像字幕)
* [CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning](https://arxiv.org/pdf/2507.01409v1)<br>:star:[code](https://github.com/omron-sinicx/captionsmiths)
* [SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning](https://arxiv.org/pdf/2508.06125v1)
* 图表字幕
  * [ChartCap: Mitigating Hallucination of Dense Chart Captioning](https://arxiv.org/pdf/2508.03164v1)
* 视频字幕
  * [Player-Centric Multimodal Prompt Generation for Large Language Model Based Identity-Aware Basketball Video Captioning](https://arxiv.org/pdf/2507.20163v1)<br>:star:[code](https://github.com/Zeyu1226-mt/LLM-IAVC)

## Super-Resolution(超分辨率)
* 图像超分辨率
  * [LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning](https://arxiv.org/pdf/2506.22710v1)<br>:star:[code](https://github.com/MJ-NCEPU/LightBSR)
  * [Bridging Diffusion Models and 3D Representations: A 3D Consistent Super-Resolution Framework](https://arxiv.org/pdf/2508.04090v1)
  * [IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution](https://arxiv.org/pdf/2507.09923v1)
  * [Fine-structure Preserved Real-world Image Super-resolution via Transfer VAE Training](https://arxiv.org/pdf/2507.20291v1)<br>:star:[code](https://github.com/Joyies/TVT)
  * [ZFusion Efficient Deep Compositional Zero-shot Learning for Blind Image Super-Resolution with Generative Diffusion Prior](https://openaccess.thecvf.com/content/ICCV2025/papers/Esmaeilzehi_ZFusion_Efficient_Deep_Compositional_Zero-shot_Learning_for_Blind_Image_Super-Resolution_ICCV_2025_paper.pdf)
* 视频超分辨率
  * [VSRM: A Robust Mamba-Based Framework for Video Super-Resolution](https://arxiv.org/pdf/2506.22762v1)
  * [TurboVSR: Fantastic Video Upscalers and Where to Find Them](https://arxiv.org/pdf/2506.23618v1)

## Image Progress(图像/视频处理)
* 去噪
  * [Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention](https://arxiv.org/pdf/2506.23542v1)<br>:star:[code](https://github.com/davidweidawang/GIGA-ToF)
  * [Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment](https://arxiv.org/pdf/2508.06160v1)<br>:star:[code](https://github.com/GATECH-EIC/PostDiff)
* 去模糊
  * [Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model](https://arxiv.org/pdf/2507.13599v1)
* 图像去雾
  * [Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing](https://arxiv.org/pdf/2507.01275v1)
  * [When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training](https://arxiv.org/pdf/2507.09524v1)<br>:star:[code](https://github.com/ywxjm/DehazeSB)
  * [PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing](https://arxiv.org/pdf/2507.14826v1)
* 修复
  * [DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting](https://arxiv.org/pdf/2507.00429v1)<br>:star:[code](https://rorisis.github.io/DiGA3D/)
* 图像恢复
  * [EAMamba: Efficient All-Around Vision State Space Model for Image Restoration](http://arxiv.org/pdf/2506.22246v1)
  * [Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints](https://arxiv.org/pdf/2507.11985v1)<br>:star:[code](https://github.com/Jiahao-UTS/MPAE)
  * [Robust Adverse Weather Removal via Spectral-based Spatial Grouping](https://arxiv.org/pdf/2507.22498v1)
  * [Exploiting Diffusion Prior for Task-driven Image Restoration](https://arxiv.org/pdf/2507.22459v1)
  * [Reverse Convolution and Its Applications to Image Restoration](https://arxiv.org/pdf/2508.09824v1)<br>:star:[code](https://github.com/cszn/ConverseNet)
  * [Enhancing Image Restoration Transformer via Adaptive Translation Equivariance](http://arxiv.org/abs/2506.18520)
* 图像增强
  * [MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices](https://arxiv.org/pdf/2507.01838v1)<br>:star:[code](https://github.com/AVC2-UESTC/MobileIE.git)
  * [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/pdf/2507.10689v1)<br>:star:[code](https://github.com/bywlzts/CWNet-Causal-Wavelet-Network)
  * [Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement](https://arxiv.org/pdf/2507.12135v1)
  * [GT-Mean Loss: A Simple Yet Effective Solution for Brightness Mismatch in Low-Light Image Enhancement](https://arxiv.org/pdf/2507.20148v1)<br>:star:[code](https://github.com/jingxiLiao/GT-mean-loss)
* 视频调色
  * [Video Color Grading via Look-Up Table Generation](https://arxiv.org/pdf/2508.00548v1)<br>:star:[code](https://github.com/seunghyuns98/VideoColorGrading)

## Other
* [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/pdf/2507.11129v1)<br>:star:[code](https://github.com/Neal2020GitHub/MMOne)
* [PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling](http://arxiv.org/pdf/2506.20936v1)
* [EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception](http://arxiv.org/pdf/2506.21080v1)
* [Learning to See in the Extremely Dark](http://arxiv.org/pdf/2506.21132v1)<br>:star:[code](https://github.com/JianghaiSCU/SIED)
* [Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation](http://arxiv.org/pdf/2506.21198v1)<br>:star:[code](https://github.com/yihong-97/UNLOCK)
* [CA-I2P: Channel-Adaptive Registration Network with Global Optimal Selection](http://arxiv.org/pdf/2506.21364v1)
* [Global and Local Entailment Learning for Natural World Imagery](http://arxiv.org/pdf/2506.21476v1)<br>:star:[code](https://vishu26.github.io/RCME/index.html)
* [Attention to Burstiness: Low-Rank Bilinear Prompt Tuning](https://arxiv.org/pdf/2506.22908v1)
* [Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding](https://arxiv.org/pdf/2506.22803v1)<br>:star:[code](https://github.com/XiGuaBo/CBM-HNMU)
* [CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models](https://arxiv.org/pdf/2507.13984v1)
* [Learning Counterfactually Decoupled Attention for Open-World Model Attribution](https://arxiv.org/pdf/2506.23074v1)<br>:star:[code](https://github.com/yzheng97/CDAL)
* [Where, What, Why: Towards Explainable Driver Attention Prediction](https://arxiv.org/pdf/2506.23088v1)
* [VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions](https://arxiv.org/pdf/2506.23236v1)<br>:star:[code](https://markomih.github.io/VolumetricSMPL)
* [AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm](https://arxiv.org/pdf/2506.23537v1)<br>:star:[code](https://github.com/eezkni/AFUNet)
* [HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity](https://arxiv.org/pdf/2506.23854v1)
* [Rectifying Magnitude Neglect in Linear Attention](https://arxiv.org/pdf/2507.00698v1)<br>:star:[code](https://github.com/qhfan/MALA)
* [Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes](https://arxiv.org/pdf/2507.00327v1)<br>:star:[code](https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA)
* [Zero-shot Inexact CAD Model Alignment from a Single Image](https://arxiv.org/pdf/2507.03292v1)<br>:star:[code](https://zerocad9d.github.io/)
* [MGSfM: Multi-Camera Geometry Driven Global Structure-from-Motion](https://arxiv.org/pdf/2507.03306v1)<br>:star:[code](https://github.com/3dv-casia/MGSfM/)
* [Learning Normals of Noisy Points by Local Gradient-Aware Surface Filtering](https://arxiv.org/pdf/2507.03394v1)<br>:star:[code](https://github.com/LeoQLi/LGSF)
* [Pose-Star: Anatomy-Aware Editing for Open-World Fashion Images](https://arxiv.org/pdf/2507.03402v1)
* [Unlearning the Noisy Correspondence Makes CLIP More Robust](https://arxiv.org/pdf/2507.03434v1)
* [Less is More: Empowering GUI Agent with Context-Aware Simplification](https://arxiv.org/pdf/2507.03730v1)
* [Voyaging into Unbounded Dynamic Scenes from a Single View](https://arxiv.org/pdf/2507.04183v1)<br>:star:[code](https://tianfr.github.io/DynamicVoyager)
* [TeethGenerator: A two-stage framework for paired pre- and post-orthodontic 3D dental data generation](https://arxiv.org/pdf/2507.04685v1)<br>:star:[code](https://github.com/lcshhh/teeth_generator)
* [Beyond One Shot, Beyond One Perspective: Cross-View and Long-Horizon Distillation for Better LiDAR Representations](https://arxiv.org/pdf/2507.05260v1)<br>:star:[code](http://github.com/Xiangxu-0103/LiMA)
* [IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization](https://arxiv.org/pdf/2507.06856v1)
* [Synchronizing Task Behavior: Aligning Multiple Tasks during Test-Time Training](https://arxiv.org/pdf/2507.07778v1)
* [From Enhancement to Understanding: Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning](https://arxiv.org/pdf/2507.08380v1)
* [ConsNoTrainLoRA: Data-driven Weight Initialization of Low-rank Adapters using Constraints](https://arxiv.org/pdf/2507.08044v1)
* [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/pdf/2507.09305v1)
* [Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves](https://arxiv.org/pdf/2507.09207v1)
* [GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space](https://arxiv.org/pdf/2507.10473v1)
* [Boosting Multimodal Learning via Disentangled Gradient Learning](https://arxiv.org/pdf/2507.10213v1)<br>:star:[code](https://github.com/shicaiwei123/ICCV2025-GDL)
* [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/pdf/2507.09291v1)
* [Dataset Ownership Verification for Pre-trained Masked Models](https://arxiv.org/pdf/2507.12022v1)<br>:star:[code](https://github.com/xieyc99/DOV4MM)
* [Imbalance in Balance: Online Concept Balancing in Generation Models](https://arxiv.org/pdf/2507.13345v1)
* [MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP](https://arxiv.org/pdf/2507.15257v1)
* [DAViD: Data-efficient and Accurate Vision Models from Synthetic Data](https://arxiv.org/pdf/2507.15365v1)<br>:house:[project](https://aka.ms/DAViD)
* [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/pdf/2507.14505v1)<br>:star:[code](https://jiahao-ma.github.io/DCHM/)
* [Open-set Cross Modal Generalization via Multimodal Unified Representation](https://arxiv.org/pdf/2507.14935v1)<br>:star:[code](https://github.com/haihuangcode/CMG)
* [FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers](https://arxiv.org/pdf/2507.15249v1)<br>:star:[code](https://github.com/Monalissaa/FreeCus)
* [M-SpecGene: Generalized Foundation Model for RGBT Multispectral Vision](https://arxiv.org/pdf/2507.16318v1)<br>:star:[code](https://github.com/CalayZhou/M-SpecGene)
* [Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility](https://arxiv.org/pdf/2507.17748v1)
* [Joint Asymmetric Loss for Learning with Noisy Labels](https://arxiv.org/pdf/2507.17692v1)<br>:star:[code](https://github.com/cswjl/joint-asymmetric-loss)
* [AnimalClue: Recognizing Animals by their Traces](https://arxiv.org/pdf/2507.20240v1)<br>:star:[code](https://dahlian00.github.io/AnimalCluePage/)
* [Rethinking Few Shot CLIP Benchmarks: A Critical Analysis in the Inductive Setting](https://arxiv.org/pdf/2507.20834v1)
* [Controllable Feature Whitening for Hyperparameter-Free Bias Mitigation](https://arxiv.org/pdf/2507.20284v1)
* [Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry](https://arxiv.org/pdf/2507.20757v1)
* [Region-based Cluster Discrimination for Visual Representation Learning](https://arxiv.org/pdf/2507.20025v1)<br>:star:[code](https://github.com/deepglint/MVT)
* [Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting](https://arxiv.org/pdf/2507.18678v1)
* [CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception](https://arxiv.org/pdf/2507.19239v1)<br>:star:[code](https://github.com/zhongjiaru/CoopTrack)
* [TR-PTS: Task-Relevant Parameter and Token Selection for Efficient Tuning](https://arxiv.org/pdf/2507.22872v1)<br>:star:[code](https://github.com/synbol/TR-PTS)
* [ShortFT: Diffusion Model Alignment via Shortcut-based Fine-Tuning](https://arxiv.org/pdf/2507.22604v1)
* [DiffuMatch: Category-Agnostic Spectral Diffusion Priors for Robust Non-rigid Shape Matching](https://arxiv.org/pdf/2507.23715v1)<br>:star:[code](https://github.com/daidedou/diffumatch/)
* [SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions](https://arxiv.org/pdf/2507.23784v1)<br>:star:[code](https://github.com/ExplainableML/sub)<br>:house:[project](http://huggingface.co/datasets/Jessica-bader/SUB)
* [Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product](https://arxiv.org/pdf/2508.00230v1)
* [DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space](https://arxiv.org/pdf/2508.00413v1)<br>:star:[code](https://github.com/dc-ai-projects/DC-Gen)
* [CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective](https://arxiv.org/pdf/2508.00359v1)
* [GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration](https://arxiv.org/pdf/2508.00152v1)<br>:star:[code](https://limirs.github.io/GeoExplorer/)<br>:star:[code](https://github.com/limirs/GeoExplorer)
* [SCFlow: Implicitly Learning Style and Content Disentanglement with Flow Models](https://arxiv.org/pdf/2508.03402v1)<br>:star:[code](https://compvis.github.io/SCFlow/)<br>:star:[code](https://github.com/compvis/scflow)
* [TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction](https://arxiv.org/pdf/2508.04682v1)
* [How Would It Sound? Material-Controlled Multimodal Acoustic Profile Generation for Indoor Scenes](https://arxiv.org/pdf/2508.02905v1)<br>:star:[code](https://mahnoor-fatima-saad.github.io/m-capa.html)<br>:star:[code](https://github.com/mahnoor-fatima-saad/m-capa)
* [VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization](https://arxiv.org/pdf/2508.05211v1)
* [Symmetry Understanding of 3D Shapes via Chirality Disentanglement](https://arxiv.org/pdf/2508.05505v1)<br>:star:[code](https://wei-kang-wang.github.io/chirality/)<br>:star:[code](https://github.com/wei-kang-wang/chirality)
* [Learning an Implicit Physics Model for Image-based Fluid Simulation](https://arxiv.org/pdf/2508.08254v1)<br>:star:[code](https://physfluid.github.io/)
* [The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility](https://arxiv.org/pdf/2508.07989v1)
* [Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images](https://arxiv.org/pdf/2508.07847v1)<br>:star:[code](https://keio-smilab25.github.io/DeepSWM)<br>:star:[code](https://github.com/keio-smilab25/deepswm)
* [Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)](https://arxiv.org/pdf/2508.07775v1)<br>:star:[code](https://github.com/bastianlb/forecasting-rotational-dynamics)
* [CObL: Toward Zero-Shot Ordinal Layering without User Prompting](https://arxiv.org/pdf/2508.08498v1)<br>:house:[project](https://vision.seas.harvard.edu/cobl/)
* [UniConvNet: Expanding Effective Receptive Field while Maintaining Asymptotically Gaussian Distribution for ConvNets of Any Scale](https://arxiv.org/pdf/2508.09000v1)<br>:star:[code](https://github.com/ai-paperwithcode/UniConvNet)
* [TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos](https://arxiv.org/pdf/2508.09811v1)<br>:star:[code](https://github.com/vLAR-group/TRACE)
* [Combinative Matching for Geometric Shape Assembly](https://arxiv.org/pdf/2508.09780v1)<br>:star:[code](https://nahyuklee.github.io/cmnet)
* [Harnessing Input-Adaptive Inference for Efficient VLN](https://arxiv.org/pdf/2508.09262v1)<br>:star:[code](https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation)

* [Gradient Extrapolation for Debiased Representation Learning](http://arxiv.org/abs/2503.13236)<br>:house:[project](https://gerne-debias.github.io/)
* [Less-to-More Generalization: Unlocking More Controllability by In-Context Generation](http://arxiv.org/abs/2504.02160)<br>:star:[code](https://github.com/bytedance/UNO.)
* [Towards a Unified Copernicus Foundation Model for Earth Vision](http://arxiv.org/abs/2503.11849)<br>:star:[code](https://github.com/zhu-xlab/Copernicus-FM)
* [UnZipLoRA Separating Content and Style from a Single Image](http://arxiv.org/abs/2412.04465)
* [FlowDPS  Flow-Driven Posterior Sampling for Inverse Problems](http://arxiv.org/abs/2503.08136)

* [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://openaccess.thecvf.com/content/ICCV2025/papers/Tang_Closed-Loop_Transfer_for_Weakly-supervised_Affordance_Grounding_ICCV_2025_paper.pdf)

* [ReconDreamer Harmonizing Generative and Reconstructive Models for Driving Scene Representation](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_ReconDreamer_Harmonizing_Generative_and_Reconstructive_Models_for_Driving_Scene_Representation_ICCV_2025_paper.pdf)
* [Generative Zoo](http://arxiv.org/abs/2412.08101)
* [PAN-Crafter Learning Modality-Consistent Alignment for PAN-Sharpening](https://openaccess.thecvf.com/content/ICCV2025/papers/Do_PAN-Crafter_Learning_Modality-Consistent_Alignment_for_PAN-Sharpening_ICCV_2025_paper.pdf)
* [ArgMatch Adaptive Refinement Gathering for Efficient Dense Matching](https://openaccess.thecvf.com/content/ICCV2025/papers/Deng_ArgMatch_Adaptive_Refinement_Gathering_for_Efficient_Dense_Matching_ICCV_2025_paper.pdf)<br>:star:[code](https://github.com/ACuOoOoO/argmatch)
* [SANA-Sprint One-Step Diffusion with Continuous-Time Consistency Distillation](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation_ICCV_2025_paper.pdf)

* [Erasing More Than Intended How Concept Erasure Degrades the Generation of Non-Target Concepts](http://arxiv.org/abs/2501.09833)
* [Demeter A Parametric Model of Crop Plant Morphology from the Real World](https://openaccess.thecvf.com/content/ICCV2025/papers/Cheng_Demeter_A_Parametric_Model_of_Crop_Plant_Morphology_from_the_ICCV_2025_paper.pdf)


## 2020 年论文分类汇总戳这里
↘️[CVPR-2020-Papers](https://github.com/52CV/CVPR-2020-Papers) 
↘️[ECCV-2020-Papers](https://github.com/52CV/ECCV-2020-Papers)

<a name="00"/>

## 2021 年论文分类汇总戳这里
↘️[ICCV-2021-Papers](https://github.com/52CV/ICCV-2021-Papers)
↘️[CVPR-2021-Papers](https://github.com/52CV/CVPR-2021-Papers)

<a name="000"/>

## 2022 年论文分类汇总戳这里
↘️[CVPR-2022-Papers](https://github.com/52CV/CVPR-2022-Papers/blob/main/README.md)
↘️[WACV-2022-Papers](https://github.com/52CV/WACV-2022-Papers)
↘️[ECCV-2022-Papers](https://github.com/52CV/ECCV-2022-Papers/blob/main/README.md)

<a name="0000"/>

## 2023 年论文分类汇总戳这里
↘️[CVPR-2023-Papers](https://github.com/52CV/CVPR-2023-Papers)
↘️[WACV-2023-Papers](https://github.com/52CV/WACV-2023-Papers)
↘️[ICCV-2023-Papers](https://github.com/52CV/ICCV-2023-Papers)
↘️[2023-CV-Surveys](https://github.com/52CV/CV-Surveys/blob/main/2023-CV-Surveys.md)

### 扫码CV君微信(注明：CVPR)入微信交流群：
![9475fa20fd5e95235d9fa23ae9587a2](https://user-images.githubusercontent.com/62801906/156720309-de92964f-a6da-464a-b21f-cfb270c13e27.png)
